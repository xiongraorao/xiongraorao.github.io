<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python多线程和协程]]></title>
    <url>%2F2018%2F11%2F15%2Fpython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程 实现方法 线程通信和线程同步 队列 协程 参考文档 多线程实现方法python 中的多线程通常有两种实现方法，直接指定函数和继承线程类 直接指定线程目标函数 123456789import threadingimport timedef run(): time.sleep(1) print('do something else')t = threading.Thread(target=run, args=())t.start() 继承线程类 12345678910111213import threadingimport timeclass MyThread(threading.Thread): def __init__(self, arg): super(MyThread, self).__init__() # 注意：一定要显式的调用父类的初始化函数。 self.arg = arg def run(self): # 定义每个线程要运行的函数 time.sleep(1) print('the arg is:%s\r' % self.arg)t = MyThread()t.start() 线程通信和线程同步多线程通信可以通过多种方法，常见的有线程间的wait和notify机制，共享内存之类的。线程间的同步可以采用线程锁来实现 通知等待机制 首先看看java是如何实现线程wait 和 notify的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.oceanai.test;import java.util.ArrayDeque;import java.util.Queue;/** * 生产者和消费者实现. * * @author Xiong Raorao * @since 2018-11-15-11:15 */public class ProduceTest &#123; public static void main(String[] args) &#123; Queue&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); Object obj = new Object(); int count = 0; Thread produceThread = new Thread(() -&gt; &#123; synchronized (obj)&#123; while(true)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if(q.size() &gt; 10)&#123; try &#123; System.out.println("[生产者]:队列已满，等待"); obj.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;else &#123; int val = (int) (Math.random()*100); System.out.println("[生产者]: 收到通知，继续生产。。。"); q.add(val); System.out.println("[生产者]: 生产: " + val); obj.notify(); &#125; &#125; &#125; &#125;); Thread consumeThread = new Thread(()-&gt;&#123; synchronized (obj)&#123; while (true)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if(q.isEmpty())&#123; try &#123; System.out.println("[消费者]:队列为空，等待"); obj.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;else&#123; System.out.println("[消费者]:收到通知，继续消费"); int val = q.poll(); System.out.println("[消费者]:消费: " + val); obj.notify(); &#125; &#125; &#125; &#125;); produceThread.start(); consumeThread.start(); &#125;&#125; java的synchronized关键字绑定了一个对象锁,利用对象的wait()和notify()机制来实现消费者和生产者模型 由于Python对象没有隐式地和一个锁关联，因此需要利用Lock来实现,实现类为Condition 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: utf-8 -*- import threading class Q(object): def __init__(self): self.n=0 self.valueSet=False #相对于java,这里的要自己声明 self.cv=threading.Condition() def get(self): cv=self.cv #先得到锁 cv.acquire() if not self.valueSet: cv.wait() print "Got:",self.n self.valueSet=False cv.notify() #放弃锁 cv.release() return self.n def put(self,n): cv=self.cv #先得到锁 cv.acquire() if self.valueSet: cv.wait() self.n=n; self.valueSet=True print "Put:",n #放弃锁 cv.notify() cv.release() class Producer(threading.Thread): def __init__(self,q): threading.Thread.__init__(self) self.q=q self.start() def run(self): i=0 while i&lt;7: i+=1 self.q.put(i) class Consumer(threading.Thread): def __init__(self,q): threading.Thread.__init__(self) self.q=q self.start() def run(self): i=0 while i&lt;7: i+=1 self.q.get() if __name__=="__main__": q=Q() Producer(q) Consumer(q) 锁机制 python 中的锁有两种实现，一种是threading.Lock()，另一种是threading.RLock()， 区别在于，RLock在同一个线程中可以多次被获取和释放，acquire和release函数需要成对出现。Lock在一个线程里面只能被获取一次，否则会发生死锁，一直等待。 1234567891011121314151617181920import threadingsummary = 0lock = threading.Lock()def add(): global summary # global lock lock.acquire() for i in range(100000): summary += 1 summary -= 1 lock.release()def start(): for t in range(100): t = threading.Thread(target=add) t.start() print('summary = ', summary)start() 使用with直接获取锁 12345678def add(): global summary # global lock with lock: for i in range(100000): summary += 1 summary -= 1 守护线程 python线程启动之前可以设置线程的daemon属性，默认Daemon为False。 Daemon = False: 当主线程执行完毕，会一直等待子线程执行完。Daemon = True: 当主线程执行完毕，会立即结束程序。 队列python 自带的queue实现了线程安全的队列，一共实现了三种队列： 先进先出(FIFO) 后进先出(LIFO) 优先队列(Priority) 常见方法：Queue.Queue(maxsize=0) FIFO， 如果maxsize小于1就表示队列长度无限Queue.LifoQueue(maxsize=0) LIFO， 如果maxsize小于1就表示队列长度无限Queue.PriorityQueue(Queue) 优先队列，升序排列，要求Queue的元素为元组(priority number, data)Queue.qsize() 返回队列的大小Queue.empty() 如果队列为空，返回True,反之FalseQueue.full() 如果队列满了，返回True,反之FalseQueue.get([block[, timeout]]) 读队列，timeout等待时间Queue.put(item, [block[, timeout]]) 写队列，timeout等待时间Queue.queue.clear() 清空队列 协程协程：用户级别的线程，能够从函数A中断，然后跳转到另外一个函数执行，并且拥有自己的栈空间。 线程是系统级别的，它们是由操作系统调度；协程是程序级别的，由程序员根据需要自己调度。我们把一个线程中的一个个函数叫做子程序，那么子程序在执行过程中可以中断去执行别的子程序；别的子程序也可以中断回来继续执行之前的子程序，这就是协程。 协程的优缺点： 协程的优点： （1）无需线程上下文切换的开销，协程避免了无意义的调度，由此可以提高性能（但也因此，程序员必须自己承担调度的责任，同时，协程也失去了标准线程使用多CPU的能力） （2）无需原子操作锁定及同步的开销 （3）方便切换控制流，简化编程模型 （4）高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。 协程的缺点： （1）无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。 （2）进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序 123456789101112131415161718192021222324252627282930#! /usr/bin/env python# -*- coding:utf-8 -*-# Author: "Zing-p"# Date: 2017/5/12def consumer(name): print("要开始啃骨头了...") while True: print("\033[31;1m[consumer] %s\033[0m " % name) bone = yield print("[%s] 正在啃骨头 %s" % (name, bone))def producer(obj1, obj2): obj1.send(None) # 启动obj1这个生成器,第一次必须用None &lt;==&gt; obj1.__next__() obj2.send(None) # 启动obj2这个生成器,第一次必须用None &lt;==&gt; obj2.__next__() n = 0 while n &lt; 5: n += 1 print("\033[32;1m[producer]\033[0m 正在生产骨头 %s" % n) obj1.send(n) obj2.send(n)if __name__ == '__main__': con1 = consumer("消费者A") con2 = consumer("消费者B") producer(con1, con2) 参考文档 协程及Python中的协程]]></content>
      <tags>
        <tag>python</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的子进程]]></title>
    <url>%2F2018%2F11%2F14%2Fpython%E4%B8%AD%E7%9A%84%E5%AD%90%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[系统命令执行 os包 subprocess 多进程 多进程的实现 系统命令执行os包通常我们可以采用os.system(cmd) 或者os.popen(cmd)来调用系统命令。 os.system(cmd): 返回值为0（成功执行）， 其实返回的是一个16位的二进制数，低8位为杀死所调用脚本的信号号码，高位为脚本的退出状态码 12345678910## test.shecho 'hello'exit 3## import osret = os.system('./test.sh')ret = ret &gt;&gt; 8print(ret) # 3 输出：hello3 os.popen(cmd): 该种方法是通过管道的方式来实现命令调用，函数返回一个file-like对象，里面的内容是脚本的标准输出，可以通过read()函数来接收 123import osret = os.popen('./test.sh')print(ret.read()) 输出：hello subprocess对子进程进行更好的封装： 123456789def sub_proc2(): print('start subprocess') p = sp.Popen(['nslookup'], stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE) out, err = p.communicate(bytes('www.baidu.com', encoding='utf-8')) print(out.decode('utf-8')) print('exit')if __name__ == '__main__': sub_proc2() 多进程多进程的实现 多线程实现生产者和消费者模式（Queue）通信 1234567891011121314151617181920212223242526272829from multiprocessing import Process, Queueimport time, randomdef produce(q): count = 0 while True: time.sleep(random.random() * 3) if q.full() is False: val = count * count print('produce product: ', val) q.put(val) count += 1def consume(q): while True: time.sleep(random.random()*3) if q.empty() is False: print('consume product: ', q.get()) else: print('q is empty') continueif __name__ == '__main__': q = Queue(20) proc_produce = Process(target=produce, args=(q,)) proc_consume = Process(target=consume, args=(q,)) proc_produce.start() proc_consume.start() 使用Pipe通信 12345678910111213141516171819202122232425from multiprocessing import Process, Queue, Pipedef proc1(conn): for i in range(10): conn.send(i) print('send &#123;&#125; to pipe '.format(i)) time.sleep(1)def proc2(conn): n = 9 while n &gt; 0: result = conn.recv() print('receive &#123;&#125; from pip'.format(result)) n -= 1if __name__ == '__main__': p1, p2 = Pipe(True) # True 表示两个管道是全双工的，均可收发 process1 = Process(target=proc1, args=(p1,)) process2 = Process(target=proc2, args=(p2,)) process1.start() process2.start() process1.join() process2.join() p1.close() p2.close()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ipython远程访问配置]]></title>
    <url>%2F2018%2F11%2F06%2Fipython%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ubuntu 中实现ipython notebook的远程访问 安装ipython 和 ipython notebook 安装 Anoconda3 bash ./Anoconda-xx.sh 直接使用pip 安装 pip install ipython pip install notebook 创建登录用户和密码12345In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password:Verify password:Out[2]: 'sha1:026678de36b2:e4b83078e02c470b15789ade069359a20b0385dd' 创建服务器123456789101112131415161718ipython profile create myservercd ~/.ipython/profile_myserver/vim ipython_notebooke_config.py&gt;&gt;&gt;&gt;&gt;c = get_config()# Kernel configc.IPKernelApp.pylab = 'inline'# Notebook configc.NotebookApp.ip='*'c.NotebookApp.open_browser = Falsec.NotebookApp.password = u'sha1:026678de36b2:e4b83078e02c470b15789ade069359a20b0385dd'# It's a good idea to put it on a know,fixed portc.NotebookApp.port = 6789&lt;&lt;&lt;&lt;&lt;&lt; 启动ipython服务器 ipython notebook --config=~/.ipython/profile_myserver/ipython_notebook_config.py ipython notebook 设置多个虚拟环境123456789101112131415161718192021222324252627282930# 查找kernel.json的位置find . -name "kernel.json"&lt;&lt;&lt;&lt;# xrr @ Cloud-06 in ~ [11:53:53] $ find . -name "kernel.json"./anaconda3/share/jupyter/kernels/python3/kernel.json./anaconda3/envs/py36/share/jupyter/kernels/python3/kernel.json./anaconda3/pkgs/ipykernel-4.9.0-py37_1/share/jupyter/kernels/python3/kernel.json(base) &gt;&gt;&gt;&gt;cd ~/anaconda3/envs/py36/share/jupyter/kernelsmkdir your_env_name &amp;&amp; cd your_env_namecp ../python3/kernel.json .vim kernel.json&lt;&lt;&lt;&#123; "argv": [ "/home/xrr/anaconda3/envs/py36/bin/python", # 替换为虚拟环境的python "-m", "ipykernel_launcher", "-f", "&#123;connection_file&#125;" ], "display_name": "py36", # 修改为你的虚拟环境的名称 "language": "python"&#125;&gt;&gt;&gt; jupyter 切换虚拟环境 Enjoy yourself！]]></content>
      <tags>
        <tag>教程</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络和深度学习基础]]></title>
    <url>%2F2018%2F10%2F30%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[符号定义 线性回归 梯度下降（Gradient Descent） 计算图 算法伪代码 python 实现 符号定义对于二分类的问题，假设输出$y$的值域为$\{0,1\}$, 输入的样本的数量为m，训练样本表示如下： \{(x^{(1)}, x^{(2)}, ... , x^{(m)}\}假设每个样本的特征向量为n维， 将每个样本作为列向量，按照行方向堆叠，得到一个$n \cdot m$的矩阵, 其中$ X \in R^{n*m} $ X = \begin{bmatrix} \vdots &\vdots &\vdots\\ x_1 & x_2 & x_3\\ \vdots & \vdots &\vdots \\ \end{bmatrix}线性回归根据输入的样本，寻找一个线性函数$\hat y = f(x)$ 使得 $\hat y$的值更加接近$y$，通常表示如下： \hat y = w^Tx+b $$ 但是这个值有可能比0或者1相差太多，因此需要一个激活函数$\sigma(z) = \frac{1}{1+e^{-z}}$ 来将结果拉伸到0 ~ 1 之间。 如下图： ![](/images/dl/logistic.png) 修正之后有： $$ \hat y = \sigma(w^Tx+b)梯度下降（Gradient Descent）如何求取w和b，使得$\hat y$的值更加接近于$y$？采用梯度下降法来更新w和b，最终使得$\hat y$更加接近于$y$ $ \hat y = \sigma(w^Tx+b) $ — 回归函数$ \sigma(z) = \frac{1}{1+e^{-z}} $ — 激活函数$ L(\hat y, y) = -(y \log \hat y + (1-y)\log (1 - \hat y)) $ — 损失函数$ J(w,b) = \frac{1}{m} \sum\limits_{i = 1 }^m L(\hat y^{(i)} + y{(i)})$ — 代价函数 有理论证明J(w,b)是关于w和b的凸函数，因此直接将J对w和b分别求导，令导数等于0，则得到的w和b即为所求的值，是的代价函数的值最小。 计算图计算图的概念比较好理解，类似于链式求导法则，前向计算，反向求导，后面一步总是可以利用前面一步的结果 下面来计算线性回归中的求导部分： 公式如下: z = w^Tx + b\hat y = a = \sigma(z) = \frac{1}{1+e^{-z}}L(a,y) = -(y\log a + (1-y)log(1-a))这里令n=2,（假设输入的样本为二维向量） 则有前向计算公式: $ z = w_1 \cdot x_1 + w_2 \cdot x_1 + b $ $a = \sigma (z)$ $ L(a,y)$ 反向求导公式： $ da = \frac{\partial L(a,y)}{\partial a}$ $ dz= \frac{\partial L(a,y)}{\partial z} \\ = \frac{\partial L(a,y)}{\partial a} \cdot \frac{\partial a}{\partial z} \\ = (- \frac {y}{a} + \frac{1-y}{1-a}) \cdot a \cdot (1-y) \\ = a-y$ $dw = \frac{\partial L(a,y)}{\partial w} \\ = dz \cdot \frac{\partial z}{\partial w} \\ = dz \cdot x $$=&gt; dw_1 = x_1 \cdot dz, dw_2 = x_2 \cdot dz, dw_3 = x_3 \cdot dz$ $db = \frac{\partial L(a,y)}{\partial b} \\=dz \cdot \frac{\partial z}{\partial b} \\=dz$ 更新方法： $ w_1:= w_1 - \alpha \cdot dw_1 $$ w_2:= w_2 - \alpha \cdot dw_2 $$ b:= b - \alpha \cdot db $ 考虑m个样本的话，将损失函数替换为代价函数，然后求均值即可。 代价函数 = avg(损失函数) $\frac{\partial J(w,b)}{\partial w} = \frac{1}{m} \sum\limits_{i = 1 }^m \frac{\partial L(a^{(i)} + y{(i)})}{\partial w}$ $ w:= w - \alpha \cdot \frac{\partial{J(w,b)}}{\partial{w}} $$ b:= b - \alpha \cdot \frac{\partial{J(w,b)}}{\partial{b}} $ 算法伪代码initialize $J = 0, dw_1 = 0, dw_2 = 0, db = 0$for i = 1 to m: $z^{(i)}=w^T \cdot x^{(i)} + b$ $J += -(y^{(i)} \log a^{(i)} + (1-y)\log (1 - a^{(i)}))$ $dz^{(i)} = a^{(i)} - y^{(i)}$ $dw_1 += x_1^i \cdot dz^{(i)}$ $dw_2 += x_2^i \cdot dz^{(i)}$ $db += dz^{(i)}$end for$J /= m$$dw1 /= m$$dw2 /= m$$db /= m$$w1 -= \alpha \cdot dw_1 $$w2 -= \alpha \cdot dw_2 $$b -= \alpha \cdot db $ python 实现12345678910111213141516171819202122232425x = np.array([[3,4],[2,5],[6,7]])x = np.transpose(x)y = np.array([1,1,0]).reshape(3,1)J = 0m = y.shape[0]w = np.zeros([2,1])dw = np.zeros([2,1])db = np.zeros([3,1])b = np.zeros([3,1])epoch = 100lr = 0.001 # 学习率for i in range(0, epoch): z = np.dot(w.T,x) + b.T a = 1/(1+np.exp(-z)) J += np.sum(-(y*np.log(a)+(1-y)*np.log(1-a))) dz = a.T - y dw += np.dot(x, dz) db += dz dw /= m db /= m J /= m print('epoch: &#123;&#125; Loss = &#123;&#125;'.format(i, J)) # 更新权值 w -= lr*dw b -= lr*db]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析-python]]></title>
    <url>%2F2018%2F10%2F26%2F%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90-python%2F</url>
    <content type="text"><![CDATA[聚类根据方法大致可以分为以下几种： （1）基于划分的方法： 给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K&lt;N。 算法：K-MEANS算法、K-MEDOIDS算法、CLARANS算法 （2） 基于层次的方法： 对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。特点：较小的计算开销。然而这种技术不能更正错误的决定。算法：BIRCH算法、CURE算法、CHAMELEON算法 （3） 基于密度的方法 只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。特点：能克服基于距离的算法只能发现“类圆形”的聚类的缺点。算法：DBSCAN算法、OPTICS算法、DENCLUE算法 （4）基于网络的方法 将数据空间划分成为有限个单元（cell）的网格结构,所有的处理都是以单个的单元为对象的。特点：处理速度很快，通常这是与目标数据库中记录的个数无关的，只与把数据空间分为多少个单元有关。算法：STING算法、CLIQUE算法、WAVE-CLUSTER算法 性能度量外部指标（有监督）根据给定的数据集，然后做参考]]></content>
      <tags>
        <tag>python</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习(1)]]></title>
    <url>%2F2018%2F10%2F25%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1%2F</url>
    <content type="text"><![CDATA[概要 卷积神经网络（CNN） 卷积层 DropOut 激活层 正则化层 Triplet Loss 本系列日志为深度学习的笔记 概要深度学习 卷积神经网络（CNN）卷积层DropOut为了提高CNN的特征表达能力，通常采用更深的网络和更多的神经元，但是复杂的网络容易导致过拟合，利用Dropout在一定程度上面可以防止过拟合 如上图左，为没有Dropout的普通2层全连接结构，记为 r=a(Wv)，其中a为激活函数。 如上图右，为在第2层全连接后添加Dropout层的示意图。即在 模 型 训 练 时 随机让网络的某些节点不工作（输出置0），其它过程不变。 参考链接： 系列解读Dropout 激活层Sigmoid 函数：$S(x) = \frac{1}{1+e^{-x}}$ 正则化层批量正则化: y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + betaTriplet LossTriplet Loss 可以理解为三元损失，主要用于训练差异性较小的样本，如人脸等，Feed数据包括锚（Anchor）示例、正（Positive）示例、负（Negative）示例，通过优化锚示例与正示例的距离小于锚示例与负示例的距离，实现样本的相似性计算。 目标函数如下： L = max(\sum\limits_{i}^{N}\Vert f(a_i^p) - f(x_i^p))\Vert^2 - \Vert f(a_i^p) - f(x_i^n))\Vert^2 + \alpha , 0)由目标函数可以看出: 当x_a与x_n之间的距离 &lt; x_a与x_p之间的距离加$\alpha$时，[]内的值大于零，就会产生损失。 当x_a与x_n之间的距离 &gt;= x_a与x_p之间的距离加$\alpha$时，损失为零。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 博客遇到的坑--host key verification failed]]></title>
    <url>%2F2018%2F10%2F24%2Ffailed%2F</url>
    <content type="text"><![CDATA[使用hexo d进行部署的时候，会遇到如下的报错信息： Host Key Verification failed …. 原因：ssh链接的时候没有加入到known_host 里面去，因此需要执行如下命令： 123456789101112ssh -T git@github.comThe authenticity of host 'github.com (192.30.252.1)' can't be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.are you sure you want to continue connecting (yes/no)?'yes# 如果有自己的服务器的话，同样需要执行ssh root@your.server.ip yes tips：遇到坑，先分析原因，分析不出来就google，==]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python学习笔记]]></title>
    <url>%2F2018%2F10%2F23%2Fpython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[安装 python 解释器 IDE 函数 可变参数 关键字参数 高级特性 迭代 列表生成式 生成器 函数式编程 高阶函数 面向对象 类属性和实例属性 @property（装饰器） 异常处理 调试 进程和线程 多进程 多线程 ThreadLocal 协程 数组 切片 索引 整数索引 布尔索引 副本和视图 参考文档 安装python 官网 Anoconda(推荐) python 解释器CPython: 命令行的python ipython: CPython 的增强版，能够自动提示，有很多magic功能 PyPy: 采用JIT技术，动态编译代码，显著提高python的执行速度 Jython: 运行在Java平台的python解释器，可以把Python代码编译成Java字节码执行 IDEPycharm(推荐，可以支持远程解释执行)、Spider 函数python 不支持函数的重载，以后面的定义的函数为准。 12345678910def power(a): return a*adef power(a,n): s = 1 while n &gt; 0: n = n - 1 s = s*a return s 当调用power(5)的时候，发现代码报错，原因是被后面的power函数给重定义了，第一个失效，因此采用默认参数来实现函数的重载 123456def power(a,n=2): s = 1 while n &gt; 0: n = n - 1 s = s*a return s 注意： 必选参数需要放在前面，默认参数放在后面 变换大的参数放前面，变化小的参数放后面 默认参数必须指向不变对象（None, str,tuple) 可变参数函数参数前面加一个星号 12345678def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sumcalc(1,2,3)calc(1,2) 关键字参数可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。请看示例： 123456789101112131415161718def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw)&gt;&gt;&gt; person('Michael', 30)name: Michael age: 30 other: &#123;&#125;&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;# 还可以自己对参数进行过滤def person(name, age, **kw): if 'city' in kw: # 有city参数 pass if 'job' in kw: # 有job参数 pass print('name:', name, 'age:', age, 'other:', kw) 高级特性迭代采用 for + 可迭代对象的方式实现 123items = [1,2,3]for i, item in enumerate(items): print(i, item) python 中如何判断对象的可迭代性： 123from collections import Iterableisinstance([], Iterable) # Trueisinstance(1, Iterable) # False 可迭代对象的本质：可迭代对象通过 __iter__⽅法向我们提供⼀个迭代器，我们在迭代⼀个可迭代对象的时候，实际上就是先获取该对象提供的⼀个迭代器，然后通过这个迭代器来依次读取对象中的每⼀个数据。那么也就是说，⼀个具备了__iter__⽅法的对象，就是⼀个可迭代对象。 123456789101112class MyList(object): def __init__(self): self.container = [] def add(self, item): self.container.append(item) def __iter__(self): """返回⼀个迭代器""" # 我们暂时忽略如何构造⼀个迭代器对象 passmylist = MyList()from collections import Iterableisinstance(mylist, Iterable) # True 列表生成式python 内置的非常强大的用来创建list的生成式 12345678910111213&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100]# 筛选出偶数的平方&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100]# java语言的a&gt;b？a：b&gt;&gt;&gt; a if a &gt; b else b 生成器列表生成式是一次性创建，而生成器可以边循环边创建,只需要把列表生成式的[]改成() 12345678910111213141516&gt;&gt;&gt;g = (x*x for x in range(10))&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt;# 使用next()函数获得generator的下一个返回值&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1...81# 没有更多元素时候，抛出异常&gt;&gt;&gt;next(g)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 生成器遇到yeild 就会返回，再次执行next()函数就会从上次返回的yeild语句处继续执行 12345678910111213141516171819202122def odd(): print('step 1') yield 1 print('step 2') yield(3) print('step 3') yield(5)&gt;&gt;&gt; o = odd()&gt;&gt;&gt; next(o)step 11&gt;&gt;&gt; next(o)step 23&gt;&gt;&gt; next(o)step 35&gt;&gt;&gt; next(o)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 函数式编程变量可以指向函数,例如:123import mathf = math.cosprint(f(5)) 高阶函数将函数作为另一个函数的参数输入，该函数称为高阶函数 1234567def add(x,y,f): return f(x)+f(y)x = -5y = 6f = absprint(add(x,y,f)) 面向对象dir(obj) : 查看对象的所有属性和方法isinstance(obj, class): 判断obj对象是否是class类型hasattr(obj, att) : 判断对象obj是否有att属性getattr(obj, att) : 获取对象obj的att属性setattr(obj, att) : 设置对象obj的att属性 类属性和实例属性实例变量和类变量同名的话，实例变量会覆盖掉类变量123456789101112class A: __slot__ = ('name','sex') # 使用turple来限制类能绑定的属性的名称 name = "sa" # 类属性 def __init__(self, name): self.name = name # 实例属性, 覆盖掉类属性 self.sex = "male" # 实例属性a = A("test")print(a.name) # testprint(A.name) # sadle a.nameprint(a.name) # sa @property（装饰器）对于私有变量，可以采用@property的方法实现把一个方法当做属性进行调用 12345678910111213141516class Student(object): @property # 负责把一个getter 方法编程属性 def score(self): return self._score @score.setter # 负责把一个setter方法变成属性赋值 def score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = values = Student()s.score = 60 异常处理try … except … finally … 的错误处理机制 1234567891011try: print('try...') r = 10 / 0 print('result:', r)except ZeroDivisionError as e: print('except:', e) # 有异常发生的时候执行else: print('no exceptions') # 无异常发生的时候执行finally: print('finally...') # 一定会执行print('END') 调试 print打印 assert 断言 logging 包 pdb 调试 python -m pdb err.py 启动调试模式，单步执行 或者在err.py里面使用pdb.set_trace()代码，使用python命令执行程序会自动在这一行进入pdb调试模式 1234567# err.pyimport pdbs = '0'n = int(s)pdb.set_trace() # 运行到这里会自动暂停print(10 / n) pdb 常用命令：p 变量名： 查看变量n ：单步执行代码c ：执行到断点处 进程和线程python 采用了GIL（Global Interpreter Lock）锁的机制,因此单进程多线程的程序只能利用一个CPU核心，想要跑满多核，需要采用多进程编程的方法来实现 多进程采用 os.fork() 或者multiprocessing 的包来实现多进程 参考链接 多线程1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) ThreadLocalPython 也支持lock，也可以使用threadlocal 来实现线程之间变量的隔离 协程python 中的协程(Coroutine)是通过generator实现的。 优势： 极高的执行效率，子程序之间不用线程切换，由程序自身控制，线程越多，协程的优势越明显 不需要多线程的锁机制。因此采用多进程+协程的方法，能够利用多核CPU的优势。 123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c) asyncio 异步IO的支持 1234567891011121314import asyncio@asyncio.coroutinedef hello(): print("Hello world!") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print("Hello again!")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello())loop.close() @asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。 hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。 把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。 数组切片 基本切片 12345import numpy as npa = np.arange(10)s = slice(2,7,2) print a[s] # [2 4 6]print a[2:7:2] # [2 4 6] 冒号分隔的切片 基本语法： ndarrayObj[start:end:step, other dims] start: 开始切片的序号，默认0 end: 结束切片的序号, 默认为该维度最后一个 step: 步长，如果是负数表示从end到start方向进行遍历，默认为1 ... : 来使选择元组的长度与数组的维度相同 示例：12345678910111213141516171819202122232425262728293031323334import numpy as npa = np.array([[1,2,3],[3,4,5],[4,5,6]]) print '我们的数组是：' print aprint '\n' # 这会返回第二列元素的数组： print '第二列的元素是：' print a[...,1] print '\n' # 现在我们从第二行切片所有元素： print '第二行的元素是：' print a[1,...] print '\n' # 现在我们从第二列向后切片所有元素：print '第二列及其剩余元素是：' print a[...,1:]#output 我们的数组是：[[1 2 3] [3 4 5] [4 5 6]]第二列的元素是：[2 4 5]第二行的元素是：[3 4 5]第二列及其剩余元素是：[[2 3] [4 5] [5 6]] 索引整数索引12345678import numpy as npx = np.arange(1,7)x = x.reshape(2,3)y = x[[0,1,2],[0,1,0]]print(y) # output :# [1 4 5] 该结果包括数组中(0,0)，(1,1)和(2,0)位置处的元素。 12345678910111213141516171819202122x = np.arange(12)x.resize(4,3)print '我们的数组是：' print x print '\n' rows = np.array([[0,0],[3,3]])cols = np.array([[0,2],[0,2]])y = x[rows,cols] print '这个数组的每个角处的元素是：' print y# output:我们的数组是： [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]这个数组的每个角处的元素是： [[ 0 2] [ 9 11]] 布尔索引当结果对象是布尔运算(例如比较运算符)的结果时，将使用此类型的高级索引。 123456789101112131415161718import numpy as np x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) print '我们的数组是：' print x print '\n' # 现在我们会打印出大于 5 的元素 print '大于 5 的元素是：' print x[x &gt; 5]# output:我们的数组是：[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]] 大于 5 的元素是：[ 6 7 8 9 10 11] 副本和视图在执行函数时，其中一些返回输入数组的副本，而另一些返回视图。 当内容物理存储在另一个位置时，称为副本。 另一方面，如果提供了相同内存内容的不同视图，我们将其称为视图。 无复制 简单的赋值不会创建对象的副本,相当于C中的指针复制，两个对象拥有相同的地址，对任何一个对象进行操作都会影响到之前的对象 123456789101112131415161718192021222324252627282930313233343536373839import numpy as np a = np.arange(6) print '我们的数组是：' print a print '调用 id() 函数：' print id(a) print 'a 赋值给 b：' b = a print b print 'b 拥有相同 id()：' print id(b) print '修改 b 的形状：' b.shape = 3,2 print b print 'a 的形状也修改了：' print a# output:我们的数组是：[0 1 2 3 4 5]调用 id() 函数：139747815479536a 赋值给 b：[0 1 2 3 4 5]b 拥有相同 id()：139747815479536修改 b 的形状：[[0 1] [2 3] [4 5]]a 的形状也修改了：[[0 1] [2 3] [4 5]] 视图或浅复制 两个对象拥有相同的存储地址，虽然id()函数得到的不一致，但是使用的是同一个内存空间，但是可以呈现出不同的形状 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647a = np.arange(6).reshape(2,3)b = a.view()print('id(a):', id(a))print('id(b):', id(b))print('a的形状')print(a)print('b的形状')print(b)b = b.reshape(3,2)print('reshape b 之后')print('a的形状')print(a)print('b的形状')print(b)b[0,0] = 10print('对b[0,0]进行赋值操作', ':b[0,0]=10')print('a的形状')print(a)print('b的形状')print(b)# outputid(a): 1796306611056id(b): 1796306500144a的形状[[0 1 2] [3 4 5]]b的形状[[0 1 2] [3 4 5]]reshape b 之后a的形状[[0 1 2] [3 4 5]]b的形状[[0 1] [2 3] [4 5]]对b[0,0]进行赋值操作 :b[0,0]=10a的形状[[10 1 2] [ 3 4 5]]b的形状[[10 1] [ 2 3] [ 4 5]] 深复制 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as np a = np.array([[10,10], [2,3], [4,5]]) print '数组 a：' print a print '创建 a 的深层副本：' b = a.copy() print '数组 b：' print b # b 与 a 不共享任何内容 print '我们能够写入 b 来写入 a 吗？' print b is a print '修改 b 的内容：' b[0,0] = 100 print '修改后的数组 b：' print b print 'a 保持不变：' print a#output数组 a：[[10 10] [ 2 3] [ 4 5]]创建 a 的深层副本：数组 b：[[10 10] [ 2 3] [ 4 5]]我们能够写入 b 来写入 a 吗？False修改 b 的内容：修改后的数组 b：[[100 10] [ 2 3] [ 4 5]]a 保持不变：[[10 10] [ 2 3] [ 4 5]] 参考文档 廖雪峰的官方网站]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8习惯用语]]></title>
    <url>%2F2018%2F08%2F09%2Fjava8%E4%B9%A0%E6%83%AF%E7%94%A8%E8%AF%AD%2F</url>
    <content type="text"><![CDATA[Java 8 习惯用语本文介绍了java 8 的常用用法： 2017年 IBM developerWorks 最受欢迎的内容 Java 中的一种更轻松的函数式编程途径命令式格式下面代码使用命令式格式寻找数组中的目标元素：12345678910111213141516171819202122232425import java.util.*; public class FindNemo &#123; public static void main(String[] args) &#123; List&lt;String&gt; names = Arrays.asList("Dory", "Gill", "Bruce", "Nemo", "Darla", "Marlin", "Jacques"); findNemo(names); &#125; public static void findNemo(List&lt;String&gt; names) &#123; boolean found = false; for(String name : names) &#123; if(name.equals("Nemo")) &#123; found = true; break; &#125; &#125; if(found) System.out.println("Found Nemo"); else System.out.println("Sorry, Nemo not found"); &#125;&#125; 声明式格式直接采用集合的contains接口来判断是否存在目标元素：123456public static void findNemo(List&lt;String&gt; names) &#123; if(names.contains("Nemo")) System.out.println("Found Nemo"); else System.out.println("Sorry, Nemo not found");&#125; 函数式格式尽管函数式格式的编程始终是声明式的，但简单地使用声明式编程并不等于函数式编程。这是因为函数式编程合并了声明式方法与高阶函数。图 1 直观地展示了命令式、声明式和函数式编程之间的关系。 图 1. 命令式、声明式和函数式编程的联系 该例子实现了统计站点访问次数的功能：12345678910111213141516171819202122232425262728293031323334353637383940package com.raorao.java.base;import java.util.HashMap;import java.util.Map;import java.util.stream.Stream;/** * hash 的函数式编程用法，统计网站访问次数. * * @author Xiong Raorao * @since 2018-08-09-10:18 */public class MapMergeTest &#123; public static void main(String[] args) &#123; String[] websites = new String[] &#123;"https://www.baidu.com", "https://www.baidu.com", "https://www.baidu.com", "https://www.google.com", "https://www.google.com", "https://www.sina.com.cn"&#125;; Map&lt;String, Integer&gt; map1 = new HashMap&lt;&gt;(); Map&lt;String, Integer&gt; map2 = new HashMap&lt;&gt;(); merge(map1, websites); incrementPageVisit(map2, websites); map1.forEach((k,v)-&gt; System.out.print("k= " +k + ", v= " + v + "\t")); System.out.println(); map2.forEach((k,v)-&gt; System.out.print("k= " +k + ", v= " + v + "\t")); &#125; public static void merge(Map&lt;String, Integer&gt; map, String[] args) &#123; Stream.of(args).forEach((e) -&gt; map.merge(e, 1, (oldValue, value) -&gt; (oldValue + value))); &#125; public static void incrementPageVisit(Map&lt;String, Integer&gt; pageVisits, String[] pages) &#123; for (String page : pages) &#123; if (!pageVisits.containsKey(page)) &#123; pageVisits.put(page, 0); &#125; pageVisits.put(page, pageVisits.get(page) + 1); &#125; &#125;&#125; merge 函数中使用了map的merge接口，merge函数有四个参数，前面两个参数分别是要处理的key，初始值（如果键不存在，则赋值1），第三个参数（一个拉姆达表达式）接收 map 中该键对应的值作为其参数，并且将该值作为变量传递给 merge 方法中的第二个参数。（拉姆达表达式第二个参数为统计后的新值，实际上就是递增计数） 下面看看Map.merge()实现的源码，merge函数直接在Map接口中作为默认函数实现的（java 8 引入了接口的默认函数），其实就是把key的旧值得到，然后加上第二个参数。 1234567891011121314default V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if(newValue == null) &#123; remove(key); &#125; else &#123; put(key, newValue); &#125; return newValue; &#125; 参考链接 函数组合与集合管道模式利用java 8 stream的特性，实现集合的自定义遍历 12345678910111213141516171819202122232425262728293031323334353637383940package agiledeveloper; public class Car &#123; private String make; private String model; private int year; public Car(String theMake, String theModel, int yearOfMake) &#123; make = theMake; model = theModel; year = yearOfMake; &#125; public String getMake() &#123; return make; &#125; public String getModel() &#123; return model; &#125; public int getYear() &#123; return year; &#125;&#125;public class Iterating &#123; public static List&lt;Car&gt; createCars() &#123; return Arrays.asList( new Car("Jeep", "Wrangler", 2011), new Car("Jeep", "Comanche", 1990), new Car("Dodge", "Avenger", 2010), new Car("Buick", "Cascada", 2016), new Car("Ford", "Focus", 2012), new Car("Chevrolet", "Geo Metro", 1992) );&#125;public static List&lt;String&gt; getModelsAfter2000UsingPipeline( List&lt;Car&gt; cars) &#123; return cars.stream() .filter(car -&gt; car.getYear() &gt; 2000) .sorted(Comparator.comparing(Car::getYear)) .map(Car::getModel) .collect(toList()); &#125; 参考链接 传统 for 循环的函数式替代方案传统的for循环会产生垃圾变量,代码量较大。采用函数式for循环更加简洁 例子：循环产生多个线程 123IntStream.range(0, 5) .forEach(i -&gt; executorService.submit(() -&gt; System.out.println("Running task " + i))); 封闭范围（闭区间） 1IntStream.rangeClosed(0, 5) 跳过值（等差或者自己实现跳过的方法） 123IntStream.iterate(1, e -&gt; e + 3) .limit(34) .sum() 逆向迭代 12IntStream.iterate(7, e -&gt; e - 1) .limit(7) 参考链接 提倡有帮助的编码使用函数式编码可以让代码更便于理解。下面是找出一个数组中名字长度为4的例子，使用函数式编程非常简洁易懂。 123456List&lt;String&gt; names = Arrays.asList("Jack", "Jill", "Nate", "Kara", "Kim","Jullie", "Paul", "Peter"); System.out.println( names.stream() .filter(name -&gt; name.length() == 4) .collect(Collectors.joining(", "))); 传递表达式（pass-through lambdas）的替代方案(使用方法引用)12345List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10); numbers.stream() .filter(e -&gt; e % 2 == 0) .forEach(e -&gt; System.out.println(e)); 第二行forEach 循环里面对于元素并没有进行实际的操作，只是传入了元素的值，因此在这种情况下可以采用元素的方法引用来代替。 123numbers.stream() .filter(e -&gt; e % 2 == 0) .forEach(System.out::println); 参考链接 实例方法的实参传递 我们使用以下格式将此 lambda 表达式替换为方法引用 System.out::println：referenceToInstance::methodName。 例如传递一个this上面的实参 123456789101112public class Example &#123; public int increment(int number) &#123; return number + 1; &#125; //...&#125;Stream.map(e -&gt; increment(e)) .map(e -&gt; this.increment(e)).map(this::increment) 静态方法的实参传递ClassName::staticMethodName 1.map(Integer::valueOf) 将形参传递给目标 这个和上面的静态方法的实参传格式是一样的，但是这里是将形参作为实例方法的调用目标。 1.map(e -&gt; e.doubleValue()) 在这个示例中，形参 e（我们假设其推断类型为 Integer）是对 doubleValue 方法的调用的目标。上图中给出了这种传递 lambda 表达式的结构。 传递构造函数调用1.collect(toCollection(() -&gt; new LinkedList&lt;Double&gt;())); 代码的目的是获取一个数据 Stream ，将它精减或收集到一个 LinkedList 中。toCollection 方法接受一个 Supplier 作为其实参。Supplier 不接受任何形参，因此 () 为空。它返回一个 Collection 实例，该实例在本例中是 LinkedList。 1.collect(toCollection(LinkedList::new)); 传递多个实参1.reduce(0, (total, e) -&gt; Integer.sum(total, e))); 1.reduce(0, Integer::sum)); 参考链接 为什么完美的 lambda 表达式只有一行参考链接 函数接口Thread 类的构造函数想要一个实现 Runnable 的实例。在本例中，我们传递了一个 lambda 表达式，而不是传递一个对象。我们可以选择向各种各样的方法和构造函数传递 lambda 表达式，包括在 Java 8 之前创建的一些方法和构造函数。这很有效，因为 lambda 表达式在 Java 中表示为函数接口。 1Thread thread = new Thread(() -&gt; System.out.println("In another thread")); 函数接口有 3 条重要法则： 一个函数接口只有一个抽象方法。在 Object 类中属于公共方法的抽象方法不会被视为单一抽象方法。函数接口可以有默认方法和静态方法。 任何满足单一抽象方法法则的接口，都会被自动视为函数接口。这包括 Runnable 和 Callable 等传统接口，以及您自己构建的自定义接口。 内置函数接口除了已经提到的单一抽象方法之外，JDK 8 还包含多个新函数接口。最常用的接口包括 Function、Predicate 和 Consumer，它们是在 java.util.function 包中定义的。Stream 的 map 方法接受 Function 作为参数。类似地，filter 使用 Predicate，forEach 使用 Consumer。该包还有其他函数接口，比如 Supplier、BiConsumer 和 BiFunction。 可以将内置函数接口用作我们自己的方法的参数。例如，假设我们有一个 Device 类，它包含方法 checkout 和 checkin 来指示是否正在使用某个设备。当用户请求一个新设备时，方法 getFromAvailable 从可用设备池中返回一个设备，或在必要时创建一个新设备。 1234567891011public void borrowDevice(Consumer&lt;Device&gt; use) &#123; Device device = getFromAvailable(); device.checkout(); try &#123; use.accept(device); &#125; finally &#123; device.checkin(); &#125;&#125; borrowDevice 方法： 接受 Consumer 作为参数。 从池中获取一个设备（我们在这个示例中不关心线程安全问题）。 调用 checkout 方法将设备状态设置为 checked out。 将设备交付给用户。 在完成设备调用后返回到 Consumer 的 accept 方法时，通过调用 checkin 方法将设备状态更改为 checked in。 下面给出了一种使用 borrowDevice 方法的方式： new Sample().borrowDevice(device -&gt; System.out.println(“using “ + device));因为该方法接收一个函数接口作为参数，所以传入一个 lambda 表达式作为参数是可以接受的。 自定义函数接口尽管最好尽量使用内置函数接口，但有时需要自定义函数接口。 要创建自己的函数接口，需要做两件事： 使用 @FunctionalInterface 注释该接口，这是 Java 8 对自定义函数接口的约定。确保该接口只有一个抽象方法。该约定清楚地表明该接口应接收 lambda 表达式。当编译器看到该注释时，它会验证该接口是否只有一个抽象方法。 使用 @FunctionalInterface 注释可以确保，如果在未来更改该接口时意外违反抽象方法数量规则，您会获得错误消息。这很有用，因为您会立即发现问题，而不是留给另一位开发人员在以后处理它。没有人希望在将 lambda 表达式传递给其他人的自定义接口时获得错误消息。 创建自定义函数接口作为一个示例，我们将创建一个 Order 类，它有一系列 OrderItem 以及一个转换并输出它们的方法。我们首先创建一个接口。 下面的代码将创建一个 Transformer 函数接口。 1234@FunctionalInterfacepublic interface Transformer&lt;T&gt; &#123; T transform(T input);&#125; 该接口用 @FunctionalInterface 注释做了标记，表明它是一个函数接口。因为该注释包含在 java.lang 包中，所以没有必要导入。该接口有一个名为 transform 的方法，后者接受一个参数化为 T 类型的对象，并返回一个相同类型的转换后对象。转换的语义将由该接口的实现来决定。 这是 OrderItem 类： 1234567891011121314public class OrderItem &#123; private final int id; private final int price; public OrderItem(int theId, int thePrice) &#123; id = theId; price = thePrice; &#125; public int getId() &#123; return id; &#125; public int getPrice() &#123; return price; &#125; public String toString() &#123; return String.format("id: %d price: %d", id, price); &#125;&#125; OrderItem 是一个简单的类，它有两个属性：id 和 price，以及一个 toString 方法。 现在来看看 Order 类。 1234567891011121314151617import java.util.*;import java.util.stream.Stream; public class Order &#123; List&lt;OrderItem&gt; items; public Order(List&lt;OrderItem&gt; orderItems) &#123; items = orderItems; &#125; public void transformAndPrint( Transformer&lt;Stream&lt;OrderItem&gt;&gt; transformOrderItems) &#123; transformOrderItems.transform(items.stream()) .forEach(System.out::println); &#125;&#125; transformAndPrint 方法接受 Transform]]></content>
      <categories>
        <category>编程</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java正则表达式]]></title>
    <url>%2F2018%2F07%2F20%2Fjava%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式处理字符串时，有很多较为复杂的字符串用普通的字符串处理函数无法干净的完成。比如说，可能需要验证一个Email地址是否合法，为此需要查看许多不容易检查的规则。这正是正则表达式的用武之地。正则表达式是功能强大而简明的字符组，其中可以包含大量的逻辑，特别值得一提的是正则表达式相当简短。 正则表达式定义了字符串的模式。正则表达式可以用来搜索、编辑或处理文本。正则表达式并不仅限于某一种语言，但是在每种语言中有细微的差别。 正则表达式语法 字符 描述 \ 将下一字符标记为特殊字符、文本、反向引用或八进制转义符。例如，”n”匹配字符”n”。”\n”匹配换行符。序列”\\\\”匹配”\\”，”\(“匹配”(“。 ^ 匹配输入字符串开始的位置。如果设置了 RegExp 对象的 Multiline 属性，^ 还会与”\n”或”\r”之后的位置匹配。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\n”或“\r”之前的位置。 * 匹配前面的子表达式任意次。例如，zo能匹配“z”，“zo”以及“zoo”。等价于{0,}。 + 匹配前面的子表达式一次或多次(大于等于1次）。例如，“zo+”能匹配“zo”以及“zoo”，但不能匹配“z”。+等价于{1,}。 \? 匹配前面的子表达式零次或一次。 例如，“do(es)?”可以匹配“do”或“does”中的“do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配“Bob”中的“o”，但是能匹配“food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。 例如，“o{2,}”不能匹配“Bob”中的“o”，但能匹配“foooood”中的所有o。“o{1,}”等价于“o+”。“o{0,}”则等价于“o*”。 {n,m} m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配“fooooood”中的前三个o。“o{0,1}”等价于“o?”。请注意在逗号和两个数之间不能有空格。 \? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，“o+?”将匹配单个“o”，而“o+”将匹配所有“o”。 .点 匹配除“\r\n”之外的任何单个字符。要匹配包括“\r\n”在内的任何字符，请使用像“[\s\S]”的模式。 (pattern) 匹配pattern并获取这一匹配。 所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用”(“或者”)“。 (?:pattern) 匹配pattern但不获取匹配结果， 也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“(\竖线)”来组合一个模式的各个部分是很有用。例如“industr(?:y\竖线ies)”就是一个比“industry\竖线industries”更简略的表达式。 (?=pattern) 正向肯定预查， 在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95\竖线98\竖线NT\竖线2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 (?!pattern) 正向否定预查， 在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95\竖线98\竖线NT\竖线2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。 (?&lt;=pattern) 反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?&lt;=95\竖线98\竖线NT\竖线2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。 (?&lt;!pattern) 反向否定预查，与正向否定预查类似，只是方向相反。例如“(?&lt;!95\竖线98\竖线NT\竖线2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。 x\竖线y 匹配x或y。 例如，“z\竖线food”能匹配“z”或“food”或”zood”(此处请谨慎)。“(z\竖线f)ood”则匹配“zood”或“food”。 [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配“plain”中的“a”。 xyz 负值字符集合。匹配未包含的任意字符。例如，“abc”可以匹配“plain”中的“plin”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配“a”到“z”范围内的任意小写字母字符。注意:只有连字符在字符组内部时,并且出现在两个字符之间时,才能表示字符的范围; 如果出字符组的开头,则只能表示连字符本身. a-z 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“a-z”可以匹配任何不在“a”到“z”范围内的任意字符。 \b 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\b就是匹配位置的）。例如，“er\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er”。 \B 匹配非单词边界。“er\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er”。 \cx 匹配由x指明的控制字符。例如，\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \d 匹配一个数字字符。等价于[0-9]。 \D 匹配一个非数字字符。等价于0-9。 \f 匹配一个换页符。等价于\x0c和\cL。 \n 匹配一个换行符。等价于\x0a和\cJ。 \r 匹配一个回车符。等价于\x0d和\cM。 \s 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。 \S 匹配任何可见字符。等价于 \f\n\r\t\v。 \t 匹配一个制表符。等价于\x09和\cI。 \v 匹配一个垂直制表符。等价于\x0b和\cK。 \w 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的”单词”字符使用Unicode字符集。 \W 匹配任何非单词字符。等价于“A-Za-z0-9_”。 \xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\x41”匹配“A”。“\x041”则等价于“\x04&amp;1”。正则表达式中可以使用ASCII编码。 \num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\1”匹配两个连续的相同字符。 \n 标识一个八进制转义值或一个向后引用。如果\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 \nm 标识一个八进制转义值或一个向后引用。如果\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\nm将匹配八进制转义值nm。 \nml 如果n为八进制数字（0-7），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 \un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\u00A9匹配版权符号（&copy;）。 \\&lt; \> 匹配词（word）的开始（\&lt;）和结束（>）。例如正则表达式\能够匹配字符串”for the wise”中的”the”，但是不能匹配字符串”otherwise”中的”the”。注意：这个元字符不是所有的软件都支持的。 \( \) 将 \( 和 \) 之间的表达式定义为“组”（group），并且将匹配这个表达式的字符保存到一个临时区域（一个正则表达式中最多可以保存9个），它们可以用 \1 到\9 的符号来引用。 \竖线 将两个匹配条件进行逻辑“或”（Or）运算。例如正则表达式(him\竖线her) 匹配”it belongs to him”和”it belongs to her”，但是不能匹配”it belongs to them.”。注意：这个元字符不是所有的软件都支持的。 + 匹配1或多个正好在它之前的那个字符。例如正则表达式9+匹配9、99、999等。注意：这个元字符不是所有的软件都支持的。 ? 匹配0或1个正好在它之前的那个字符。注意：这个元字符不是所有的软件都支持的。 {i} {i,j} 匹配指定数目的字符，这些字符是在它之前的表达式定义的。例如正则表达式A[0-9]{3} 能够匹配字符”A”后面跟着正好3个数字字符的串，例如A123、A348等，但是不匹配A1234。而正则表达式[0-9]{4,6} 匹配连续的任意4个、5个或者6个数字 java使用方法String 类有三个基本操作使用正则： 匹配： matches()切割： split()替换： replaceAll() 12345678910111213141516171819202122232425262728293031323334//以空格分割String str1 = "1 2 3 4 54 5 6";String[] numbers = str1.split(" +");for (String temp : numbers) &#123; System.out.println(temp);&#125;// 替换，替换所有的数字为*String str2 = "abd123:adad46587:asdadasadsfgi#%^^9090";System.out.println(str2.replaceAll("[0-9]", "*"));System.out.println(str2.replaceAll("\\d", "*"));// 匹配匹配邮箱String mail1 = "ababc@asa.com";String mail2 = "ababc@asa.com.cn";String mail3 = "ababc@asa";// String mainRegex = "[0-9a-zA-Z_]+@[0-9a-zA-Z_]++(\\.[0-9a-zA-Z_]+&#123;2,4&#125;)+";String mainRegex = "\\w+@\\w+(\\.\\w&#123;2,4&#125;)+";System.out.println(mail1.matches(mainRegex));//trueSystem.out.println(mail2.matches(mainRegex));//trueSystem.out.println(mail3.matches(mainRegex));//falsejava中正则匹配的对象：pattern: Pattern Pattern.complie(regexString) Macther Pattern.matches(regexString)Matcher： boolean matcher.find() //查找下一个匹配对象 String matcher.guorp() //返回整个匹配模式匹配到的结果 boolean matcher.matches() //尝试将整个区域与模式匹配 int matcher.groupCount() //返回匹配规则的分组，如：(aa)(bb)：这表示两组 String matcher.group(int group) //返回匹配对象对应分组的匹配结果 MatcheResult matcher.toMatchResult() //将匹配结果一MatchResult的形式返回 场景一：仅仅使用Matcher对象来匹配想要的字符串1234567891011121314// 匹配出3个字符的字符串String str = "abc 124 ewqeq qeqe qeqe qeqe aaaa fs fsdfs d sf sf sf sf sfada dss dee ad a f s f sa a'lfsd;'l";Pattern pt = Pattern.compile("\\b\\w&#123;3&#125;\\b");Matcher match = pt.matcher(str);while (match.find()) &#123; System.out.println(match.group());&#125;// 匹配出邮箱地址String str2 = "dadaadad da da dasK[PWEOO-123- DASJAD@DHSJK.COM DADA@DAD.CN =0KFPOS9IR23J0IS ADHAJ@565@ADA.COM.CN shuqi@162.com UFSFJSFI-SI- ";Pattern pet2 = Pattern.compile("\\b\\w+@\\w+(\\.\\w&#123;2,4&#125;)+\\b");Matcher match2 = pet2.matcher(str2);while (match2.find()) &#123; System.out.println(match2.group());&#125; 场景二：匹配规则中包含匹配组，要求匹配得到相应的匹配组的数据12345678910111213String sr = "dada ada adad adsda ad asdda adr3 fas daf fas fdsf 234 adda";//包含两个匹配组，一个是三个字符，一个是匹配四个字符Pattern pet = Pattern.compile("\\b(\\w&#123;3&#125;) *(\\w&#123;4&#125;)\\b");Matcher match = pet.matcher(sr);int countAll = match.groupCount();//2 while (match.find()) &#123; System.out.print("匹配组结果："); for (int i = 0; i &lt; countAll; i++) &#123; System.out.print(String.format("\n\t第%s组的结果是:%s",i+1,match.group(i + 1))); &#125; System.out.print("\n匹配的整个结果:"); System.out.println(match.group());&#125; 场景三：将每次得到的结果使用MatcheResult保存12345678910111213String sr = "dada ada adad adsda ad asdda adr3 fas daf fas fdsf 234 adda";Pattern pet = Pattern.compile("\\b(\\w&#123;3&#125;) *(\\w&#123;4&#125;)\\b");Matcher match = pet.matcher(sr);MatchResult ms = null;while (match.find()) &#123; ms = match.toMatchResult(); System.out.print("匹配对象的组结果："); for (int i = 0; i &lt; ms.groupCount(); i++) &#123; System.out.print(String.format("\n\t第%s组的结果是:%s",i+1,ms.group(i + 1))); &#125; System.out.print("\n匹配的整个结果:"); System.out.println(ms.group());&#125; 参考文档： java正则表达式示例 Java正则表达式的语法与示例 Java正则表达式]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java锁和监视器]]></title>
    <url>%2F2018%2F07%2F16%2Fjava%E9%94%81%E5%92%8C%E7%9B%91%E8%A7%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[在JVM的规范中，有这么一些话： “在JVM中，每个对象和类在逻辑上都是和一个监视器相关联的，为了实现监视器的排他性监视能力，JVM为每一个对象和类都关联一个锁（内置锁），锁住了一个对象，就是获得对象相关联的监视器” 锁为监视器的实现提供了必要的支持。 java 提供了synchronized关键字来支持内置锁。synchronized 关键字可以放在方法的前面、对象的前面、类的前面。 同步方法中的锁当线程调用同步方法时，它自动获得这个方法所在对象的内在锁，并且方法返回时释放锁，如果发生未捕获的异常，也会释放锁。 当调用静态同步方法时，因为静态方法和类相关联，线程获得和这个类关联的Class对象的内在锁。使用内在锁后，把deposit方法和withdraw方法修改为同步方法，就可以避免线程干扰。 123456public synchronized void deposit(int amount) &#123;balance = balance + amount;&#125;public synchronized void withdraw(int amount) &#123;balance = balance - amount;&#125; 同步语句同步语句必须指定提供内在锁的对象，其基本用法如下： 1234567891011121314synchronized（提供锁的对象）&#123;临界代码&#125;用同步语句修改BankAccount类中的方法如下：public void deposit(int amount) &#123;synchronized (this) &#123;balance = balance + amount;&#125;&#125;public void withdraw(int amount) &#123;synchronized (this) &#123;balance = balance - amount;&#125;&#125; 同步类把synchronized关键字放在类的前面，这个类中的所有方法都是同步方法。 可重入同步线程可以获得他已经拥有的锁，运行线程多次获得同一个锁，就是可以重入（reentrant）同步。这种情况通常是同步代码直接或者间接的调用也包含了同步代码的方法，并且两个代码集都使用同一个锁。如果没有可重入同步，那么，同步代码就必须采取很多额外的预防措施避免线程阻塞自己。java java.util.concurrent 包中的 ReentrantLock 即为可重入锁 12345678910111213141516171819private Lock bankLock = new ReentrantLock();… public double getTotalBalance() &#123; bankLock.lock(); try &#123; double sum = 0; for (double a : accounts) sum += a; return sum; &#125; finally &#123; bankLock.unlock(); &#125; &#125; 参考资料 锁和监视器之间的区别 – Java并发 java 内在锁（intrinsic lock）或者监视器锁（monitor lock）]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java面试常考]]></title>
    <url>%2F2018%2F07%2F15%2Fjava%E9%9D%A2%E8%AF%95%E5%B8%B8%E8%80%83%2F</url>
    <content type="text"><![CDATA[java基础方法重载和方法重写链接：https://www.nowcoder.com/questionTerminal/7b2152a85b9a4ebab6dfda7e995a8491来源：牛客网 方法重载发生在编译器，方法重写发生在运行期。 方法重写的原则： 重写方法的方法名称、参数列表必须与原方法的相同，返回类型可以相同也可以是原类型的子类型(从Java SE5开始支持)。 重写方法不能比原方法访问性差（即访问权限不允许缩小）。 重写方法不能比原方法抛出更多的异常。 被重写的方法不能是final类型，因为final修饰的方法是无法重写的。 被重写的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行重写。 被重写的方法不能为static。如果父类中的方法为静态的，而子类中的方法不是静态的，但是两个方法除了这一点外其他都满足重写条件，那么会发生编译错误；反之亦然。即使父类和子类中的方法都是静态的，并且满足重写条件，但是仍然不会发生重写，因为静态方法是在编译的时候把静态方法和类的引用类型进行匹配。 重写是发生在运行时的，因为编译期编译器不知道并且没办法确定该去调用哪个方法，JVM会在代码运行的时候作出决定。 方法重载的原则： 方法名称必须相同。 参数列表必须不同（个数不同、或类型不同、参数类型排列顺序不同等）。 方法的返回类型可以相同也可以不相同。 仅仅返回类型不同不足以成为方法的重载。 重载是发生在编译时的，因为编译器可以根据参数的类型来选择使用哪个方法。 重写和重载的不同： 方法重写要求参数列表必须一致，而方法重载要求参数列表必须不一致。 方法重写要求返回类型必须一致(或为其子类型)，方法重载对此没有要求。 方法重写只能用于子类重写父类的方法，方法重载用于同一个类中的所有方法。 方法重写对方法的访问权限和抛出的异常有特殊的要求，而方法重载在这方面没有任何限制。 父类的一个方法只能被子类重写一次，而一个方法可以在所有的类中可以被重载多次。 重载是编译时多态，重写是运行时多态。 java构造方法 在java中，一个新的对象被创建的时候，构造方法会被调用。每一个类都有构造方法。在程序员没有给类提供构造方法的情况下，Java编译器会为这个类创建一个默认的构造方法。 Java中构造方法重载和方法重载很相似。可以为一个类创建多个构造方法。每一个构造方法必须有它自己唯一的参数列表。Java不支持像C++中那样的复制构造方法，这个不同点是因为如果你不自己写构造方法的情况下，Java不会创建默认的复制构造方法。 接口和抽象类区别java 类可以继承一个抽象类，实现多个接口，都不能被实例化 抽象类 抽象类定义为包含抽象方法的类(不包含抽象方法，仅用abstract修饰的类也可以是抽象类，只不过没有抽象方法设计成的抽象类没多大意义)，使用abstrac关键字修饰。 抽象类为继承而设计，抽象方法必须为public或protected,缺省默认为public，抽象类不能被实例化，如果一个类继承一个抽象类，必须实现所有的抽象方法，否则子类也必须定义为抽象类。 接口 接口定义为提供别人调用的方法或函数，java中使用interface定义，接口可以含有变量和方法，但是变量会被隐式指定为public static final变量，方法则会被隐式地指定为public abstrac方法，变量和方法也仅仅只能如此，否则会编译错误。 抽象类实现接口的时候，可以不实现接口的抽象方法。 区别 抽象类 接口 构造函数 有 无 普通成员变量 有 无 非abstract 方法 有 无 访问修饰符 public, protected 默认也只能是public 静态方法 有 无 静态成员变量 访问类型任意 默认也只能是public static final 其他 单继承 多实现 语法层面讲 1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法；2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；3）接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法；4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。 设计层面上的区别 1）抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。此时可以将 飞行 设计为一个接口Fly，包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 2）设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。什么是模板式设计？最简单例子，大家都用过ppt里面的模板，如果用模板A设计了ppt B和ppt C，ppt B和ppt C公共的部分就是模板A了，如果它们的公共部分需要改动，则只需要改动模板A就可以了，不需要重新对ppt B和ppt C进行改动。而辐射式设计，比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更；而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 参考文档： https://www.cnblogs.com/dolphin0520/p/3811437.html 进程和线程的区别进程：是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态概念，竞争计算机系统资源的基本单位。线程：是进程的一个执行单元，比进程更小的独立运行的基本单位。线程也被称为轻量级进程。 一个程序至少有一个进程，一个进程至少有一个线程。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 区别： 进程和线程最大的区别在于不同的操作系统资源管理方式： 线程 进程 地址空间 同一进程的线程间共享 进程间相互独立、不可见 并发性 高 低 资源开销 小，不利于资源管理 大，方便资源管理和利用 资源拥有 共享CPU 内存 I/O 进程间资源相互独立 通信 管道，消息队列，信号量，共享存储，Socket, Streams 共享内存，同步，wait/notify,管道通信 如何选用： 对资源的管理和保护要求高，不限制开销和效率时，使用多进程。要求效率高，频繁切换时，资源的保护管理要求不是很高时，使用多线程。 同步方法和同步块的区别同步方法：默认使用this（成员方法）或者当前类的class对象（类方法）作为锁进行同步，使用关键字 synchronized修饰方法。 同步块：自定义锁，比同步方法要更细颗粒度，我们可以选择只同步会发生同步问题的部分代码而不是整个方法，用synchronized（object）{代码内容}进行修饰。 死锁定义：指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。 产生死锁的4个必要条件： 互斥条件：同一个资源只能有一个进程占有，不能有两个或者两个以上的占有。 不可抢占条件：在一个进程所获取的资源在未使用完毕之前，资源申请者不能强行的从资源占有者手中抢夺资源，资源只能由占有者释放。 请求和保持条件：进程已经占有了一个资源，但是有申请新的资源；但是新申请的资源已经被别的进程占有了，此时该进程就会阻塞，但是在获取申请的资源之前他还会一直占有已占有的那个资源。 循环等待条件：存在一个循环等待序列，p1等待p2,p2等待p3,p3等待p1。形成一个进程循环等待。 如何避免死锁： (1)打破互斥条件：允许进程同时访问某些资源，但是，有的资源不允许被同时访问，就像打印机，这是由资源的本身来决定的，所以这个方法并没有什么实用的价值。 (2)打破不可抢占的条件：就是说允许进程强行从资源的占有者那里抢夺资源。这种方法实现起来很困难，会降低性能。 (3)打破占有申请条件：可以实现资源预先分配策略，在进程运行前一次性向系统申请他所需要的全部资源。如果进程所需的资源不能满足，则不分配任何资源，进程暂时不运行。(问题：1.在很多时候，一个进程在执行之前不可能知道它所有的全部资源，进程在执行的过程中，是动态的。2.资源利用率低。3.降低进程的并发性，因为资源有效，有加上存在浪费，能分配的所需全部资源的进程个数必然很少。) (4)打破循环等待条件：实行资源的有序分配策略，把资源事先分类编号，按号分配，使进程在申请，占用资源时候不能形成环路，所有进程对资源的请求必须严格按照资源号递增的顺序提出，进程占用了小号的资源，才能申请大号资源。就会形成环路。(缺点：限制进程对资源的请求，同时对系统中的所有资源合理编号也是很有困难的，增加额外的系统开销。) 死锁例子： 进程A占有对象1的锁，进程B占有对象2的锁，进程A需要对象2的锁才能继续执行，所以进程A会等待进程B释放对象2的锁，而进程B需要对象1的锁才能继续执行，同样会等待进程A释放对象1的锁，由于这两个进程都不释放已占有的锁，所以导致他们进入无限等待中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// LockTest.javaimport java.util.Date; public class LockTest &#123; public static String obj1 = "obj1"; public static String obj2 = "obj2"; public static void main(String[] args) &#123; LockA la = new LockA(); new Thread(la).start(); LockB lb = new LockB(); new Thread(lb).start(); &#125;&#125;class LockA implements Runnable&#123; public void run() &#123; try &#123; System.out.println(new Date().toString() + " LockA 开始执行"); while(true)&#123; synchronized (LockTest.obj1) &#123; System.out.println(new Date().toString() + " LockA 锁住 obj1"); Thread.sleep(3000); // 此处等待是给B能锁住机会 synchronized (LockTest.obj2) &#123; System.out.println(new Date().toString() + " LockA 锁住 obj2"); Thread.sleep(60 * 1000); // 为测试，占用了就不放 &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;class LockB implements Runnable&#123; public void run() &#123; try &#123; System.out.println(new Date().toString() + " LockB 开始执行"); while(true)&#123; synchronized (LockTest.obj2) &#123; System.out.println(new Date().toString() + " LockB 锁住 obj2"); Thread.sleep(3000); // 此处等待是给A能锁住机会 synchronized (LockTest.obj1) &#123; System.out.println(new Date().toString() + " LockB 锁住 obj1"); Thread.sleep(60 * 1000); // 为测试，占用了就不放 &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 程序输出： Mon Jul 16 22:40:18 CST 2018LockA 开始执行Mon Jul 16 22:40:18 CST 2018 LockA 锁住 obj1Mon Jul 16 22:40:18 CST 2018 LockB 开始执行Mon Jul 16 22:40:18 CST 2018 LockB 锁住 obj2 如何解决死锁： 为了解决这个问题，我们不使用显示的去锁，我们用信号量去控制。 信号量可以控制资源能被多少线程访问，这里我们指定只能被一个线程访问，就做到了类似锁住。而信号量可以指定去获取的超时时间，我们可以根据这个超时时间，去做一个额外处理。 对于无法成功获取的情况，一般就是重复尝试，或指定尝试的次数，也可以马上退出。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.Date;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit; public class UnLockTest &#123; public static String obj1 = "obj1"; public static final Semaphore a1 = new Semaphore(1); public static String obj2 = "obj2"; public static final Semaphore a2 = new Semaphore(1); public static void main(String[] args) &#123; LockAa la = new LockAa(); new Thread(la).start(); LockBb lb = new LockBb(); new Thread(lb).start(); &#125;&#125;class LockAa implements Runnable &#123; public void run() &#123; try &#123; System.out.println(new Date().toString() + " LockA 开始执行"); while (true) &#123; if (UnLockTest.a1.tryAcquire(1, TimeUnit.SECONDS)) &#123; System.out.println(new Date().toString() + " LockA 锁住 obj1"); if (UnLockTest.a2.tryAcquire(1, TimeUnit.SECONDS)) &#123; System.out.println(new Date().toString() + " LockA 锁住 obj2"); Thread.sleep(60 * 1000); // do something &#125;else&#123; System.out.println(new Date().toString() + "LockA 锁 obj2 失败"); &#125; &#125;else&#123; System.out.println(new Date().toString() + "LockA 锁 obj1 失败"); &#125; UnLockTest.a1.release(); // 释放 UnLockTest.a2.release(); Thread.sleep(1000); // 马上进行尝试，现实情况下do something是不确定的 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;class LockBb implements Runnable &#123; public void run() &#123; try &#123; System.out.println(new Date().toString() + " LockB 开始执行"); while (true) &#123; if (UnLockTest.a2.tryAcquire(1, TimeUnit.SECONDS)) &#123; System.out.println(new Date().toString() + " LockB 锁住 obj2"); if (UnLockTest.a1.tryAcquire(1, TimeUnit.SECONDS)) &#123; System.out.println(new Date().toString() + " LockB 锁住 obj1"); Thread.sleep(60 * 1000); // do something &#125;else&#123; System.out.println(new Date().toString() + "LockB 锁 obj1 失败"); &#125; &#125;else&#123; System.out.println(new Date().toString() + "LockB 锁 obj2 失败"); &#125; UnLockTest.a1.release(); // 释放 UnLockTest.a2.release(); Thread.sleep(10 * 1000); // 这里只是为了演示，所以tryAcquire只用1秒，而且B要给A让出能执行的时间，否则两个永远是死锁 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; finalize()方法finalize()方法什么时候被调用?析构函数(finalization)的目的是什么？ 调用时机：当垃圾回收器要宣告一个对象死亡时，至少要经过两次标记过程：如果对象在进行可达性分析后发现没有和GC Roots相连接的引用链，就会被第一次标记，并且判断是否执行finalizer( )方法，如果对象覆盖finalizer()方法且未被虚拟机调用过，那么这个对象会被放置在F-Queue队列中，并在稍后由一个虚拟机自动建立的低优先级的Finalizer线程区执行触发finalizer( )方法，但不承诺等待其运行结束。 finalization的目的：对象逃脱死亡的最后一次机会。（只要重新与引用链上的任何一个对象建立关联即可。）但是不建议使用，运行代价高昂，不确定性大，且无法保证各个对象的调用顺序。可用try-finally或其他替代。 GC决定回收某对象的时候，就会运行对象的finalize()方法，一般情况下不用重写该方法。 最主要的用途是，回收特殊渠道申请的内存，native方法。例如使用JNI调用C++程序的时候，finalize()方法就回收这部分的内存。 System.gc()和Runtime.gc()会做什么事情？这两个方法用来提示JVM要进行垃圾回收。但是，立即开始还是延迟进行垃圾回收是取决于JVM的。 JVM内存分配策略、各个代区、GC类别各个代区 年轻代(Young Generation)： 所有新生对象首先放在年轻代中。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。年轻代分三个区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个 Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制“年老区(Tenured)”。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor去过来的对象。而且，Survivor区总有一个是空的。同时，根据程序需要，Survivor区是可以配置为多个的（多于两个），这样可以增加对象在年轻代中的存在时间，减少被放到年老代的可能。 年老代(Old Generation)： 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 永久代(Permanent Generation): 用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置. GC类别 Minor GC: 从年轻代空间（包括 Eden 和 Survivor 区域）回收内存被称为 Minor GC, 因大多数新生对象生命周期很短，所以MinorGC通常很频繁，回收速度也较快; FullGC: 针对整个新生代、老生代、元空间（metaspace，java8以上版本取代永久代(perm gen)）的全局范围的GC, 执行频率低，回收速度慢。 Major GC: 定义不清楚，有的地方同FullGC（深入理解java虚拟机），有的地方指的是清理永久代。 分配策略 对象优先在Eden分配 一般情况下，对象会在新生代的Eden区分配，Eden区没有足够空间时，虚拟机会 发起一次MinorGC；当MinorGC时，若无法放入survivor空间，就会再通过分配担保机制转移到老年代中； 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。-XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 长期存活的对象进入老年代 通过 -XX:MaxTenuringThreshold参数设置；每MinorGC一次还存活在Survivor中，则年龄加1； 动态对象年龄判定 虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 区中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的；如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC，尽管这次 Minor GC 是有风险的；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那这时也要改为进行一次 Full GC。 触发条件MinorGC: Eden区满时触发；FullGC也会伴随有MinorGC；通常会使得Old gen变大； FullGC：a. MinorGC触发前，检查历次进入老年代的平均大小，若小于则FullGC；b. 如果有永久代（perm gen），在不足哆分配时，触发FullGC；c. 调用System.gc()，提醒JVM FullGC，但不可控； 串行(serial)收集器和吞吐量(throughput)收集器的区别是什么？串行GC：整个扫描和复制过程均采用单线程的方式，相对于吞吐量GC来说简单；适合于单CPU、客户端级别。串行收集器在GC时会停止其他所有工作线程（stop-the-world），CPU利用率是最高的，所以适用于要求高吞吐量（throughput）的应用，但停顿时间（pause time）会比较长，所以对web应用来说就不适合，因为这意味着用户等待时间会加长。 吞吐量GC：采用多线程的方式来完成垃圾收集（新生代），适合于吞吐量要求较高的场合，比较适合中等和大规模的应用程序，关注点在于达到一个可控制的吞吐量，另外吞吐量收集器有自适应调节策略的能力。采用并行收集器停顿时间很短，回收效率高，适合高频率执行。 java中，对象在什么情况下被回收？当一个对象到GC Roots不可达时，在下一个垃圾回收周期中尝试回收该对象，如果该对象重写了finalize()方法，并在这个方法中成功自救(将自身赋予某个引用)，那么这个对象不会被回收。但如果这个对象没有重写finalize()方法或者已经执行过这个方法，也自救失败，该对象将会被回收。 JVM的永久代中会发生垃圾回收么？垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区(注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区) java中的异常java.lang.Throwable 是所有异常的超类 Error: 是错误Exception: 是异常 分类1. 受检查异常 从程序角度来说，是必须经过捕捉处理的异常，要么使用try…catch,要么使用throws 语句声明抛出，否则编译通不过 例如java.io.IOException 2. 不受检查异常（运行时异常，RuntimeException） 运行时发生，即时不用try…catch或者throws语句声明，会编译通过。例如java.lang.NullPointerException写程序的时候尽量避免 Error当程序发生不可控的错误是，通常是通知用户并终止程序的运行。例如OutOfMemoryError,动态链接失败，虚拟机错误等。 什么是JDBCJDBC（Java DataBase Connectivity）,是一套面向对象的应用程序接口（API），制定了统一的访问各类关系数据库的标准接口，为各个数据库厂商提供了标准的实现。通过JDBC技术，开发人员可以用纯Java语言和标准的SQL语句编写完整的数据库应用程序，并且真正地实现了软件的跨平台性。 通常情况下使用JDBC完成以下操作：1.同数据库建立连接；2.向数据库发送SQL语句；3.处理从数据库返回的结果； JDBC具有下列优点：1.JDBC与ODBC(Open Database Connectivity，即开放数据库互连）十分相似，便于软件开发人员理解；2.JDBC使软件开发人员从复杂的驱动程序编写工作中解脱出来，可以完全专注于业务逻辑开发；3.JDBC支持多种关系型数据库，大大增加了软件的可移植性；4.JDBC API是面向对象的，软件开发人员可以将常用的方法进行二次封装，从而提高代码的重用性； JDBC驱动：JDBC驱动提供了特定厂商对JDBC API接口类的实现，驱动必须要提供java.sql包下面这些类的实现：Connection, Statement, PreparedStatement,CallableStatement, ResultSet和Driver。 一句话总结，在使用jdbc前，应该保证相应的Driver类已经被加载到jvm中，并且完成了类的初始化工作就行了 1234567Class.forName("com.mysql.jdbc.Driver" ); // 初始化参数指定的类，并返回此类对应的对象com.mysql.jdbc.Driver driver = new com.mysql.jdbc.Driver();ClassLoader cl = new ClassLoader(); cl.loadClass("com.mysql.jdbc.Driver" ); // 以上三种方法其实都可以加载驱动// 加载完驱动就可以使用驱动管理器来建立链接了Connection con = DriverManager.getConnection(url,user,psw); java RMIjava RMI(Remote Method Invocation), java的远程方法调用是java API对远程过程调用（RPC）提供的面向对象的等价形式，能够让某个java虚拟机上的对象像调用本地对象一样调用另一个java虚拟机中对象的方法，支持直接传输序列化的java对象和分布式垃圾回收。 RMI远程调用步骤： 1，客户对象调用客户端辅助对象上的方法 2，客户端辅助对象打包调用信息（变量，方法名），通过网络发送给服务端辅助对象 3，服务端辅助对象将客户端辅助对象发送来的信息解包，找出真正被调用的方法以及该方法所在对象 4，调用真正服务对象上的真正方法，并将结果返回给服务端辅助对象 5，服务端辅助对象将结果打包，发送给客户端辅助对象 6，客户端辅助对象将返回值解包，返回给客户对象 7，客户对象获得返回值 对于客户对象来说，步骤2-6是完全透明的 参考文档： Java RMI详解 RMI 体系的基本原则：RMI体系结构是基于一个非常重要的行为定义和行为实现相分离的原则。RMI允许定义行为的代码和实现行为的代码相分离，并且运行在不同的JVM上。 RMI体系结构分层存根和骨架层(Stub and Skeleton layer)：这一层对程序员是透明的，它主要负责拦截客户端发出的方法调用请求，然后把请求重定向给远程的RMI服务。 远程引用层(Remote Reference Layer)：RMI体系结构的第二层用来解析客户端对服务端远程对象的引用。这一层解析并管理客户端对服务端远程对象的引用。连接是点到点的。 传输层(Transport layer)：这一层负责连接参与服务的两个JVM。这一层是建立在网络上机器间的TCP/IP连接之上的。它提供了基本的连接服务，还有一些防火墙穿透策略。 分布式垃圾回收DGC叫做分布式垃圾回收。RMI使用DGC来做自动垃圾回收。因为RMI包含了跨虚拟机的远程对象的引用，垃圾回收是很困难的。DGC使用引用计数算法来给远程对象提供自动内存管理。 概念： 1)Java虚拟机中，一个远程对象不仅会被本地虚拟机内的变量引用，还会被远程引用。 2)只有当一个远程对象不受到任何本地引用和远程引用，这个远程对象才会结束生命周期。 说明： 1)服务端的一个远程对象在3个地方被引用： 1&gt;服务端的一个本地对象持有它的本地引用 2&gt;服务端的远程对象已经注册到rmiregistry注册表中，也就是说，rmiregistry注册表持有它的远程引用。 3&gt;客户端获得远程对象的存根对象，也就是说，客户端持有它的远程引用。 2)服务端判断客户端是否持有远程对象引用的方法： 1&gt;当客户端获得一个服务端的远程对象的存根时，就会向服务器发送一条租约(lease)通知，以告诉服务器自己持有了这个远程对象的引用了。 2&gt;客户端定期地向服务器发送租约通知，以保证服务器始终都知道客户端一直持有着远程对象的引用。 3&gt;租约是有期限的，如果租约到期了，服务器则认为客户端已经不再持有远程对象的引用了。 Serializaton 和 Deserialization序列化和反序列化的对象必须实现serializable 接口，该接口没有抽象方法，只是为了标注对象可以被序列化，然后使用一个输出流就可以构造一个ObjectOutputStream(对象)，调用writeObject方法就可以将任意对象序列化输出，恢复的话则使用输出流。 12345678910111213141516171819202122232425262728293031323334353637383940import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;/** * 对象序列化测试. * * @author Xiong Raorao * @since 2018-07-17-11:05 */public class SerializeTest &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; A a = new A(); a.setName("hhh"); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("D:\\oos")); oos.writeObject(a); oos.close(); ObjectInputStream ois = new ObjectInputStream(new FileInputStream("D:\\oos")); A aa = (A) ois.readObject(); System.out.println(aa.getName()); &#125; static class A implements Serializable &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125;&#125; Java Web什么是 Servlet?Servlet是用来处理客户端请求并产生动态网页内容的Java类。Servlet主要是用来处理或者是存储HTML表单提交的数据，产生动态内容，在无状态的HTTP协议下管理状态信息。 servlet的体系结构：所有的Servlet都必须要实现的核心的接口是javax.servlet.Servlet。每一个Servlet都必须要直接或者是间接实现这个接口，或者是继承javax.servlet.GenericServlet或者javax.servlet.http.HTTPServlet。最后，Servlet使用多线程可以并行的为多个请求服务。 参考文档： servlet的体系结构 servlet的生命周期和工作原理 初始化阶段，调用init()方法 响应用户请求的方法，调用service()方法，自己实现doGet和doPost方法 终止阶段，调用destroy()方法 1：Web Client向Servlet容器(tomcat)发出Http请求2：Servlet容器接收Web Client的请求3：Servlet容器创建一个HttpRequest对象，将Web Client请求的信息封装到这个对象中。4：Servlet容器创建一个HttpResponse对象5：Servlet容器调用HttpServlet对象的service方法，把HttpRequest对象与HttpResponse对象作为参数传递给HttpServlet对象。6：HttpServlet调用HttpRequest对象的有关方法，获取Http请求信息7：HttpServlet调用HttpResponse对象的有关方法，生成响应数据8：Servlet容器把HttpServlet的响应结果传入Web Client。 HTTP响应结构http响应由三个部分组成： 状态码(Status Code)：描述了响应的状态。可以用来检查是否成功的完成了请求。请求失败的情况下，状态码可用来找出失败的原因。如果Servlet没有返回状态码，默认会返回成功的状态码HttpServletResponse.SC_OK。 HTTP头部(HTTP Header)：它们包含了更多关于响应的信息。比如：头部可以指定认为响应过期的过期日期，或者是指定用来给用户安全的传输实体内容的编码格式。如何在Serlet中检索HTTP的头部看这里。 主体(Body)：它包含了响应的内容。它可以包含HTML代码，图片，等等。主体是由传输在HTTP消息中紧跟在头部后面的数据字节组成的。 cookie和sessioncookie是web服务器发送给浏览器的一块信息。浏览器会在本地文件中给每一个web服务器存储cookie。以后浏览器在给特定的Web服务器发请求的时候，同时会发送所有为该服务器存储的cookie，例如密码填充等。 seesion是存储在服务器端的信息，客户端访问服务器的时候，服务器把客户端信息已某种形式记录在服务器上。客户端浏览器再次访问服务器的时候， 服务器只需要在session中查找该用户的状态就可以了。如果说cookie机制是通过检查客户身上的“通信证”，那么session机制就是通过检查服务器上的“客户明细表”来确认客户身份。 区别： 存储位置：cookie在客户端浏览器存储，session在服务器端存储 安全性：cookie安全性低，session安全性高 数据量：单个cookie只能保存不大于4k的数据，且只能是string，很多浏览器一个站点最多保存20个cookie，session可以存储任意的java对象 将登陆信息等重要信息存放为SESSION其他信息如果需要保留，可以放在COOKIE中 HTTP隧道HTTP隧道是一种利用HTTP或者是HTTPS把多种网络协议封装起来进行通信的技术。因此，HTTP协议扮演了一个打通用于通信的网络协议的管道的包装器的角色。把其他协议的请求掩盖成HTTP的请求就是HTTP隧道。 sendRedirect()和forward()方法有什么区别？sendRedirect()方法会创建一个新的请求，而forward()方法只是把请求转发到一个新的目标上。重定向(redirect)以后，之前请求作用域范围以内的对象就失效了，因为会产生一个新的请求，而转发(forwarding)以后，之前请求作用域范围以内的对象还是能访问的。一般认为sendRedirect()比forward()要慢。 URL编码和URL解码URL编码是负责把URL里面的空格和其他的特殊字符替换成对应的十六进制表示，反之就是解码。 JSP如何被处理的客户端通过浏览器发送jsp请求，服务器端接受到请求后，判断是否是第一次请求该页面，或者该页面是否改变，若是，服务器将jsp页面翻译为servlet，jvm将servlet编译为.class文件，字节码文件加载到服务器内存上执行，服务器将处理结果以.html页面的形式返回给客户端，若该页面不是第一次请求，则省略翻译和编译的步骤，直接执行。]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见查找算法]]></title>
    <url>%2F2018%2F07%2F14%2F%E5%B8%B8%E8%A7%81%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算。这里介绍常见的几种查找算法。 查找定义：根据给定的某个值，在查找变中确定一个其关键字等于给定值的数据元素 查找算法分类： （1）静态查找和动态查找:静态或则动态都是针对查找表而言的，静态表指的是只作查找的表，动态表指查找表中有删除和插入操作的表 静态表查找方法：顺序表查找、有序表查找、线性索引查找动态表查找方法：二叉排序树、平衡二叉树（AVL树）、多路查找树（B树） （2）无序查找和有序查找；无序查找：被查找的数列有序无序均可。有序查找：被查找的数列必须为有序数列 平均查找长度（Average Search Length, ASL）：需和制定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度 对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和。Pi：查找表中第i个数据元素的概率。Ci：找到第i个数据元素时已经比较过的次数。 顺序查找说明：顺序查找适合于存储结构为顺序或者链接存储的线性表 基本思想：顺序查找也称为线性查找，是在一个已知无(或有序）序队列中找出与给定关键字相同的数的具体位置。原理是让关键字与队列中的数从第一个开始逐个比较，直到找出与给定关键字相同的数为止，它的缺点是效率低下。 复杂度： O(N) 12345678public static int sequenceSearch(int[] array, int des) &#123; for (int i = 0, len = array.length; i &lt; len; i++) &#123; if (array[i] == des) &#123; return i; &#125; &#125; return -1;&#125; 二分查找说明：二分查找为有序查找，数列必须有序，否则需要先进性排序操作 基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度：最坏为 \log_2 (n+1),期望时间复杂度为O(\log_2 n)) 12345678910111213141516171819202122/** * 二分查找 * @param arr 目标数组 * @param key 关键字 * @return 目标数组的索引 */public static int binarySearch(int[] arr, int key) &#123; return binarySearch(arr, key, 0, arr.length - 1);&#125;private static int binarySearch(int[] arr, int key, int low, int hight) &#123; int mid = (low + hight) / 2; if (arr[mid] == key) &#123; return mid; &#125; else if (arr[mid] &lt; key) &#123; low = mid + 1; return binarySearch(arr, key, low, hight); &#125; else &#123; hight = mid - 1; return binarySearch(arr, key, low, hight); &#125;&#125; 插值查找基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率，当然插值查找也属于有序查找。 对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为O(\log_2 (\log_2 n)) 123456789101112131415161718192021222324/** * 插入查找 * * @param arr 目标数组 * @param key 关键字 * @return 目标数组的索引 */ public static int insertSearch(int[] arr, int key) &#123; return insertSearch(arr, key, 0, arr.length - 1); &#125; private static int insertSearch(int[] arr, int key, int low, int high) &#123; // 把二分查找的比例参数改为自适应的，让mid值的变化更加靠近关键字key，间接减少比较次数 int mid = low + (key - arr[low]) / (arr[high] - arr[low]) * (high - low); if (arr[mid] == key) &#123; return mid; &#125; else if (arr[mid] &lt; key) &#123; low = mid + 1; return insertSearch(arr, key, low, high); &#125; else &#123; high = mid - 1; return insertSearch(arr, key, low, high); &#125; &#125; 斐波那契查找黄金分割：是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1:0.618或1.618:1。 大家记不记得斐波那契数列：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89…….（从第三个数开始，后边每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。 基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素比较，比较结果分三种情况： 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1; 3）&lt;，high=mid-1。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契数小1，及n=F(k)-1; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1,k-=2; 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为n-(F(k-1))= Fk-1-F(k-1)=Fk-F(k-1)-1=F(k-2)-1个，所以可以递归的应用斐波那契查找。 3）&lt;，high=mid-1,k-=1。 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归 的应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为O(\log_2 n)，且其期望复杂度也为O(\log_2 n)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// 斐波那契查找.cpp #include "stdafx.h"#include &lt;memory&gt;#include &lt;iostream&gt;using namespace std;const int max_size=20;//斐波那契数组的长度/*构造一个斐波那契数组*/ void Fibonacci(int * F)&#123; F[0]=0; F[1]=1; for(int i=2;i&lt;max_size;++i) F[i]=F[i-1]+F[i-2];&#125;/*定义斐波那契查找法*/ int FibonacciSearch(int *a, int n, int key) //a为要查找的数组,n为要查找的数组长度,key为要查找的关键字&#123; int low=0; int high=n-1; int F[max_size]; Fibonacci(F);//构造一个斐波那契数组F int k=0; while(n&gt;F[k]-1)//计算n位于斐波那契数列的位置 ++k; int * temp;//将数组a扩展到F[k]-1的长度 temp=new int [F[k]-1]; memcpy(temp,a,n*sizeof(int)); for(int i=n;i&lt;F[k]-1;++i) temp[i]=a[n-1]; while(low&lt;=high) &#123; int mid=low+F[k-1]-1; if(key&lt;temp[mid]) &#123; high=mid-1; k-=1; &#125; else if(key&gt;temp[mid]) &#123; low=mid+1; k-=2; &#125; else &#123; if(mid&lt;n) return mid; //若相等则说明mid即为查找到的位置 else return n-1; //若mid&gt;=n则说明是扩展的数值,返回n-1 &#125; &#125; delete [] temp; return -1;&#125;int main()&#123; int a[] = &#123;0,16,24,35,47,59,62,73,88,99&#125;; int key=100; int index=FibonacciSearch(a,sizeof(a)/sizeof(int),key); cout&lt;&lt;key&lt;&lt;" is located at:"&lt;&lt;index; return 0;&#125; 树表查找1. 最简单的鼠标查找—二叉树查找算法基本思想：二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 1）若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 2）若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 3）任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。 复杂度分析：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 基于对二叉查找树进行优化，进而可以得到其他树表查找算法，如平衡树，红黑树等高效算法 2. 平衡查找树—2-3查找树（2-3 Tree)为了保证在最差的情况下也能达到logN的效率，引入了平衡查找树。因此需要保证树在插入之后始终保持平衡状态，这就是平衡查找树。 2-3查找树：和二叉树不一样，2-3树运行每个节点保存1个或者两个的值。对于普通的2节点(2-node)，他保存1个key和左右两个自己点。对应3节点(3-node)，保存两个Key，2-3查找树的定义如下： （1）要么为空，要么：（2）对于2节点，该节点保存一个key及其对应的value，以及两个指向左右节点的节点，左节点也是一个2-3节点，所有的值都比key要小，右节点也是一个2-3节点，所有的值比key要大。（3）对于3节点，该节点保存两个key及对应value，以及三个指向左中右的节点。左节点也是一个2-3节点，所有的值均比两个key中的最小的key还要小；中间节点也是一个2-3节点，中间节点的key值在两个跟节点key值之间；右节点也是一个2-3节点，节点的所有key值比两个key中的最大的key还要大。 2-3查找树的性质： 1）如果中序遍历2-3查找树，就可以得到排好序的序列； 2）在一个完全平衡的2-3查找树中，根节点到每一个为空节点的距离都相同。（这也是平衡树中“平衡”一词的概念，根节点到叶节点的最长距离对应于查找算法的最坏情况，而平衡树中根节点到叶节点的距离都一样，最坏情况也具有对数复杂度。） 复杂度分析： 2-3树的查找效率与树的高度是息息相关的。 在最坏的情况下，也就是所有的节点都是2-node节点，查找效率为lgN 在最好的情况下，所有的节点都是3-node节点，查找效率为log3N约等于0.631lgN 3. 平衡查找树—红黑树（Red-Black Tree)2-3查找树能保证在插入元素之后能保持树的平衡状态，最坏情况下即所有的子节点都是2-node，树的高度为lgn，从而保证了最坏情况下的时间复杂度。但是2-3树实现起来比较复杂，于是就有了一种简单实现2-3树的数据结构，即红黑树（Red-Black Tree）。 基本思想：红黑树的思想就是对2-3查找树进行编码，尤其是对2-3查找树中的3-nodes节点添加额外的信息。红黑树中将节点之间的链接分为两种不同类型，红色链接，他用来链接两个2-nodes节点来表示一个3-nodes节点。黑色链接用来链接普通的2-3节点。特别的，使用红色链接的两个2-nodes来表示一个3-nodes节点，并且向左倾斜，即一个2-node是另一个2-node的左子节点。这种做法的好处是查找的时候不用做任何修改，和普通的二叉查找树相同。 红黑树定义：红黑树是一种具有红色和黑色链接的平衡查找树，同时满足： 红色节点向左倾斜 一个节点不可能有两个红色链接 整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同。 还有一种定义： 红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或者黑色，除了二叉查找树的要求外，还满足如下要求： （1）节点是红色或者黑色（2）根节点是黑色（3）每个叶节点（NIL节点，空节点）是黑色的（4）每个红色节点的两个子节点都是黑色（从每个叶子到根节点的所有路径上不能有两个连续的红色节点）（5）从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点 这些约束强制了红黑树的关键性质: 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。 红黑树的性质：整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同（2-3树的第2）性质，从根节点到叶子节点的距离都相等）。 复杂度分析：最坏的情况就是，红黑树中除了最左侧路径全部是由3-node节点组成，即红黑相间的路径长度是全黑路径长度的2倍。 红黑树的平均高度大约为logn。 4. B树和B+树（B Tree/B+ Tree）平衡查找树中的2-3树以及其实现红黑树。2-3树种，一个节点最多有2个key，而红黑树则使用染色的方式来标识这两个key。 维基百科对B树的定义为“在计算机科学中，B树（B-tree）是一种树状数据结构，它能够存储数据、对其进行排序并允许以O(log n)的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。B树，概括来说是一个节点可以拥有多于2个子节点的二叉查找树。与自平衡二叉查找树不同，B树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在数据库和文件系统。 B树定义： B树可以看作是对2-3查找树的一种扩展，即他允许每个节点有M-1个子节点。 根节点至少有两个子节点 每个节点有M-1个key，并且以升序排列 位于M-1和M key的子节点的值位于M-1 和M key对应的Value之间 其它节点至少有M/2个子节点 下图是一个M=4 阶的B树: 可以看到B树是2-3树的一种扩展，他允许一个节点有多于2个的元素。B树的插入及平衡化操作和2-3树很相似，这里就不介绍了。下面是往B树中依次插入6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4的演示动画： B+树定义： B+树是对B树的一种变形树，它与B树的差异在于： 有k个子结点的结点必然有k个关键码；非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录。 如下图，是一个B+树: 下图是B+树的插入动画： B和B+树的区别在于，B+树的非叶子结点只包含导航信息，不包含实际的值，所有的叶子结点和相连的节点使用链表相连，便于区间查找和遍历。 B+ 树的优点在于： 由于B+树在内部节点上不好含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子几点上关联的数据也具有更好的缓存命中率。 B+树的叶子结点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 但是B树也有优点，其优点在于，由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 B/B+树常用于文件系统和数据库系统中，它通过对每个节点存储个数的扩展，使得对连续的数据能够进行较快的定位和访问，能够有效减少查找时间，提高存储的空间局部性从而减少IO操作。它广泛用于文件系统及数据库中，如： Windows：HPFS文件系统；Mac：HFS，HFS+文件系统；Linux：ResiserFS，XFS，Ext3FS，JFS文件系统；数据库：ORACLE，MYSQL，SQLSERVER等中。 有关B/B+树在数据库索引中的应用，请看张洋的MySQL索引背后的数据结构及算法原理这篇文章，这篇文章对MySQL中的如何使用B+树进行索引有比较详细的介绍，推荐阅读。 参考文档： B树和B+树的插入、删除图文详解 【经典数据结构】B树与B+树 MySQL索引背后的数据结构及算法原理 树表查找总结： 二叉查找树平均查找性能不错，为O(logn)，但是最坏情况会退化为O(n)。在二叉查找树的基础上进行优化，我们可以使用平衡查找树。平衡查找树中的2-3查找树，这种数据结构在插入之后能够进行自平衡操作，从而保证了树的高度在一定的范围内进而能够保证最坏情况下的时间复杂度。但是2-3查找树实现起来比较困难，红黑树是2-3树的一种简单高效的实现，他巧妙地使用颜色标记来替代2-3树中比较难处理的3-node节点问题。红黑树是一种比较高效的平衡查找树，应用非常广泛，很多编程语言的内部实现都或多或少的采用了红黑树。 除此之外，2-3查找树的另一个扩展——B/B+平衡树，在文件系统和数据库系统中有着广泛的应用。 分块查找分块查找又称索引顺序查找，它是顺序查找的一种改进方法。 算法思想：将n个数据元素”按块有序”划分为m块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须”按块有序”；即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字；而第2块中任一元素又都必须小于第3块中的任一元素，…… 算法流程： step1 先选取各块中的最大关键字构成一个索引表； step2 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 哈希查找什么是哈希表（Hash）？哈希表（Hash table，也叫散列表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 我们使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数， 也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标）相对应，于是用这个数组单元来存储这个元素；也可以简单的理解为，按照关键字为每一个元素”分类”，然后将这个元素存储在相应”类”所对应的地方。但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了”冲突”，换句话说，就是把不同的元素分在了相同的”类”之中。后面我们将看到一种解决”冲突”的简便做法。 总的来说，”直接定址”与”解决冲突”是哈希表的两大特点。 只接定址可以提高查找速度，但是需要大量存储空间，通常查找的时间复杂度越低，空间复杂度越高 数组的特点是：寻址容易，插入和删除困难； 而链表的特点是：寻址困难，插入和删除容易。 那么我们能不能综合两者的特性，做出一种寻址容易，插入删除也容易的数据结构？答案是肯定的，这就是我们要提起的哈希表。 什么是哈希函数？ 哈希函数的规则是：通过某种转换关系，使关键字适度的分散到指定大小的的顺序结构中，越分散，则以后查找的时间复杂度越小，空间复杂度越高。 算法思想：哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 算法流程： 1）用给定的哈希函数构造哈希表； 2）根据选择的冲突处理方法解决地址冲突； 常见的解决冲突的方法：拉链法和线性探测法。 3）在哈希表的基础上执行哈希查找。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析： 单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)（注意，在查找之前我们需要构建相应的Hash表）。 哈希函数的构造：采用哈希函数的关键作用是减少需要被处理的数组大小。在构造散列函数时有几点需要加以注意： 其一、散列函数的定义域必须包括需要存储的全部数据元素的关键字，而如果散列表允许有m个地址时，其值域必须在0到m-1之间； 其二、散列函数计算出来的地址应能均匀分布在整个地址空间中，若key是从关键字集合中随机抽取的一个关键字，散列函数应能以同等概率取0到m-1中的每一个值； 其三、散列函数应是简单的，能在较短的时间内计算出结果。下面我们介绍几个散列函数。 1. 直接地址法此类函数取关键字的某个线性函数值作为散列地址：Hash（key）=a*key+c （其中a、c是整常数） 这类散列函数是一对一的映射，一般不会产生冲突，但是，它要求散列地址空间的大小与关键字集合的大小相同，这种要求是很苛刻的。特别是当关键字集合很大而且又不连续时，这种方法就不太适宜。 2. 数字分析法设数据表的长度为n，数据元素的关键字是一个d位数，关键字上每一位可能有r种不同的符号。这r种不同的符号在各位上出现的概率不一定相同，可能在某些位上分布均匀，出现的机会均等；在某些位上分布不均匀，只有某几种符号经常出现。数字分析法就是根据散列表的大小，在关键字中选取某些分布均匀的若干位作为散列地址。 3. 除留余数法设散列表地址空间大小为m，取一个不大于m，但最接近于或等于m的质数p，或者选取一个不含有小于20的质因数的合数作为除数，除留余数法的散列函数为： Hash（key）=key % p （p≤m, p 避免去2的幂） 4. 乘余取整法（乘法散列法）使用此方法时，先让关键字key乘上一个常数a(0＜a＜1)，提取乘积的小数部分，然后再用整数n乘以这个值，对结果向下取整，把它作为散列地址 h(k)=n*(kA - \left \lfloor kA \right \rfloor)h(k)=\left \lfloor n(kA mod 1) \right \rfloor如何解决Hash冲突1. 链地址法数组 + 链表，把哈希到同一位置的所有元素都放到一个链表中，桶结构 创建一个存放单词链表的数组，数组内不直接存放单词，这样，当冲突发生时，新的数据项直接接到数组下标所指的链表中，这种方法叫做链地址法。 2. 开放地址法或者叫再散列法基本思想是，当产生哈希冲突时，即关键字key的地址p=hash(key)上已经有值了，那么这次以p为基础，产生另外一个哈希地址p1,如果p1不冲突了，那么就将元素key存在位置p1,如果p1也冲突，就计算hash(p1)=p2，不冲突就存在p2,冲突继续计算； 再散列有几种方式： 1&gt;线性探测再散列：冲突发生时，查看下个位置是否空，然后遍历下去找到个空的地方存放； 2&gt;二次探测再散列：冲突发生时，在表的左右进行跳跃探测，di=12 -12 22 -22….k2 -k2; 3&gt;伪随机探测再散列：di=伪随机序列； 公式如下： Hi = (H(key) + d_i) MOD m , i = 1, 2, ...., s例子：hash表长度11，哈希函数是： h(key) = key%11;那么h(47)=3,h(26)=4,h(60)=5;下一个关键字69，h(69)=3,与47冲突了 1&gt;线性探测的解决方法：往后遍历找到个空的位置 0 1 2 3 4 5 6 7 8 9 10 47 26 60 69 2&gt;二次探测再散列：下个哈希地址是h1 = (3+12)%11 = 4,冲突，再找下一个哈希地址，(3-12)%11 = 2，就放在第二个位置 3&gt;再看看伪随机数的处理办法，假设随机数是：2 5 9 ，下一个哈希地址（3+2）%11 = 5，冲突，再找下一个，（3+5）%11 = 8，就放在8的位置了。 3. 在哈希法这种方法是同时构造多个不同的哈希函数：hi = rhi(key) i=1,2…k 当哈希地址rh1(key)冲突时，再计算hi = rh2(key)….直到不再冲突 4. 建立一个公共溢出区间这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux僵尸进程]]></title>
    <url>%2F2018%2F07%2F11%2Flinux%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[linux僵尸进程原因： 父进程调用fork创建子进程后，子进程运行直至其终止，它立即从内存中移除，但进程描述符仍然保留在内存中（进程描述符占有极少的内存空间）。子进程的状态变成EXIT_ZOMBIE，并且向父进程发送SIGCHLD 信号，父进程此时应该调用 wait() 系统调用来获取子进程的退出状态以及其它的信息。在 wait 调用之后，僵尸进程就完全从内存中移除。因此一个僵尸存在于其终止到父进程调用 wait 等函数这个时间的间隙，一般很快就消失，但如果编程不合理，父进程从不调用 wait 等系统调用来收集僵尸进程，那么这些进程会一直存在内存中。 检测僵尸进程123456$ top top - 09:58:31 up 3 min, 2 users, load average: 0.76, 0.45, 0.19Tasks: 212 total, 1 running, 210 sleeping, 0 stopped, 1 zombie%Cpu(s): 6.4 us, 3.1 sy, 0.6 ni, 78.6 id, 11.2 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 4037576 total, 1664952 used, 2372624 free, 82416 buffersKiB Swap: 1998844 total, 0 used, 1998844 free. 916128 cached Mem 可以看到，我们系统中有一个僵尸进程（1 zombie）。 杀死僵尸进程1234$ ps aux | grep -w 'Z'#或者只查看特定的栏目：$ ps -A -o stat,ppid,pid,cmd | grep -e '^[Zz]'$ sudo kill -9 ppid]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见排序算法]]></title>
    <url>%2F2018%2F07%2F11%2F%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[简介 分类排序算法通常指的是内部排序算法，即数据记录在内存中进行排序。 比较排序 时间复杂度为O(nlogn) ~ O(n^2), 有冒泡排序、选择排序、 插入排序、希尔排序、归并排序、堆排序、快速排序等 非比较排序 时间复杂度可以达到O(n), 主要有：计数排序，基数排序，桶排序等。 算法复杂度 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 冒泡排序算法描述：从左到右，不断的交换逆序的相邻元素，在一轮的交换过后，可以让未排序的元素上浮到右侧。在一轮循环中，如果没有发生交换，就说明数组已经是有序的，此时可以直接退出。 1234567891011121314151617181920212223242526272829/** * 冒泡排序 */public static void bubbleSort(int[] arr) &#123; boolean hasSorted = false; for (int i = 0; i &lt; arr.length - 1 &amp;&amp; !hasSorted; i++) &#123; hasSorted = true; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j + 1] &lt; arr[j]) &#123; hasSorted = false; swap(arr, j, j + 1); &#125; &#125; &#125;&#125;private static void swap(int[] arr, int a, int b) &#123; if (arr == null || arr.length == 0) &#123; return; &#125; if (a &lt; 0 || a &gt; arr.length || b &lt; 0 || b &gt; arr.length) &#123; return; &#125; int temp = arr[a]; arr[a] = arr[b]; arr[b] = temp;&#125; 选择排序算法描述：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 123456789101112131415/** * 选择排序 */ public static void selectSort(int[] arr) &#123; int N = arr.length; for (int i = 0; i &lt; N - 1; i++) &#123; int minIndex = i; for (int j = i + 1; j &lt; N; j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; minIndex = j; &#125; &#125; swap(arr, minIndex, i); &#125; &#125; 插入排序算法描述：通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后；重复步骤2~5。 12345678910111213141516171819202122232425262728/** * 插入排序 */ public static void insertSort(int[] arr) &#123; int N = arr.length; for (int i = 1; i &lt; N; i++) &#123; int current = arr[i]; int j = i - 1; for (; j &gt;= 0 &amp;&amp; arr[j] &gt; current; j--) &#123; arr[j + 1] = arr[j]; &#125; arr[j + 1] = current; &#125; &#125; /** * 插入排序2 * * 直接相邻数据交换，不需要中间变量 */ public static void insertSort2(int[] arr) &#123; int N = arr.length; for (int i = 1; i &lt; N; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; arr[j - 1] &gt; arr[j]; j--) &#123; swap(arr, j - 1, j); &#125; &#125; &#125; 希尔排序算法描述：简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；按增量序列个数k，对序列进行k 趟排序；每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 对于大规模的数组，插入排序很慢，因为它只能交换相邻的元素，每次只能将逆序数量减少 1。希尔排序的出现就是为了改进插入排序的这种局限性，它通过交换不相邻的元素，每次可以将逆序数量减少大于 1。希尔排序使用插入排序对间隔 h 的序列进行排序。通过不断减小 h，最后令 h=1，就可以使得整个数组是有序的。 算法的核心在于间隔序列的设定 1234567891011121314151617181920/** * 希尔排序 */public static void shellSort(int[] arr) &#123; int N = arr.length; int h = 1; // 动态定义间隔 while (h &lt; N / 3) &#123; h = h * 3 + 1; // h = 1, 4, 13, 40 ... &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; N; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; arr[j - h] &gt; arr[j]; j -= h) &#123; swap(arr, j - h, j); &#125; &#125; h = h / 3; &#125;&#125; 归并排序算法描述：归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 归并排序 */ public static void mergeSort(int[] arr) &#123; mergeSort(arr, 0, arr.length - 1); &#125; private static int[] mergeSort(int[] arr, int low, int high) &#123; int mid = (low + high) / 2; if (low &lt; high) &#123; mergeSort(arr, low, mid); mergeSort(arr, mid + 1, high); merge(arr, low, mid, high); &#125; return arr; &#125; /** * 合并两个有序数组 */ private static void merge(int[] a, int low, int mid, int high) &#123; int[] temp = new int[high - low + 1]; int i = low; int j = mid + 1; int k = 0; // 把较小的数先移到新数组中 while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (a[i] &lt; a[j]) &#123; temp[k++] = a[i++]; &#125; else &#123; temp[k++] = a[j++]; &#125; &#125; // 把左边剩余的数移入数组 while (i &lt;= mid) &#123; temp[k++] = a[i++]; &#125; // 把右边边剩余的数移入数组 while (j &lt;= high) &#123; temp[k++] = a[j++]; &#125; // 把新数组中的数覆盖nums数组 for (int x = 0; x &lt; temp.length; x++) &#123; a[x + low] = temp[x]; &#125; &#125; 快速排序算法描述：归并排序将数组分为两个子数组分别排序，并将有序的子数组归并使得整个数组排序；快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。 1234567891011121314151617181920212223242526272829303132/** * 快速排序 */ public static void quickSort(int[] arr) &#123; quickSort(arr, 0, arr.length - 1); &#125; private static void quickSort(int[] arr, int left, int right) &#123; if (right &lt;= left) &#123; return; &#125; int index = partition(arr, left, right); quickSort(arr, left, index - 1); quickSort(arr, index + 1, right); &#125; private static int partition(int[] arr, int left, int right) &#123; int pivot = arr[left]; // 以第一个数为基准 while (left &lt; right) &#123; // 先从后向前比较 while (arr[right] &gt; pivot &amp;&amp; right &gt; left) &#123; right--; &#125; arr[left] = arr[right]; while (arr[left] &lt;= pivot &amp;&amp; right &gt; left) &#123; left++; &#125; arr[right] = arr[left]; &#125; arr[right] = pivot; return right; &#125; 堆排序算法简介：堆排序指利用堆这种数据结构所设计的一种排序算法。堆是一种近似完全二叉树的结构，并满足性质：以最大堆为例，其中父节点的值总是大于它的孩子节点。 堆可以用数组来表示，因为堆是完全二叉树，而完全二叉树很容易就存储在数组中。位置 k 的节点的父节点位置为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。这里不使用数组索引为 0 的位置，是为了更清晰地描述节点的位置关系。 流程： 由输入的无序数组构造一个最大堆，作为初始的无序区 把堆顶元素（最大值）和堆尾元素互换,交换之后需要进行下沉操作维持堆的有序状态 12345678910111213141516171819202122232425262728/** * 堆排序 */public static void heapSort(int[] arr) &#123; int N = arr.length - 1; for (int k = N / 2; k &gt;= 1; k--) &#123; sink(arr, k, N); &#125; while (N &gt; 1) &#123; swap(arr, 1, N--); sink(arr, 1, N); &#125;&#125;private static void sink(int[] arr, int k, int N) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; arr[j] &lt; arr[j + 1]) &#123; j++; &#125; if (!(arr[k] &lt; arr[j])) &#123; break; &#125; swap(arr, k, j); k = j; &#125;&#125; 计数排序算法描述：计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序适合分布集中的排序，如统计年龄的排序等 123456789101112131415161718192021222324252627282930/** * 计数排序 */ public static void countSort(int[] arr) &#123; int maxValue = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; &#125; &#125; countSort(arr, maxValue); &#125; private static void countSort(int[] arr, int maxValue) &#123; int[] bucket = new int[maxValue + 1]; int sortedIndex = 0; int len = arr.length; int bucketLen = maxValue + 1; for (int i = 0; i &lt; len; i++) &#123; bucket[arr[i]]++; &#125; for (int j = 0; j &lt; bucketLen; j++) &#123; while (bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; &#125; 桶排序算法原理：桶排序是计数排序的升级版，利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 设置一个定量的数组当作空桶；遍历输入数据，并且把数据一个一个放到对应的桶里去；对每个不是空的桶进行排序；从不是空的桶里把排好序的数据拼接起来。 代码实现： 123456789101112131415161718192021222324252627282930313233/** * 桶排序 */ public static void bucketSort(int[] arr, int bucketSize) &#123; int i; int minValue = arr[0]; int maxValue = arr[0]; for (i = 0; i &lt; arr.length; i++) &#123; if (arr[i] &lt; minValue) &#123; minValue = arr[i]; &#125; else if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; &#125; &#125; // 初始化桶 int bucketCount = (int) (Math.floor((maxValue - minValue) / bucketSize) + 1); List&lt;List&lt;Integer&gt;&gt; buckets = new ArrayList&lt;&gt;(); for (i = 0; i &lt; arr.length; i++) &#123; int index = (int) Math.floor((arr[i] - minValue) / bucketSize); List&lt;Integer&gt; temp = buckets.get(index); if (temp == null) &#123; buckets.add(index, Arrays.asList(arr[i])); &#125; else &#123; temp.add(arr[i]); &#125; &#125; for (i = 0; i &lt; buckets.size(); i++) &#123; //对每个桶进行排序。。。 &#125; &#125; 基数排序算法描述： 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 取得数组中的最大数，并取得位数；arr为原始数组，从最低位开始取每个位组成radix数组；对radix进行计数排序（利用计数排序适用于小范围数的特点） 12345678910111213141516171819202122232425// LSD Radix Sortvar counter = [];function radixSort(arr, maxDigit) &#123; var mod = 10; var dev = 1; for (var i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; for(var j = 0; j &lt; arr.length; j++) &#123; var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) &#123; counter[bucket] = []; &#125; counter[bucket].push(arr[j]); &#125; var pos = 0; for(var j = 0; j &lt; counter.length; j++) &#123; var value = null; if(counter[j]!=null) &#123; while ((value = counter[j].shift()) != null) &#123; arr[pos++] = value; &#125; &#125; &#125; &#125; return arr;&#125; 参考链接： 常用排序算法总结(一) 常用排序算法总结(二) 十大经典排序算法（动图演示） 各种排序算法总结和比较]]></content>
      <categories>
        <category>笔记</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome解决http自动跳转https的问题]]></title>
    <url>%2F2018%2F07%2F03%2Fchrome%E8%A7%A3%E5%86%B3http%E8%87%AA%E5%8A%A8%E8%B7%B3%E8%BD%AChttps%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1.地址栏输入： chrome://net-internals/#hsts 2.找到底部Delete domain security policies一栏，输入想处理的域名，点击delete。 3.搞定了，再次访问http域名不再自动跳转https了]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lvm磁盘合并]]></title>
    <url>%2F2018%2F07%2F03%2Flvm%E7%A3%81%E7%9B%98%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[显示逻辑卷组sudo vgdisplay 显示逻辑卷sudo lvdisplay 显示磁盘sudo fdisk -l 创建物理卷sudo pvcreate /dev/sdb 显示物理卷信息sudo pvdisplay 物理卷/dev/sdb加入卷组vg-rootsudo vgextend vg-root /dev/sdb 增大逻辑卷root 1Tsudo lvextend -L +1T /dev/ubuntu-vg/rootsudo lvextend -l +100%FREE /de/ubuntu-vg/root 调整文件系统大小sudo resize2fs /dev/ubuntu-vg/root 查看结果df -h issue: remove bad disk]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑苹果仿冒声卡驱动AppleALC]]></title>
    <url>%2F2018%2F07%2F02%2F%E9%BB%91%E8%8B%B9%E6%9E%9C%E4%BB%BF%E5%86%92%E5%A3%B0%E5%8D%A1%E9%A9%B1%E5%8A%A8AppleALC%2F</url>
    <content type="text"><![CDATA[参考教程： 黑苹果定制声卡驱动（ALC892为例) 使用AppleALC声卡仿冒驱动AppleHDA的正确姿势 傻瓜式仿冒声卡驱动第二季（仿冒ALC892) [教程] AppleALC使用和修改教程 资源链接： vit9696/AppleALC acidanthera/Lilu HDA 工具 zlib 转换器 注意事项： 该教程可以通过AppleALC修改驱动，然后通过Clover 的方式注入Layout，也可以直接在原生AppleHDA上面修改。 AppleHDA-10.14.kext.zip clover注意要设置一个参数：FixHPET,否则无法加载AppleHDA 附：ALC269VC 的confidData的数据格式 123456789id_modified:4001271C40 01271D01 01271EA0 01271F90 01471C10 01471D01 01471E17 01471F90 01470C02 01571C20 01571D10 01571E21 01571F00 01871C30 01871D10 01871E81 01871F00oneline:01271C40 01271D01 01271EA0 01271F90 01471C10 01471D01 01471E17 01471F90 01470C02 01571C20 01571D10 01571E21 01571F00 01871C30 01871D10 01871E81 01871F00 每组数字构成：Address+Node+71+[c/d/e/f]+描述数字描述数字每组一共有8个分别表示：1 优先级耳机优先级一定要低于内置扬声器，外置麦克风一定要低于内置麦克风2 lineout 为f，其余03 接口颜色4 接口为 0，表示当接口被检测到时使用。如果是笔记本的话内建的麦克风和扬声器要设成1，即当耳机插入时，内建扬声器静音，耳机0 接口被检测到就是用耳机。5 接口功能信息6 链接装置类型7 接口类型0为插入接口的，如外置麦克风、耳机等。(如果codec_dump出来有 [N/A] 的就是无用的port，数字为4。)9为给笔记本內建，像内置扬声器、内置麦克风等8 接口位置 常见问题 睡眠后无法耳机没有声音 解决方法: 解决耳机切换问题 CodeCommander.kext 上面不管用的情况，直接重新加载AppleHDA就可以了 12sudo kextunload /System/Library/Extensions/AppleHDA.kextsudo kextload /System/Library/Extensions/AppleHDA.kext]]></content>
      <categories>
        <category>笔记</category>
        <category>mac</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>黑苹果</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh反向代理新姿势]]></title>
    <url>%2F2018%2F07%2F02%2Fssh%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E6%96%B0%E5%A7%BF%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[基础知识ssh 的端口转发应用链接方向： 从应用的client到应用的server端ssh链接方向：ssh client —&gt; ssh server 本地转发(local forwarding) SSH链接方向和本地链接方向一致 ssh -L &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 本地转发会在本地机器上监听一个端口，所有访问这个端口的数据都会通过SSH隧道传输到远程主机的对应端口上, 最后将数据返回到客户端，完成整个转发过程 由于本地转发绑定的地址是lookback接口，因此绑定的local port只能在本机访问，不能让其他的client访问，如果想实现其他的client也可以访问的话，需要在ssh host 的 sshd_config 文件中添加一个参数 “GatewayPorts yes”，或者直接使用 -g 命令 远程转发(local forwarding) SSH链接方向和本地方向不一致，就是远程转发 ssh -R &lt;local port&gt;:&lt;remote host&gt;:&lt;remote port&gt; &lt;SSH hostname&gt; 远程转发会在远程主机上面监听一个端口，所有访问远程主机的端口的数据都会被转发到本地对应的端口上,ssh反向代理(隧道)技术就是这样实现的 例如实现远程服务器A的1234端口映射到本机的22端口，实现远程服务器访问本地的22端口 ssh -R 1234:localhost:22 username@ServerA 动态转发(dynamic forwarding) 指定一个本地机器 “动态的’’ 应用程序端口转发. 工作原理是这样的, 本地机器上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转发出去, 根据应用程序的协议可以判断出远程主机将和哪里连接. 目前支持 SOCKS4 协议, 将充当 SOCKS4 服务器. 只有 root 才能转发特权端口. 可以在配置文件中指定动态端口的转发. ssh -D &lt;local port&gt; &lt;SSH Server&gt; ssh 端口转发参数ssh的三个强大的端口转发命令： 转发到远端：ssh -C -f -N -g -L 本地端口:目标IP:目标端口 用户名@目标IP 转发到本地：ssh -C -f -N -g –R 本地端口:目标IP:目标端口 用户名@目标IP ssh -C -f -N -g -D listen_port user@Tunnel_Host -C：压缩数据传输。 -f ：后台认证用户/密码，通常和-N连用，不用登录到远程主机。 -N ：不执行脚本或命令，通常与-f连用。 -g ：在-L/-R/-D参数中，允许远程主机连接到建立的转发的端口，如果不加这个参数，只允许本地主机建立连接。 -L 本地端口:目标IP:目标端口 将本地机(客户机)的某个端口转发到远端指定机器的指定端口. 工作原理是这样的, 本地机器上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转发出去, 同时远程主机和 host 的 hostport 端口建立连接. 可以在配置文件中指定端口的转发. 只有 root 才能转发特权端口. IPv6 地址用另一种格式说明: port/host/hostport -R本地端口:目标IP:目标端口 将远程主机(服务器)的某个端口转发到本地端指定机器的指定端口. 工作原理是这样的, 远程主机上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转向出去, 同时本地主机和 host 的 hostport 端口建立连接. 可以在配置文件中指定端口的转发. 只有用 root 登录远程主机才能转发特权端口. IPv6 地址用另一种格式说明: port/host/hostport -p ：被登录的ssd服务器的sshd服务端口。 -D port 指定一个本地机器 “动态的’’ 应用程序端口转发. 工作原理是这样的, 本地机器上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转发出去, 根据应用程序的协议可以判断出远程主机将和哪里连接. 目前支持 SOCKS4 协议, 将充当 SOCKS4 服务器. 只有 root 才能转发特权端口. 可以在配置文件中指定动态端口的转发. 实现内网端口转发问题： 内网有服务器器A, 无公网IP，现有公网服务器B，如何实现在外网情况下链接服务器B的某个端口访问到A的服务 host 类型 ip port A 内网 192.168.1.3 1234 B 外网 www.123.com 5678 通过ssh的反向代理可以实现B服务器的5678 端口绑定在A服务器的1234端口上 12# 登录到A服务器ssh -C -f -N -g -R 5678:127.0.0.1:1234 username@www.123.com 实现内网VPN服务问题： 内网有服务器器A, 无公网IP，现有公网服务器B，如何实现在外网情况下访问到内网的所有服务 host 类型 ip port A 内网 192.168.1.3 192.168.1.0/24:* B 外网 www.123.com 5678 解决方案： shadowsocks + ssh远程转发 A机器安装shadowsock 服务端 shadowsocksR一键安装 A机器做远程转发 ssh -C -f -N -g -R 5678:127.0.0.1:shadowsocks_server_port username@www.123.com VPN访问 安装shadowsocks客户端，链接后配置socks5代理，就可以链接到内网的任何服务了 autossh—强大而稳定的解决方案ssh隧道非常容易断掉，在做远程转发的时候，有时候往往无法链接服务器，因此需要一个稳定的SSH链接，autossh就能实现SSH的断线自动重启 1234567891011121314# 配置好客户端到远程服务的ssh免密码连接autossh -f -M 55888 -NR *:10013:localhost:22 hadoop@oceanai.com.cnusage: autossh [-V] [-M monitor_port[:echo_port]] [-f] [SSH_OPTIONS] -M specifies monitor port. Overrides the environment variable AUTOSSH_PORT. 0 turns monitoring loop off. Alternatively, a port for an echo service on the remote machine may be specified. (Normally port 7.) -f run in background (autossh handles this, and does not pass it to ssh.) -V print autossh version and exit. 设置crontab任务，实现代理的自检查和重启功能，基本不会断连接 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354crontab -e */1 * * * * bash /home/hadoop/autossh/monitor-auto-ssh.sh* * * * * sleep 30; bash /home/hadoop/autossh/monitor-auto-ssh.shvim /home/hadoop/autossh/monitor-auto-ssh.sh---- #! /bin/shPROCESS_NAME=&apos;55566&apos;PROCESS_PATH=&apos;/home/hadoop/autossh&apos;START_PROCESS=&quot;autossh -f -M 55567 -NR 55566:localhost:22 hadoop@www.123.com&quot;VPN_PROCESS=&quot;autossh -f -M 55577 -NR 10012:localhost:13838 hadoop@www.123.com&quot;VPN_PROCESS_NAME=&quot;10012&quot;#PORT_TRANS=&quot;ssh -C -f -N -g -R 8888:127.0.0.1:8888 hadoop@www.123.com&quot;#PORT_TRANS_NAME=&quot;8888:127.0.0.1:8888&quot;proc_num() #查询进程数量 &#123; num=`ps -ef | grep $&#123;PROCESS_NAME&#125; | grep -v grep | wc -l` return $num &#125; proc_num2()&#123; num=`ps -ef | grep $&#123;VPN_PROCESS_NAME&#125; | grep -v grep | wc -l` return $num&#125; proc_num number=$?proc_num2number2=$?if [ $number -eq 0 ] #如果进程数量为0 then #重新启动服务 echo &quot;Restarting $&#123;PROCESS_NAME&#125; ...&quot; echo $(date &apos;+%Y-%m-%d %T&apos;) &gt;&gt; $&#123;PROCESS_PATH&#125;/restart.log ($&#123;START_PROCESS&#125;) &amp; echo &quot;over&quot;elif [ $number2 -eq 2 ]then echo &quot;ssh proxy has been started!&quot;fi if [ $number2 -eq 0 ] #如果进程数量为0 then #重新启动服务 echo &quot;Restarting $&#123;VPN_PROCESS_NAME&#125; ...&quot; echo $(date &apos;+%Y-%m-%d %T&apos;) &gt;&gt; $&#123;PROCESS_PATH&#125;/restart.log ($&#123;VPN_PROCESS&#125;) &amp; echo &quot;over&quot;elif [ $number -eq 2 ]then echo &quot;shadow proxy has been started!&quot;fi]]></content>
      <categories>
        <category>笔记</category>
        <category>网络</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ShadowsocksR一键安装脚本]]></title>
    <url>%2F2018%2F06%2F25%2FShadowsocksR%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[ShadowsocksR一键安装脚本默认配置：服务器端口：自己设定（如不设定，默认从 9000-19999 之间随机生成）密码：自己设定（如不设定，默认为 teddysun.com）加密方式：自己设定（如不设定，默认为 aes-256-cfb）协议（Protocol）：自己设定（如不设定，默认为 origin）混淆（obfs）：自己设定（如不设定，默认为 plain） 客户端下载：github shadowsock 安装步骤123wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.shchmod +x shadowsocksR.sh./shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log 卸载步骤./shadowsocksR.sh uninstall 多用户配置123456789101112131415161718192021&#123;"server":"0.0.0.0","server_ipv6": "[::]","local_address":"127.0.0.1","local_port":1080,"port_password":&#123; "8989":"password1", "8990":"password2", "8991":"password3"&#125;,"timeout":300,"method":"aes-256-cfb","protocol": "origin","protocol_param": "","obfs": "plain","obfs_param": "","redirect": "","dns_ipv6": false,"fast_open": false,"workers": 1&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[groovy学习笔记]]></title>
    <url>%2F2018%2F06%2F22%2Fgroovy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[概述Groovy是一种基于Java平台的面向对象语言。 Groovy 1.0于2007年1月2日发布，其中Groovy 2.4是当前的主要版本。 Groovy通过Apache License v 2.0发布。 Groovy的特点Groovy中有以下特点: 同时支持静态和动态类型。 支持运算符重载。 本地语法列表和关联数组。 对正则表达式的本地支持。 种标记语言，如XML和HTML原生支持。 Groovy对于Java开发人员来说很简单，因为Java和Groovy的语法非常相似。 您可以使用现有的Java库。 Groovy扩展了java.lang.Object。 用法groovy 大部分的语法和java相同，并且支持静态类型。 123def x = 5x = "jack"println x 数值和表达式 支持的数据类型：byte short int long float double char boolean String 同java 对应的类封装类型 其中整数是Intgerger类的实例，有小数部分的数据是BigDecimal类的实例 特殊运算符： 范围运算符： 12def range = 0..5println range // 输出： [0,1,2,3,4,5] groovy 支持运算符的重载 动态类型： def variable_name动态类型在运行时确定，不在编译的时候确定，类似java的多态 字符串和正则表达式字符串字面值单引号：所见即所得双引号：解释性字符串三引号：解释性字符串 多行文本用这个 1234567def age=25'My age is $&#123;age&#125;' //My age is $&#123;age&#125;"My age is $&#123;age&#125;" //My age is 25"""//My age is $&#123;age&#125;""" //My age is 25"""My age is \$&#123;age&#125;""" //My age is $&#123;age&#125; 字符串索引1234567def greeting='Hello World'greeting[4] //ogreeting[-1] //dgreeting[1..2] //elgreeting[1..&lt;3] //el, 等价于greeting[1..2]greeting[4..2] //ollgreeting[4,1,6] //oew groovy 中的字符串是有序序列，单个字符可以通过位置访问，支持负索引。 字符串操作参考链接：groovy字符串 12345678def greeting='Hello world''Hello'+'world' //Helloworld'Hello'*3 //HelloHelloHellogreeting-'o world' //Hellgreeting.size() //11greeting.length() //11greeting.count('o') //2greeting.contains('ell') //true 索引类似java String 的 charAt()方法获取指定位置的字符 字符串方法def message=’Hello’message.center(11) //返回长度为11，左右两边均使用空格填充的字符串message.center(3)message.center(11,’#’)message.eachMatch(‘.’) 正则表达式~’regex’定义正则表达式 def regex=~’cheese’ if(‘cheesecake’=~’cheese’) //左边String对象和右边的正则匹配，返回布尔值 ==~ 精确匹配 ‘cheesecake’==~’cheese’ \\\\在正则中表示一个反斜杠字符类似于java中正则的使用，用单引号括起也可以用js的方法，直接使用，不用引号 1 def matcher=”$abc.”=~\\$(.)\.2 def matcher=”$abc.”=~/$(.)./ 方法12345678910111213141516def methodName() &#123; //Method code &#125;// 不用显式指定参数名字def methodName(parameter1, parameter2, parameter3) &#123; // Method code goes here &#125;// 默认参数def someMethod(para1,para2=0,para3=0)&#123; // Method code goes here &#125;// 返回值return 可选，省略了，代码的最后一条语句的值就是方法的返回值 groovy 定义方法时不用显示指定参数类型 groovy 列表、映射、范围列表在java list基础上进行了一些扩展 [11，12，13，14] - 整数值列表[‘Angular’，’Groovy’，’Java’] - 字符串列表[1，2，[3，4]，5] - 嵌套列表[‘Groovy’，21，2.11] - 异构的对象引用列表[] - 一个空列表 12345678def list = ['hello', 12, 1.5]println list[0..1]list &lt;&lt; 'world' println list[0..-1]//输出如下：[hello, 12][hello, 12, 1.5, world] &lt;&lt; 把新元素追加到列表末尾 调用#leftShift 连接两个列表 调用#plus 从列表删除元素 调用#minusnumber[1]=[33,44] 调用#putAt 映射类似 java 的Map 123456789101112131415161718192021// 1. 访问 调用getAtdef names=[‘ken’:’Barclay,’John’:’Savage’] def divisors=[4:[2],6:[3,2],12:[6,4,3,2]] names[‘Ken’] //第一种访问写法 names.Ken //第二种访问写法 divisors[6]// 2. 赋值 调用putAtdivisors[6]=[6,3,2,1]// 3. 空映射[:] 空映射// 4. 映射方法def map=['ken':2745,'john':2746,'sally':2742]map.get('david',9999) //9999map.get('sally') //2742map.keySet() //[david,ken,sally,john]mp['ken'] //2745mp.values.asList() 范围1..10 - 包含范围的示例1 .. &lt;10 - 独占范围的示例, 排除最后一个数值‘a’..’x’ - 范围也可以由字符组成10..1 - 范围也可以按降序排列‘x’..’a’ - 范围也可以由字符组成并按降序排列。 类如果不声明public/private等访问权限的话，Groovy中类及其变量默认都是public的。 在使用默认修饰符的时候，自动生成隐藏的getter和setter方法，不过也可以直接访问 其余和java差不多 闭包闭包，英文叫Closure，是Groovy中非常重要的一个数据类型或者说一种概念了。闭包，是一种数据类型，它代表了一段可执行的代码。 12345def aClosure = &#123;//闭包是一段代码，所以需要用花括号括起来.. String param1, int param2 -&gt; //这个箭头很关键。箭头前面是参数定义，箭头后面是代码 println"this is code" //这是代码，最后一句是返回值， //也可以使用return，和Groovy中普通函数一样 &#125; closure 的定义格式为： 1234def xxx = &#123;paramters -&gt; code&#125; //或者 def xxx = &#123;无参数，纯code&#125; 闭包的访问： 1234aClosure.call("this is string",100) //或者 aClosure("this is string", 100) 闭包的默认参数，it, 类似 this 12345def fun2 = &#123; it-&gt; "dsdsd"&#125;println( fun2.call()) 闭包默认参数的覆盖 123456def fun3 = &#123; -&gt; "dsdsd"&#125;println( fun3.call())fun3.call("d") //执行这个方法的时候就会报错 闭包和函数的区别接口特征(trait)特征是语言的构造，允许 行为的组成 接口的运行时实现 与静态类型检查/编译的兼容性 traint 可以被看做是承载默认实现和状态的接口 traint 的实现 12345678910111213141516171819class Example &#123; static void main(String[] args) &#123; Student st = new Student(); st.StudentID = 1; st.Marks1 = 10; println(st.DisplayMarks()); &#125; &#125; trait Marks &#123; int Marks1; void DisplayMarks() &#123; println("Display Marks"); &#125; &#125; class Student implements Marks &#123; int StudentID&#125; traint 用于行为的构成： 特征可以用于以受控的方式实现多重继承，避免钻石问题。在下面的代码示例中，我们定义了两个特征 - Marks和Total。我们的Student类实现了两个特征。由于学生类扩展了这两个特征，它能够访问这两种方法 - DisplayMarks和DisplayTotal。 12345678910111213141516171819202122232425class Example &#123; static void main(String[] args) &#123; Student st = new Student(); st.StudentID = 1; println(st.DisplayMarks()); println(st.DisplayTotal()); &#125; &#125; trait Marks &#123; void DisplayMarks() &#123; println("Marks1"); &#125; &#125; trait Total &#123; void DisplayTotal() &#123; println("Total"); &#125; &#125; class Student implements Marks,Total &#123; int StudentID &#125; 扩展特征 特征可能扩展另一个特征，在这种情况下，必须使用extends关键字。在下面的代码示例中，我们使用Marks trait扩展了Total trait。 1234567891011121314151617181920212223class Example &#123; static void main(String[] args) &#123; Student st = new Student(); st.StudentID = 1; println(st.DisplayMarks()); &#125; &#125; trait Marks &#123; void DisplayMarks() &#123; println("Marks1"); &#125; &#125; trait Total extends Marks &#123; void DisplayMarks() &#123; println("Total"); &#125; &#125; class Student implements Total &#123; int StudentID &#125; 总的来说，特征可以实现接口，可以被类实现，可以继承其他的特征，相比java, groovy的这一个特性相当于实现了多继承 xmlMarkupBuilder用于构造整个XML文档。通过首先创建XML文档类的对象来创建XML文档 xml 构建 12345678910111213141516171819202122232425262728293031import groovy.xml.MarkupBuilder class Example &#123; static void main(String[] args) &#123; def mB = new MarkupBuilder() // Compose the builder mB.collection(shelf : 'New Arrivals') &#123; movie(title : 'Enemy Behind') type('War, Thriller') format('DVD') year('2003') rating('PG') stars(10) description('Talk about a US-Japan war') &#125; &#125; &#125;// 得到如下结果&lt;collection shelf = 'New Arrivals'&gt; &lt;movie title = 'Enemy Behind' /&gt; &lt;type&gt;War, Thriller&lt;/type&gt; &lt;format&gt;DVD&lt;/format&gt; &lt;year&gt;2003&lt;/year&gt; &lt;rating&gt;PG&lt;/rating&gt; &lt;stars&gt;10&lt;/stars&gt; &lt;description&gt;Talk about a US-Japan war&lt;/description&gt; &lt;/movie&gt; &lt;/collection&gt; xml 解析 123456789101112131415161718192021222324252627282930313233343536import groovy.xml.MarkupBuilder import groovy.util.*class Example &#123; static void main(String[] args) &#123; def parser = new XmlParser() def doc = parser.parse("D:Movies.xml"); doc.movie.each&#123; bk-&gt; print("Movie Name:") println "$&#123;bk['@title']&#125;" print("Movie Type:") println "$&#123;bk.type[0].text()&#125;" print("Movie Format:") println "$&#123;bk.format[0].text()&#125;" print("Movie year:") println "$&#123;bk.year[0].text()&#125;" print("Movie rating:") println "$&#123;bk.rating[0].text()&#125;" print("Movie stars:") println "$&#123;bk.stars[0].text()&#125;" print("Movie description:") println "$&#123;bk.description[0].text()&#125;" println("*******************************") &#125; &#125;&#125; JSON 功能 对象 JsonSlurper | JsonSlurper是一个将JSON文本或阅读器内容解析为Groovy数据的类结构，例如地图，列表和原始类型，如整数，双精度，布尔和字符串。JsonOutput | 此方法负责将Groovy对象序列化为JSON字符串。 json 解析 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849http.request( GET, TEXT ) &#123; headers.Accept = 'application/json' headers.'User-Agent' = USER_AGENT response.success = &#123; res, rd -&gt; def jsonText = rd.text //Setting the parser type to JsonParserLax def parser = new JsonSlurper().setType(JsonParserType.LAX) def jsonResp = parser.parseText(jsonText) &#125;&#125;// 解析文本数据class Example &#123; static void main(String[] args) &#123; def jsonSlurper = new JsonSlurper() def object = jsonSlurper.parseText('&#123; "name": "John", "ID" : "1"&#125;') println(object.name); println(object.ID); &#125; &#125;// 解析整数列表class Example &#123; static void main(String[] args) &#123; def jsonSlurper = new JsonSlurper() Object lst = jsonSlurper.parseText('&#123; "List": [2, 3, 4, 5] &#125;') lst.each &#123; println it &#125; &#125; &#125;// 解析基本数据类型列表class Example &#123; static void main(String[] args) &#123; def jsonSlurper = new JsonSlurper() def obj = jsonSlurper.parseText ''' &#123;"Integer": 12, "fraction": 12.55, "double": 12e13&#125;''' println(obj.Integer); println(obj.fraction); println(obj.double); &#125; &#125; json 输出 1234567891011121314151617181920212223242526import groovy.json.JsonOutput class Example &#123; static void main(String[] args) &#123; def output = JsonOutput.toJson([name: 'John', ID: 1]) println(output); &#125;&#125;// 输出&#123;"name":"John","ID":1&#125;// 作用于普通 groovy 对象class Example &#123; static void main(String[] args) &#123; def output = JsonOutput.toJson([ new Student(name: 'John',ID:1), new Student(name: 'Mark',ID:2)]) println(output); &#125; &#125; class Student &#123; String name int ID; &#125; DSLSGroovy允许在顶层语句的方法调用的参数周围省略括号。这被称为“命令链”功能。这个扩展的工作原理是允许一个人链接这种无括号的方法调用，在参数周围不需要括号，也不需要链接调用之间的点。 DSL或域特定语言旨在简化以Groovy编写的代码，使得它对于普通用户变得容易理解。以下示例显示了具有域特定语言的确切含义。 参考资料： Groovy 使用完全解析 Groovy基础——接口的实现方式 Groovy教程]]></content>
      <categories>
        <category>编程</category>
        <category>groovy</category>
      </categories>
      <tags>
        <tag>groovy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据传输的加密]]></title>
    <url>%2F2018%2F06%2F22%2F%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E7%9A%84%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[简介加密算法首先分为两种：单向加密、双向加密 单向加密是不可逆的，只能加密，不能解密，钥匙即本身。通常用来传输类似用户名和密码，直接将加密后的数据提交到后台，因为后台不需要知道用户名和密码，可以直接将收到的加密后的数据存储到数据库。 双向加密通常分为对称加密和非对称加密。对称加密指的是，加密和解密使用的是同一把秘钥。非对称加密指的是，加密和解密双方事先生成一对秘钥，使用其中的一把作为公钥用来加密信息，并且只能使用对应的私钥进行解密信息。这样可以避免钥匙的传输风险了。 常用算法 几种对称性加密算法：AES,DES,3DES DES是一种分组数据加密技术（先将数据分成固定长度的小数据块，之后进行加密），速度较快，适用于大量数据加密，而3DES是一种基于DES的加密算法，使用3个不同密匙对同一个分组数据块进行3次加密，如此以使得密文强度更高。 相较于DES和3DES算法而言，AES算法有着更高的速度和资源使用效率，安全级别也较之更高了，被称为下一代加密标准。 几种非对称性加密算法：RSA,DSA,ECC RSA和DSA的安全性及其它各方面性能都差不多，而ECC较之则有着很多的性能优越，包括处理速度，带宽要求，存储空间等等。 RSA 通常采用1024位，ECC 160 位, AES 128 位 几种线性散列算法（签名算法，单向加密，不可逆加密）：MD5,SHA1,SHA-2,HMAC 这几种算法只生成一串不可逆的密文，经常用其效验数据传输过程中是否经过修改，因为相同的生成算法对于同一明文只会生成唯一的密文，若相同算法生成的密文不同，则证明传输数据进行过了修改。通常在数据传说过程前，使用MD5和SHA1算法均需要发送和接收数据双方在数据传送之前就知道密匙生成算法，而HMAC与之不同的是需要生成一个密匙，发送方用此密匙对数据进行摘要处理（生成密文），接收方再利用此密匙对接收到的数据进行摘要处理，再判断生成的密文是否相同。 MD5即Message-Digest Algorithm 5（信息-摘要算法 5），用于确保信息传输完整一致，目前广泛用于错误检查，例如种子下载的碎片完整性。输入不定长的数据，输出为128位的hash值。 SHA-1: SHA-1在许多安全协议中广为使用，包括TLS和SSL、PGP、SSH、S/MIME和IPsec，曾被视为是MD5（更早之前被广为使用的散列函数）的后继者。但SHA-1的安全性如今被密码学家严重质疑。 SHA-2: SHA-224、SHA-256、SHA-384，和SHA-512并称为SHA-2。输出的hash的位数不一样。 参考文档： 各种加密算法比较 HTTP和HTTPS的区别]]></content>
      <tags>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 远程构建]]></title>
    <url>%2F2018%2F06%2F20%2Fdocker-%E8%BF%9C%E7%A8%8B%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[docker 远程连接docker server和client是分开的，因此支持远程连接 dockerd 设置方法1： 修改 /etc/default/docker 文件 DOCKER_OPTS=&quot;-H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375&quot; 方法2： 修改 /etc/systemd/system/docker.service 文件 ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 重启docker服务 systemctl daemon-reload &amp;&amp; systemctl restart docker TSL 认证如果主机在公网环境，则需要使用TSL认证 参考资料： Docker Daemon连接方式详解]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑苹果安装记录]]></title>
    <url>%2F2018%2F06%2F20%2F%E9%BB%91%E8%8B%B9%E6%9E%9C%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[笔记本配置： 项目 详情 型号 | 神舟战神Z6-i78154S2CPU | Intel-core i7-4720HQ核心显卡 | Intel Graphics HD 4600独显 | NVIDIA GTX 960M 这是我的Config文件 软件准备： transMac 烧录镜像 DiskGenius 硬盘分区工具 BIOS + MBR 转 GPT + EFI 使用U盘 PE 系统，将硬盘MBR分区表备份出来 使用DG 软件将MBR 转为 GPT 分区 挂载ESP分区,很关键，不然无法启动系统 bcdboot c:\windows /s X: /f uefi /l zh-cn 其中X为你指派的ESP分区盘符。（修复UEfi也可借助工具） 查看ESP分区是否已经写入了efi等启动文件 参考链接：免重装系统 手把手教你MBR转GPT分区表 已经驱动的硬件： 显卡 声卡 键盘 背光亮度 麦克风 摄像头 无解： 无线网卡，准备换一个博通的 蓝牙 修复黑苹果和windows系统时间不同步的问题：bios 时钟设置UTC时间，mac OS 自动识别到UTC+8的时间，但是windows无法变成UTC+8, 因此需要在windows上面执行如下指令： Reg add HKLM\SYSTEM\CurrentControlSet\Control\TimeZoneInformation /v RealTimeIsUniversal /t REG_DWORD /d 1 修复黑苹果睡眠重启问题：参考链接 参考链接： 修改屏幕亮度的快捷键]]></content>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cloudera和CDH5的安装]]></title>
    <url>%2F2018%2F06%2F15%2FCloudera%E5%92%8CCDH5%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1.准备工作1.安装Oracel JDK（所有节点）查看cloudera官网上堆系统的要求安装jdk版本，查看链接CDH5.11.X要求JDK1.7u55 or higher 、JDK1.8u31 or higher，要求集群所有机器的JDK版本一致 2.安装Mysql （主节点）集群中至少有一台机器安装mysql，建议安装在主节点机器上。安装mysql参考链接 mysql配置 1234567# 配置mysql数据库远程连接$ mysql -uroot -p123456mysql&gt;grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;mysql&gt;flush privileges;$ sudo vi /etc/mysql/my.confbind-address=127.0.0.1 注释掉$ sudo service mysql restart 生成CDH5所需数据库 123456789101112131415161718create database amon default character set utf8;grant all on amon.* to 'amon'@'%' identified by 'amon';create database rman default character set utf8;grant all on rman.* to 'rman'@'%' identified by 'rman';create database metastore default character set utf8;grant all on metastore.* to 'hive'@'%' identified by 'hive';create database sentry default character set utf8;grant all on sentry.* to 'sentry'@'%' identified by 'sentry';create database nav default character set utf8;grant all on nav.* to 'nav'@'%' identified by 'nav';create database navms default character set utf8;grant all on navms.* to 'navms'@'%' identified by 'navms';create database hue default character set utf8 default collate utf8_general_ci;grant all on hue.* to 'hue'@'%' identified by 'hue';select * from information_schema.schemata;create database oozie;grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie';grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie'; 3.网络配置 （所有节点）12345678910111213# 1.修改hostnamesudo vi /etc/hostsname ==&gt; cloud01,cloud02,cloud03# 2. 修改ip映射sudo vi /etc/hosts ==&gt; example: 192.168.1.1 cloud01[,other name]192.168.1.3 cloud02[,other name]192.168.1.3 cloud03[,other name]# 3. 关闭防火墙sudo apt-get install ufw -ysudo ufw stopsudo ufw status //查看防火墙状态 3.设置SSH无密码访问123456789101112131415161718192021222324252627# 1.生成密钥对，集群所有机器上执行命令ssh-keygen -t rsa //默认生成两个文件 ~/.ssh/id_rsa ~/.ssh/id_rsa.pub# 2.选择集群一台机器，例如cloud01,假设集群有5台机器root@cloud01:~# for ip in `seq 1 5`;do scp root@cloud0$ip:~/.ssh/id_rsa.pub ~/.ssh/rsa_temp &amp;&amp; cat ~/.ssh/rsa_temp &gt;&gt; ~/.ssh/authorized_keys;done //此时会在~/.ssh/目录下生成authorizied_keys文件root@cloud01:~# for ip in `seq 2 5`;do scp ~/.ssh/authorized_keys root@cloud0$ip:~/.ssh/;done //复制authorizedkeys到其他的主机中root@cloud01:~# rm ~/.ssh/rsa_temp# 3.SSH服务器配置root@cloud01:~# vi /etc/ssh/sshd_configPermitRootLogin yes //修改默认的without password# 4.重启SSH服务sudo service sshd restart# 5.免密码登录失败原因1、权限问题.ssh目录，以及/home/当前用户 需要700权限，参考以下操作调整sudo chmod 700 ~/.sshsudo chmod 700 /home/当前用户.ssh目录下的authorized_keys文件需要600或644权限，参考以下操作调整sudo chmod 600 ~/.ssh/authorized_keys 4.配置NTP时间同步服务（所有节点） 时间服务器配置（主节点） 123456789101112131415161718192021222324252627# 所有节点安装ntp服务sudo apt-get install ntp#查看服务是否启动service --status-all[+]表示启动#选择cloud01作为时间同步服务器ssh cloud01sudo vi /etc/ntp.conf=====1. 修改serverserver [IP or hostname] [prefer]在 server 后端可以接 IP 或主机名，个人比较喜欢使用 IP 来设定， perfer 表示『优先使用』的服务器。server 2.cn.pool.ntp.org preferserver 0.asia.pool.ntp.org preferserver 3.asia.pool.ntp.org prefer2. 修改本地server#让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端server 127.127.1.0fudge 127.127.1.0 stratum 53. restrict管理时间服务器权限#不允许来自公网上ipv4和ipv6客户端的访问restrict -4 default kod notrap nomodify nopeer noqueryrestrict -6 default kod notrap nomodify nopeer noquery#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap===== 请求对时客户端配置（其他节点） 123456#安装ntp服务sudo apt-get install ntpsudo vi /etc/ntp.conf#去掉之前默认的serverserver 192.168.1.1（时间服务器的ip）fudge 192.168.1.1 stratum 5 对时操作 12345678910111. 使用ntp服务对时$ sudo service ntp stop$ sudo ntpdate 时间服务器ip$ sudo service ntp start2.查看时间状态$ timedatectl status$ sudo ntpq -p3.更换时区$ sudo tzselect 5.修改source.list123456# 1.增加两个源deb [arch=amd64] http://archive.cloudera.com/cm5/ubuntu/trusty/amd64/cm trusty-cm5 contribdeb-src http://archive.cloudera.com/cm5/ubuntu/trusty/amd64/cm trusty-cm5 contrib# 2.更新sudo apt-get update 2.安装CDH1.下载CDH安装包（主节点）Ubuntu14.04下载链接（CM5.11）cloudera-manager下载链接CDH5下载链接123456789101112131415# 1.解压安装包sudo mkdir /opt/cloudera-managersudo tar xzf cloudera-manager*.tar.gz -C /opt/cloudera-manager# 假设$CMF_DEFAULTS=/opt/cloudera-manager/cm-5.11.1# 2.初始化数据库sudo $CMF_DEFAULTS/share/cmf/schema/scm_prepare_database.sh mysql cm -hMasterNode -uroot -p123456 --scm-host MasterNode scm scm scm# 3.修改angent配置sudo vi $CMF_DEFAULTS/etc/cloudera-scm-agent/config.ini server_host = MasterNode# 4.复制cloudera-manager到其他主机root@cloud01:~# for $ip in `seq 2 5`;do scp -r $CMF_DEFAULTS/ root@cloud0$ip:/opt/ 2.为CDH创建用户（所有节点）1root@cloud01:~# useradd --system --home=$CMF_DEFAULTS/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment "Cloudera SCM User" cloudera-scm 3.创建相关文件夹（主节点）12$ sudo mkdir /var/lib/cloudera-scm-server$ sudo chown cloudera-scm:cloudera-scm /var/lib/cloudera-scm-server 4.准备Parcels，用于安装CDH51234567891011# 1.主节点放置CDH5 Parcel 库文件$ sudo mkdir -p /opt/cloudera/parcel-repo$ sudo chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo$ sudo mv /path/to/CDH5.parcel /opt/cloudera/parcel-repo/ $ sudo mv /path/to/CDH5.parcel.sha1 /opt/cloudera/parcel-repo/CDH5.parcel.sha$ sudo mv /path/to/manifest.json /opt/cloudera/parcel-repo/# 2.所有节点创建parcels目录$ sudo mkdir -p /opt/cloudera/parcels$ sudo chown cloudera-scm:cloudera-scm /opt/cloudera/parcels 5.启动cloudera scm 服务1234567891011121314151617# 1. 主节点启动server服务$ root@cloud01:~# /$CMF_DEFAULTS/etc/init.d/cloudera-scm-server start//server启动日志/var/log/cloudera-scm-server/cloudera-scm-server.log# 2. 所有节点启动agent服务$ root@cloud01:~# /$CMF_DEFAULTS/etc/init.d/cloudera-scm-agent start//agent启动日志/var/log/cloudera-scm-agent/cloudera-scm-agent.log# 3.设置机器重启自动运行服务$ cp $CMF_DEFAULTS/etc/init.d/cloudera-scm-server /etc/init.d/cloudera-scm-server$ update-rc.d cloudera-scm-server defaults$ cp $CMF_DEFAULTS/etc/init.d/cloudera-scm-agent /etc/init.d/cloudera-scm-agent$ update-rc.d cloudera-scm-agent defaults# 4.取消自动重启$ sudo update-rc.d -f cloudera-scm-agent remove tips:这一步如果没有完成好，会导致后面的cloudera-manager agent安装出错，推荐顺序启动/opt目录下的agent服务设置步骤3，但不要使用service 命令启动安装完成后，可以去掉自动重启（执行步骤4） 6.CDH5网页配置 打开http://clouder-server-host:7180 默认密码是admin 选择免费版本，不需要安装JDK root用户登录，提前将集群机器的root密码设置一样，并且打通root用户的ssh登录 7.安装kafka服务1.下载kafka parcel，在线或者离线都可以，然后在集群所有机器中分发激活Kafka Parcel 2.集群添加kafka服务若提示，或者错误日志中提示Java Heap空间太小：Missing required value: Destination Broker ListMissing required value: Source Broker List 可按如下方法配置后重试即可：a. 填写Source Brokers List填写Kafka Broker所在节点构成的列表（用逗号分隔），如下（本文在所有节点部署了Kafka Broker）：master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092 b. 填写Destination Brokers List若添加了Kafka MirrorMaker，则可填写其所在节点构成的列表；若未添加Kafka MirrorMaker，可填写任意服务器即可，如下：master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092或：example.com:9092 c. 修改Java Heap Size填写上面列表后，点击继续，出错后，Kafka服务未启动。返回集群配置，打卡Kafka服务配置页，查找“Java Heap Size of Broker”项，将对大小从50MB修改为256MB。d. 配置Topic Whitelist配置Topic Whitelist项为正则表达式：(?!x)x，保存更改。然后添加角色实例，重新配置即可。参考链接：adding a Kafka service failed 3.CDH5安装常见问题1. 主机检查警告123456789101112131415161718第一个警告： Cloudera 建议将 /proc/sys/vm/swappiness 设置为 10。当前设置为 60。使用 sysctl 命令在运行时更改该设置并编辑 /etc/sysctl.conf 以在重启后保存该设置。 echo 10 &gt; /proc/sys/vm/swappiness这样操作重启机器还是还原，要永久改变vim /etc/sysctl.confvm.swappiness=10 第二个警告，提示执行命令： echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled执行完毕，重启后，警告依然，暂时不处理 2.mysql数据库生成报错123456789# 1.主要是系统的java环境中没有连接数据库的包，因此需要手动把包拷贝到系统目录$ sudo cp mysql-connector-*.jar /usr/share/java/mysql-connector-java.jar# 2.设置mysql远程登录和访问权限问题mysql&gt;grant all privileges on . to ‘root’@’%’ identified by ‘youpassword’ with grant option;mysql&gt;flush privileges;$ sudo vi /etc/mysql/my.confcomment bind-address = 127.0.0.1$ sudo service mysql restart 3.kafka安装问题参考官方教程，分配Kafka parcel包，第一次安装会遇到问题，解决方案如下：http://www.aboutyun.com/thread-19903-1-1.html 4.参考链接http://www.cnblogs.com/codedevelop/p/6762555.html 5.agent启动报错ProtocolError: resolution:Steps to find and kill the process:1) Find the port which is used by supervisor: ps aux |grep supervisor2) kill the portsudo kill -9]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>cloudera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins使用]]></title>
    <url>%2F2018%2F06%2F12%2Fjenkins%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[jenkins 是一个自动化集成工具，可以实现对程序员完全透明化，只需要专注于自己的业务代码即可 安装准备工作：硬件条件：256 MB of RAM + 1 GB of drive space软件条件：JRE 8 docker docker 方式安装 install docker 安装jenkins 启动 docker container123456789docker run \ -u root \ --rm \ -d \ -p 8080:8080 \ -p 50000:50000 \ -v jenkins-data:/var/jenkins_home \ -v /var/run/docker.sock:/var/run/docker.sock \ jenkinsci/blueocean download war file java -jar jenkins.war brows http://localhost:8080 and wait until the Unlock Jenkins page appears. apt 方式安装1234wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'sudo apt-get updatesudo apt-get install jenkins 使用jenkins 本身是一个持续集成部署的工具，可以和很多工具平台联动。 credentialscredentials 主要是用来存储一些用户密码、token 这些敏感信息 配置： Credentials &gt; Create permission detail… pipeline（流水线）环境配置： jenkins version &gt;= 2.x pipeline plugin in jenkins 定义一个流水线：通过编写Jenkinsfile 来完成流水线作业。 Jenkinsfile 采用 groovy 语法 jenkins 支持两种类型的流水线： 一种是 Declarative Pipeline, 另一种是 scripted pipeline, 个人推荐使用第二种，简洁 使用例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243# declarative pipelinepipeline &#123; agent any environment &#123; CC = 'clang' &#125; stages &#123; stage('Build') &#123; steps &#123; echo 'Building..' &#125; &#125; stage('Test') &#123; steps &#123; echo 'Testing..' &#125; &#125; stage('Deploy') &#123; steps &#123; echo 'Deploying....' &#125; &#125; &#125;&#125;# scripted pipelinenode &#123; withEnv(['CC=lang']) &#123; &#125; stage('Build') &#123; echo 'Building....' &#125; stage('Test') &#123; echo 'Building....' &#125; stage('Deploy') &#123; echo 'Deploying....' &#125;&#125; agent/node: instructs Jenkins to allocate an executor (on any available agent/node in the Jenkins environment) and workspace for the entire Pipeline. 环境变量：http://localhost:8080/pipeline-syntax/globals#env 常用的环境变量：BUILD_IDJOB_NAMEJENKINS_URL pipeline generateorclick http://localhost:8080/pipeline-syntax/ 可以看到很多帮助文档，其中Snippet Generator 用来生成scripted pipeline, Declarative Directive Generator 生成 declarative pipeline handle credentials通过环境变量注入的方式, 前提是已经做好了credentials, 这里只是提供了一个读取的方法 12345678910111213141516# secret textenvironment &#123; AWS_ACCESS_KEY_ID = credentials('jenkins-aws-secret-key-id') AWS_SECRET_ACCESS_KEY = credentials('jenkins-aws-secret-access-key')&#125;# username and passwordenvironment &#123; BITBUCKET_COMMON_CREDS = credentials('jenkins-bitbucket-common-creds')&#125;## actually environment variablesBITBUCKET_COMMON_CREDS - contains a username and a password separated by a colon in the format username:password.BITBUCKET_COMMON_CREDS_USR - an additional variable containing the username component only.BITBUCKET_COMMON_CREDS_PSW - an additional variable containing the password component only. 其他的证书获取的方法可以参考 generator handle parameters1234567891011121314151617181920pipeline &#123; agent any parameters &#123; string(name: 'Greeting', defaultValue: 'Hello', description: 'How should I greet the world?') &#125; stages &#123; stage('Example') &#123; steps &#123; echo "$&#123;params.Greeting&#125; World!" &#125; &#125; &#125;&#125;# scripted pipelineproperties([parameters([string(defaultValue: 'Hello', description: 'How should I greet the world?', name: 'Greeting')])])node &#123; echo "$&#123;params.Greeting&#125; World!"&#125; handle failure12345678910111213141516171819202122232425262728293031323334pipeline &#123; agent any stages &#123; stage('Test') &#123; steps &#123; sh 'make check' &#125; &#125; &#125; post &#123; always &#123; junit '**/target/*.xml' &#125; failure &#123; mail to: team@example.com, subject: 'The Pipeline failed :(' &#125; &#125;&#125;# scripted pipelinenode &#123; /* .. snip .. */ stage('Test') &#123; try &#123; sh 'make check' &#125;catch&#123; mail to: team@example.com, subject: 'The Pipeline failed :(' &#125; finally &#123; junit '**/target/*.xml' &#125; &#125; /* .. snip .. */&#125; 多个 angent/node 运行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273pipeline &#123; agent none stages &#123; stage('Build') &#123; agent any steps &#123; checkout scm sh 'make' stash includes: '**/target/*.jar', name: 'app' &#125; &#125; stage('Test on Linux') &#123; agent &#123; label 'linux' &#125; steps &#123; unstash 'app' sh 'make check' &#125; post &#123; always &#123; junit '**/target/*.xml' &#125; &#125; &#125; stage('Test on Windows') &#123; agent &#123; label 'windows' &#125; steps &#123; unstash 'app' bat 'make check' &#125; post &#123; always &#123; junit '**/target/*.xml' &#125; &#125; &#125; &#125;&#125;# scripted pipelinestage('Build') &#123; node &#123; checkout scm sh 'make' stash includes: '**/target/*.jar', name: 'app' &#125;&#125;stage('Test') &#123; node('linux') &#123; checkout scm try &#123; unstash 'app' sh 'make check' &#125; finally &#123; junit '**/target/*.xml' &#125; &#125; node('windows') &#123; checkout scm try &#123; unstash 'app' bat 'make check' &#125; finally &#123; junit '**/target/*.xml' &#125; &#125;&#125; stash： 捕获对应的文件，让流水线的其他angent 一起使用 label: jenkins lable expression mutibranch pipeline多分支的scm 流水线 detail docker pipeline12345678910111213141516171819202122pipeline &#123; agent &#123; docker &#123; image 'node:7-alpine' &#125; &#125; stages &#123; stage('Test') &#123; steps &#123; sh 'node --version' &#125; &#125; &#125;&#125;# scripted pipelinenode &#123; /* Requires the Docker Pipeline plugin to be installed */ docker.image('node:7-alpine').inside &#123; stage('Test') &#123; sh 'node --version' &#125; &#125;&#125; 缓存 docker container 数据12345678910111213141516171819202122232425pipeline &#123; agent &#123; docker &#123; image 'maven:3-alpine' args '-v $HOME/.m2:/root/.m2' &#125; &#125; stages &#123; stage('Build') &#123; steps &#123; sh 'mvn -B' &#125; &#125; &#125;&#125;# scripted pipelinenode &#123; /* Requires the Docker Pipeline plugin to be installed */ docker.image('maven:3-alpine').inside('-v $HOME/.m2:/root/.m2') &#123; stage('Build') &#123; sh 'mvn -B' &#125; &#125;&#125; 使用多容器1234567891011121314151617181920212223242526272829303132333435363738pipeline &#123; agent none stages &#123; stage('Back-end') &#123; agent &#123; docker &#123; image 'maven:3-alpine' &#125; &#125; steps &#123; sh 'mvn --version' &#125; &#125; stage('Front-end') &#123; agent &#123; docker &#123; image 'node:7-alpine' &#125; &#125; steps &#123; sh 'node --version' &#125; &#125; &#125;&#125;# scripted pipelinenode &#123; /* Requires the Docker Pipeline plugin to be installed */ stage('Back-end') &#123; docker.image('maven:3-alpine').inside &#123; sh 'mvn --version' &#125; &#125; stage('Front-end') &#123; docker.image('node:7-alpine').inside &#123; sh 'node --version' &#125; &#125;&#125; 使用 DockerfileJenkins 支持从 scm 的 Dockerfile 来构建镜像并运行，而不是从镜像仓库里面拉取 123# file DockerfileFROM node:7-alpineRUN apk add -U subversion Jenkinsfile1234567891011pipeline &#123; agent &#123; dockerfile true &#125; stages &#123; stage('Test') &#123; steps &#123; sh 'node --version' sh 'svn --version' &#125; &#125; &#125;&#125; sidecar(解决docker 服务依赖问题)12345678910111213node &#123; checkout scm /* * In order to communicate with the MySQL server, this Pipeline explicitly * maps the port (`3306`) to a known port on the host machine. */ docker.image('mysql:5').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw" -p 3306:3306') &#123; c -&gt; /* Wait until mysql service is up */ sh 'while ! mysqladmin ping -h0.0.0.0 --silent; do sleep 1; done' /* Run some tests which require MySQL */ sh 'make check' &#125;&#125; 通过 docker link 来链接mysql 服务, cenos 提供了一个程序执行环境 12345678910111213141516node &#123; checkout scm docker.image('mysql:5').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw"') &#123; c -&gt; docker.image('mysql:5').inside("--link $&#123;c.id&#125;:db") &#123; /* Wait until mysql service is up */ sh 'while ! mysqladmin ping -hdb --silent; do sleep 1; done' &#125; docker.image('centos:7').inside("--link $&#123;c.id&#125;:db") &#123; /* * Run some tests which require MySQL, and assume that it is * available on the host name `db` */ sh 'make check' &#125; &#125;&#125; 构建新的镜像12345678910111213141516171819202122232425262728293031323334353637383940414243444546node &#123; checkout scm def customImage = docker.build("my-image:$&#123;env.BUILD_ID&#125;") customImage.inside &#123; sh 'make test' &#125;&#125;# 如何pushnode &#123; checkout scm def customImage = docker.build("my-image:$&#123;env.BUILD_ID&#125;") customImage.push()&#125;# 如何打tagnode &#123; checkout scm def customImage = docker.build("my-image:$&#123;env.BUILD_ID&#125;") customImage.push() customImage.push('latest')&#125;# 自定义dockerfile, 添加 build 函数参数node &#123; checkout scm // Builds test-image from the Dockerfile found at ./dockerfiles/test/Dockerfile. def testImage = docker.build("test-image", "./dockerfiles/test") testImage.inside &#123; sh 'make test' &#125;&#125;node &#123; //Builds my-image:$&#123;env.BUILD_ID&#125; from the Dockerfile found at ./dockerfiles/Dockerfile.test checkout scm def dockerfile = 'Dockerfile.test' def customImage = docker.build("my-image:$&#123;env.BUILD_ID&#125;", "-f $&#123;dockerfile&#125; ./dockerfiles") &#125; 自定义 registry1234567891011121314151617181920node &#123; checkout scm docker.withRegistry('https://registry.example.com') &#123; docker.image('my-custom-image').inside &#123; sh 'make test' &#125; &#125;&#125;# authenticationnode &#123; checkout scm //add a "Username/Password" Credentials item from the Jenkins home page docker.withRegistry('https://registry.example.com', 'credentials-id') &#123; def customImage = docker.build("my-image:$&#123;env.BUILD_ID&#125;") /* Push the container to the custom Registry */ customImage.push() &#125;&#125; 共享库Extending with Shared Libraries pipeline 语法1 Declarative Pipelines 1234pipeline &#123; /* insert Declarative Pipeline here */&#125; detail blueocean一款新的UI，detail jenkins in k8sJenkins Server和slave节点直接有几种连接方式：ssh连接和jnlp连接。Kubernetes plugin插件用的是jnlp方式。这种方式是通过运行slave.jar，指定Jenkins Server的url参数和secret token参数，来建立连接。 docker 运行jenkins slave (ssh 模式)这种模式docker 运行一个slave 容器跟普通物理机使用完全一致，这里不做说明。 同样可以再在同一个slave节点（docker 容器）上绑定很多个工程或者任务。 docker 运行jenkins slave (jnlp 模式)Jnlp 模式的则相对应用的比较少，jnlp 是由jenkins slave节点（物理节点，虚机或者容器均可）发起连接的，他 会根据配置的jenkins master的url , Jenkins连接的token和jenkins slave name( lable)来进行进行连接。 jenkins k8s plugins的使用配置安装 kubernetes plugins ，进入jenkins 系统设置，如下图所示 注意事项： 在使用自定义的镜像的时候，因为/home/jenkins 目录被挂载到了NFS上面，因此在镜像中保留的文件全部被清空掉，kube/config文件需要通过NFS才能挂载 参考资料: Jenkins On Kubernetes—-Jenkins上Kubernetes Plugin的使用 初试 Jenkins 使用 Kubernetes Plugin 完成持续构建与发布 Kubernetes集群上基于Jenkins的CI/CD流程实践]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s学习-1--常用命令]]></title>
    <url>%2F2018%2F04%2F27%2Fk8s%E5%AD%A6%E4%B9%A0-1%2F</url>
    <content type="text"><![CDATA[k8s 采用声明式的 API , 因此许多命令可以组合使用 查看查看资源 kubectl get \&lt;resource type\&gt; | all [-n namespace] [-o wide] 查看集群 kubectl cluster-info 查看各组件的状态 kubectl -s http://masterip:8080 get componentstatuses 详细信息查看资源的详细信息，类似docker inspect kubectl describe \&lt;resource type\&gt; 创建创建k8s资源 kubectl create -f filename filename 为定义资源的yaml文件。也可以直接只用子命令 [run/namespace/secret/configmap/serviceaccount] 等直接创建相应的resource。从追踪和维护的角度出发，建议使用json或yaml的方式定义资源。 更新更新资源有以下几种方式： 1.replace kubectl replace -f filename replace命令用于对已有资源进行更新、替换。当我们需要更新resource的一些属性的时候，如果修改副本数量，增加、修改label，更改image版本，修改端口等。都可以直接修改原yaml文件，然后执行replace命令。 注意名字不可以被更新 2.patch 对一个已经在运行的pod进行更新操作，不会删除容器 3.edit 交互式编辑资源文件并更新 4.apply 比 pathch 和 edit 更加严格的更新操作，保留更新历史版本库 5.rolling-update 不中断更新，先产生新的 pod, 更新完成后再生成再删除旧的 pod, 直到替换掉所有的pod 6.scale 资源的扩容或缩容 7.autoscale 自动根据系统资源的情况进行扩容或缩容 删除kubectl delete &lt;resource type&gt; 日志kubectl logs 进入容器kubectl attach kubectl exec [-c container-name] 节点管理kubectl get nodes //查看所有节点 节点维护：cordon, drain, uncordon cordon: 标记节点为SchedulingDisable, 禁止新的资源被调度到该节点 drain: 将要维护的节点的pod赶到其他的节点 uncordon: 恢复维护节点到正常的工作状态]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据平台维护笔记]]></title>
    <url>%2F2017%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E7%BB%B4%E6%8A%A4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.hdfs文件迁移方法：参考链接：http://blog.csdn.net/bigkeen/article/details/51034902http://www.cnblogs.com/juncaoit/p/6178747.htmlhttp://blog.csdn.net/weipanp/article/details/42713149 可以相互通信的两个集群在老的hadoop集群中开启yarn和hdfs服务，新的集群开启hdfs服务即可执行以下命令： 1hadoop distcp -i hftp://old cluster ip:50070/src directory hdfs://192.168.91.130:8020/new cluster directory 删除老集群的hdfs文件使用hdfs命令删除文件后执行hadoop dfs -expunge删除对应的DataNode和namenode路径下的文件夹 2.多次格式化NameNode导致无法启动DataNode的解决方案http://www.cnblogs.com/yeahwell/p/5642798.html 3.查看某端口被映射到端口上iptables -t nat -L -n | grep 80iptables -t nat —list //检查nat列表信息iptables -t nat -D PREROUTING 1 /Nat列表信息删除：序号从1 开始，后边以此+1. 参考链接：http://blog.csdn.net/xin_yu_xin/article/details/46416101 4.安装mysqlhttp://blog.csdn.net/chenpy/article/details/50344085 5.storm 并行度rebalance操作：http://blog.csdn.net/jmppok/article/details/17243857storm并行度理解：http://www.cnblogs.com/catkins/p/5254377.html 6.自定义系统定制Ubuntu镜像：https://www.zybuluo.com/fanisfun/note/802272 7.cloudera-agent 启动报错解决方法错误1：ERROR Failed to connect to previous supervisor.通常是由于之前已经启动了agent残留下来的进程产生的影响，因此需要将之前的进程清除掉执行命令 kill -9 $(pgrep -f supervisord) 然后重启，除了第一次安装要求复制cloudera-agent到系统/etc/init.d/目录下，建议放置到rc.local文件启动吧]]></content>
      <categories>
        <category>笔记</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe相关博客]]></title>
    <url>%2F2017%2F06%2F04%2Fcaffe%E7%9B%B8%E5%85%B3%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[深度学习（六）caffe入门学习 Deep Learning模型之：CNN卷积神经网络（一）深度解析CNN - 莫小 - 博客园]]></content>
      <categories>
        <category>笔记</category>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Linux系统/etc/init.d目录和/etc/rc.local脚本]]></title>
    <url>%2F2017%2F05%2F24%2F%E7%90%86%E8%A7%A3Linux%E7%B3%BB%E7%BB%9F-etc-init-d%E7%9B%AE%E5%BD%95%E5%92%8C-etc-rc-local%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[一、关于/etc/init.d如果你使用过Linux系统，那么你一定听说过init.d目录。这个目录到底是干嘛的呢？它归根结底只做了一件事情，但这件事情非同小可，是为整个系统做的，因此它非常重要。init.d目录包含许多系统各种服务的启动和停止脚本。它控制着所有从acpid到x11-common的各种事务。当然，init.d远远没有这么简单。（译者注：acpid 是linux操作系统新型电源管理标准 ；X11也叫做X Window系统，X Window系统 (X11或X)是一种位图显示的 视窗系统 。它是在 Unix 和 类Unix 操作系统 ，以及 OpenVMS 上建立图形用户界面 的标准工具包和协议，并可用于几乎已有的现代操作系统）。 当你查看/etc目录时，你会发现许多rc#.d 形式存在的目录（这里#代表一个指定的初始化级别，范围是0~6）。在这些目录之下，包含了许多对进程进行控制的脚本。这些脚本要么以”K”开头，要么以”S”开头。以K开头的脚本运行在以S开头的脚本之前。这些脚本放置的地方，将决定这些脚本什么时候开始运行。在这些目录之间，系统服务一起合作，就像运行状况良好的机器一样。然而，有时候你希望能在不使用kill 或killall 命令的情况下，能干净的启动或杀死一个进程。这就是/etc/init.d能够派上用场的地方了！ 如果你在使用Fedora系统，你可以找到这个目录：/etc/rc.d/init.d。实际上无论init.d放在什么地方，它都发挥着相同的作用。 为了能够使用init.d目录下的脚本，你需要有root权限或sudo权限。每个脚本都将被作为一个命令运行，该命令的结构大致如下所示： /etc/init.d/command 选项comand是实际运行的命令，选项可以有如下几种： start stop reload restart force-reload 大多数的情况下，你会使用start,stop,restart选项。例如，如果你想关闭网络，你可以使用如下形式的命令：123456789101112 /etc/init.d/networking stop又比如，你改变了网络设置，并且需要重启网络。你可以使用如下命令：/etc/init.d/networking restartinit.d目录下常用初始化脚本有：networkingsambaapache2ftpdsshddovecotmysql 当然，你可能有其他更多常用的脚本，这个取决于你安装了什么linux操作系统。 二、关于/etc/rc.localrc.local也是我经常使用的一个脚本。该脚本是在系统初始化级别脚本运行之后再执行的，因此可以安全地在里面添加你想在系统启动之后执行的脚本。常见的情况是你可以再里面添加nfs挂载/mount脚本。此外，你也可以在里面添加一些调试用的脚本命令。例如，我就碰到过这种情况：samba服务总是无法正常运行，而检查发现，samba是在系统启动过程中就该启动执行的，也就是说，samba守护程序配置保证了这种功能本应该正确执行。碰到这种类似情况，一般我也懒得花大量时间去查为什么，我只需要简单的在/etc/rc.local脚本里加上这么一行： /etc/init.d/samba start 这样就成功的解决了samba服务异常的问题。 三、总结Linux是灵活的。正因为它的灵活性，我们总是可以找到许多不同的办法来解决同一个问题。启动系统服务的例子就是一个很好的佐证。有了/etc/init.d目录下的脚本，再加上/etc/rc.local这个利器，你可以放心的确保你的服务可以完美的启动和运行。]]></content>
      <categories>
        <category>IT</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu系统操作命令笔记（持续更新...）]]></title>
    <url>%2F2017%2F05%2F24%2F%E5%8E%9F-Ubuntu%E7%B3%BB%E7%BB%9F%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0-%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、修改用户密码1sudo passwd =&gt;&gt;输入new-root-password 2、切换用户1su username =&gt;&gt;输入待切换用户的密码 3、提升权限12su root (临时提升权限)sudo gedit /etc/passwd (永久提升权限) 4、设置环境变量 为单个用户设置（～/.bashrc） 全局设置（/etc/profile） /etc/bashrc. 该文件是为每一个运行的shell用户执行操作，当bashshell被打开是，文件被读取 ～/bash_profile: 每个用户可以使用该文件输入专用于自己的shell信息 /etc/environment: 在登录的时候操作系统使用的的第二个文件，系统在读取你的profile之前，设置文件的环境变量 5、ssh使用1.登录远程主机ssh username@host [-p port] =&gt;input your host passwordequals (ssh -l username host)2.文件传输scp username@serverhost:/serverfile /save path (download from server)scp /local file username@serverhost:/server save path(upload to server)从服务器下载或者上传整个目录scp -r username@serverhost:/remote directory /local directoryscp -r /local directory username@serverhost:/remote directory3.开启无密码ssh登录client A; server B A: ssh-keygen -t rsa (三次回车，在～/.ssh目录下生成id_ras，id_rsa.pub两个文件，分别是私钥和公钥) A: cat ~/.ssh/id_ras.pub &gt;&gt; ~/.ssh/authorized-keys（复制公钥到文件authorized-keys） A: scp ~/.ssh/id_rsa.pub XXX@remotehost:/home/XXX/id_rsa.pub B: cat ~/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys(追加公钥到服务器的authorizied_keys文件中，注意登录到远程服务器操作此步骤) A: ssh-add ~/.ssh/id_rsa (添加私钥) 更改权限: chmod 700 ~/.ssh; chmod 600 ~/.ssh/authorized_keys 6、更改文件权限和所有者chmod [options] mode files/directorymode=u,g,o 7=4读+2写+1执行，u用户，g，用户组，o，其他+，-表示增加或者删除权限r,w,x表示读，写，执行三种权限，对应上面的三个数字。加上-R参数后就可以修改整个目录的文件的权限了。chown username file/directory 更改文件的所有者 7、系统查看指令tail -n number file 查看文件末尾number行cat file 查看文件所有内容ls -lh查看文件详细情du -lh查看单独文件的大小df -lh查看磁盘分区gnome-system-monitor 查看资源管理器 8、用户和用户组管理http://www.cnblogs.com/xd502djj/archive/2011/11/23/2260094.html 9、修改localehttp://dgd2010.blog.51cto.com/1539422/1684813http://blog.csdn.net/myweishanli/article/details/23576847]]></content>
      <categories>
        <category>IT</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu端口转发]]></title>
    <url>%2F2017%2F05%2F23%2Fubuntu%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[1. 内网有一台公网服务器iptables方法 2. 内网没有公网服务器，只有云服务器 思路：利用ssh反向代理实现内网穿透 准备工作：假设内网有机器A(192.168.1.2),机器B(192.168.1.3),机器A机器B在一个局域网内，但是只有机器A能够访问互联网，本身没有静态公网IP,机器B可以访问机器A，但是无法访问互联网。另外有一台可访问的云服务器C(公网IP: 104.28.39.108) 目的：实现在任意有网络的地方访问服务器A和服务器B 将机器A的ssh公钥复制到机器C的authorized_keys，实现A到C的免密码登陆 在机器A执行如下脚本： 12touch /home/hadoop/autossh/monitor-auto-ssh.sh &amp;&amp; chmod +x /home/hadoop/autossh/monitor-auto-ssh.shvim /home/hadoop/autossh/monitor-auto-ssh.sh monitor-auto-ssh.sh内容如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#! /bin/sh# check every 30 seconds# crontab -e# */1 * * * * bash /home/hadoop/autossh/monitor-auto-ssh.sh# * * * * * sleep 30; bash /home/hadoop/autossh/monitor-auto-ssh.shPROCESS_NAME='55566'PROCESS_PATH='/home/hadoop/autossh'START_PROCESS="autossh -f -M 55567 -NR 55566:localhost:22 hadoop@104.28.39.108"VPN_PROCESS="autossh -f -M 55577 -NR 10012:localhost:13838 hadoop@104.28.39.108"VPN_PROCESS_NAME="10012"#PORT_TRANS="ssh -C -f -N -g -R 8888:127.0.0.1:8888 hadoop@39.108.120.127"#PORT_TRANS_NAME="8888:127.0.0.1:8888"proc_num() #查询进程数量 &#123; num=`ps -ef | grep $&#123;PROCESS_NAME&#125; | grep -v grep | wc -l` return $num &#125; proc_num2()&#123; num=`ps -ef | grep $&#123;VPN_PROCESS_NAME&#125; | grep -v grep | wc -l` return $num&#125;proc_num number=$?proc_num2number2=$?if [ $number -eq 0 ] #如果进程数量为0 then #重新启动服务 echo "Restarting $&#123;PROCESS_NAME&#125; ..." echo $(date '+%Y-%m-%d %T') &gt;&gt; $&#123;PROCESS_PATH&#125;/restart.log ($&#123;START_PROCESS&#125;) &amp; echo "over"elif [ $number2 -eq 2 ]then echo "ssh proxy has been started!"fiif [ $number2 -eq 0 ] #如果进程数量为0 then #重新启动服务 echo "Restarting $&#123;VPN_PROCESS_NAME&#125; ..." echo $(date '+%Y-%m-%d %T') &gt;&gt; $&#123;PROCESS_PATH&#125;/restart.log ($&#123;VPN_PROCESS&#125;) &amp; echo "over"elif [ $number -eq 2 ]then echo "shadow proxy has been started!"fi 机器A执行定时检查任务，确保ssh隧道断开自动重连 1234# check every 30 secondscrontab -e*/1 * * * * bash /home/hadoop/autossh/monitor-auto-ssh.sh* * * * * sleep 30; bash /home/hadoop/autossh/monitor-auto-ssh.sh 服务器C需要在云网络安全组开放55566和10012两个端口，这两个端口分别通过隧道反向代理A机器的22号端口和13838端口 外网访问方式 ssh 指定机器C的55566端口就可以连接到A机器了，通过A机器可以跳转到B机器，实现间接链接 搭建VPN。在机器A上安装shadowsocks服务器端，并且设置端口为13838（要和脚本中的端口保持一致），然后客户端机器安装shadowsocks客户端,代理服务器地址为104.28.39.108，端口为10012，用户名和密码从A机器上获取。 至此，代理服务器搭建完毕，设置shadowsocks全局代理模式，默认将本地127.0.0.1的1080端口的请求全部转发到C服务器上的10012端口，而C服务器的10012端口通过autossh反向代理到了A机器的13838端口，从而实现了从外网任何一台可以上网的设备远程连接到没有公网环境的内网服务器A和服务器B，能够访问内网的任何服务。]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>iptable</tag>
        <tag>端口转发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu14.04下 Cloudera Manager和CDH5.11的配置]]></title>
    <url>%2F2017%2F05%2F23%2FCloudera%20Manager%E5%92%8CCDH5-11%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本教程用于搭建Clodera Manager和CDH5的大数据分析平台的搭建工作 1.参考链接：http://www.cnblogs.com/codedevelop/p/6762555.html （博客园）http://blog.csdn.net/a921122/article/details/51939692 （csdn）http://blog.csdn.net/hualiu163/article/details/46659375 （CDH5.33 详细）http://www.aboutyun.com/thread-9075-1-1.html （在线安装，不推荐） 基本思路：集群主机ip配置，关闭防火墙，设置ssh无密码登录，统一root密码，集群时间同步，安装oracle jdk，主节点安装mysql，并初始化scm数据库，下载parcel包，进入控制台设置等 2.注意的问题：jdbc的问题：安装hive时，需要将mysql驱动包复制到/opt/cloudera/parcels/CDH/lib/hive/lib/目录下，并且需要把mysql驱动复制到/usr/share/java/mysql-connector-java.jar下 目录权限问题，ssh免密码登录问题，mysql远程访问设置。。。 注意要添加root用户的访问host，不然后面初始化会出问题grant all privileges on . to ‘root’@’n1’ identified by ‘xxxx’ with grant option; 3、官方安装教程官方手动安装教程（强烈建议按照官方教程来操作，可以少走很多坑，阿西吧）：https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_install_path_c.html#id_v4k_pnn_25system requirement:https://www.cloudera.com/downloads/manager/5-11-0.html 4.主节点安装mysql5.6在线安装：http://blog.csdn.net/chenpy/article/details/50344085离线安装：http://blog.csdn.net/linlinv3/article/details/51774040注意要设置远程登录（具体参照官方教程：MySQL设置） 创建数据库：123456789101112131415161718192021222324create database amon default character set utf8;grant all on amon.* to 'amon'@'%' identified by 'amon';create database rman default character set utf8;grant all on rman.* to 'rman'@'%' identified by 'rman';create database metastore default character set utf8;grant all on metastore.* to 'hive'@'%' identified by 'hive';create database sentry default character set utf8;grant all on sentry.* to 'sentry'@'%' identified by 'sentry';create database nav default character set utf8;grant all on nav.* to 'nav'@'%' identified by 'nav';create database navms default character set utf8;grant all on navms.* to 'navms'@'%' identified by 'navms';create database hue default character set utf8 default collate utf8_general_ci;grant all on hue.* to 'hue'@'%' identified by 'hue';select * from information_schema.schemata;create database oozie;grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie';grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie'; master节点交换内存设置：http://blog.chinaunix.net/uid-20051192-id-3557817.html 这里注意下，官网教程上面的reboot后自动启动的命令有点问题，直接启动和命令启动的时候运行的进程不一致update-rc.d -f cloudera-scm-server removeupdate-rc.d -f cloudera-scm-agent remove貌似不能随便去掉这个过程，否则后面又会出现这个问题 安装kafka的问题：首先参考官方教程，分配kafka parcel包，然后安装，第一次安装会遇到问题，解决方案如下：http://www.aboutyun.com/thread-19903-1-1.html]]></content>
      <categories>
        <category>解决方案</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>CDH5</tag>
        <tag>Cloudera Manager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux服务器时间同步]]></title>
    <url>%2F2017%2F04%2F25%2Flinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[Ubuntu下服务器时间同步1.时间服务器配置123456789101112131415161718192021222324252627282930#服务器cloud01,cloud02,cloud03都安装ntp服务sudo apt-get install ntp#查看服务是否启动service --status-all[+]表示启动#选择cloud01作为时间同步服务器ssh cloud01sudo vi /etc/ntp.conf=====1. 修改serverserver [IP or hostname] [prefer]在 server 后端可以接 IP 或主机名，个人比较喜欢使用 IP 来设定， perfer 表示『优先使用』的服务器。server 2.cn.pool.ntp.org preferserver 0.asia.pool.ntp.org preferserver 3.asia.pool.ntp.org prefer2. 修改本地server#让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端server 127.127.1.0fudge 127.127.1.0 stratum 53. restrict管理时间服务器权限#不允许来自公网上ipv4和ipv6客户端的访问restrict -4 default kod notrap nomodify nopeer noqueryrestrict -6 default kod notrap nomodify nopeer noquery#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap===== 2.请求对时客户端配置这里cloud02和cloud03分别是客户端1234567#安装ntp服务sudo apt-get install ntpsudo vi /etc/ntp.conf#去掉之前默认的serverserver 192.168.1.1（时间服务器的ip）fudge 192.168.1.1 stratum 5 3.客户端和时间服务器同步 使用ntpdate更新时间 注意要停止ntp服务才能使用ntpdate更新时间sudo ntpdate 192.168.1.1sudo hwclock —systohc #将系统时间写入硬件时间sudo hwclock —hctosys #将硬件时间写入系统时间 使用ntpd更新时间sudo service ntp start #启动ntp进程，自动逐渐同步时间 4.查看时间状态timedatectl statussudo ntpq -p 5.更换时区sudo tzselect 常见错误解决方案 no server suitable for synchronization found参考链接：http://www.blogjava.net/spray/archive/2008/07/10/213964.html]]></content>
      <categories>
        <category>解决方案</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql命令]]></title>
    <url>%2F2017%2F04%2F24%2Fmysql%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1、mysql的安装linux下mysql安装-博客园 2、用户管理启动服务：net start mysqld链接数据库：mysql -uroot -p修改root用户密码：mysqladmin -u root password “new_password”; 2.1新建用户mysql -uroot -p //登录mysqlmysql&gt; insert into mysql.user(Host,User,Password) values(“localhost”,”test”,password(“1234”));//localhost表示只能在本机登录，改为”%”则可以远程登录 或者：mysql&gt; CREATE USER ‘username’@’host’ IDENTIFIED BY ‘password’; 2.2用户授权grant 权限 on 数据库.* to ‘username’@’host’ identified by ‘password’权限：select, update, all, delete, create, drop 2.3删除用户Delete FROM user Where User=’test’ and Host=’localhost’;flush privileges;drop database 用户数据库名;drop user 用户名@’%’drop user 用户名@‘localhost’ 2.4修改用户密码mysql&gt;update mysql.user set password=password(‘新密码’) where User=”test” and Host=”localhost”;mysql&gt;flush privileges; 在5.7版本的mysql中，没有“password”这个字段，因此需要使用“authentication_string”这个字段来替换“password”字段 或者采用以下命令：ALTER USER ‘root’@’localhost’IDENTIFIED BY ‘**‘ 2.5撤销用户权限REVOKE 权限 ON databasename.tablename FROM ‘username’@’host’; 2.6常用命令show databases;//列出所有数据库use ‘databasename’;//切换数据库show tables;//列出所有表describe tablename;//显示数据表结构drop database 数据库名;//删除数据库drop database 表名;//删除表 2.7重置root密码 修改安装目录下的my.ini文件，添加一行：skip-grant-tables 重启mysql服务：windows(管理员模式):net stop mysql; net start mysql; mysql -uroot -p(直接回车登录) update mysql.user set authentication_string=password(‘新密码’) where User=”root” 注释my.ini文件中skip-grant-tables这一行 重启mysql服务 3.常见问题 远程连接问题： 解决方案如下：1、授权mysql&gt;grant all privileges on . to ‘root’@’%’ identified by ‘youpassword’ with grant option;mysql&gt;flush privileges;2、修改/etc/mysql/my.conf找到bind-address = 127.0.0.1这一行,注释即可 重启mysql服务 12[root@localhost /]# service mysqld start (5.0版本是mysqld)[root@szxdb etc]# service mysql start (5.5.7版本是mysql)]]></content>
      <categories>
        <category>笔记</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java核心技术（一）]]></title>
    <url>%2F2017%2F04%2F21%2Fjava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[java基本程序设计结构1.java字符串 java编译器让不可变的字符串共享，所有的不可变字符串会存放在公共的存储池中，字符串变量指向存储池中的字符串的相应位置。类似于c++中的字符串指针，一旦字符串没人用，java自动回收机制会自动回收该字符串。 如何判断String字符串是否为空？ 123456789case 1: 判断java字符串是否为空======if(str.length()==0)或 if(str.equals(""))case 2: 判断是否为null======if(str == null)这两种方式不一样，null表示当前的String变量没有与对象关联。 常用String API trim() 去掉原始字符串头尾空格 lasetIndexOf(String str) 返回str匹配的第一个子串的开始位置 toLowerCase() toUpperCase() 2.java输入输出 读取文本输入 1234Scanner in = new Scanner(System.in);String name = in.nextLine();//输入如果有空格采用该种方法String sex = in.next();//输入空格隔开调用该方法int age = in.nextInt();//读取整数,nextDouble类似返回双精度浮点数 读取密码输入 123Console cons = System.console();//console对象每次只能读取一行输入String user = cons.readLine("User name: ");char[] passwd = cons.readPassword("Password: "); 大数值的计算java的基本数据类型： 类型 字节 int 4 short 2 logn 8 byte 1 float 4 double 8 NaN：当数据溢出时显示无穷大 java提供了BigInteger和BigDecimal两个类，可以处理包含任意长度数字序列的数值，分别实现了任意精度的整数运算和浮点数运算。 java数组for each循环: for(T element: collection){};匿名数组 new String[] {“ss”,”rr”};java中允许数组长度为0，但数组长度为0不等于nullnew elementType[0];// 不是null数组拷贝：Arrays.copyOf(T[] arrayVar, int lenth); java对象和类 java运行内存管理http://www.importnew.com/21463.htmlhttp://www.cnblogs.com/gw811/archive/2012/10/18/2730117.html堆和栈的区别 访问时间Date()类： 1234Date date = new Date();date.getTime();// = System.currentTimeMillis()//还可以使用 LocalDate()和Calendar()这两个类 java编译javac 类名.javajava有两种编译方式：显示编译和隐式编译。显示编译某个文件时，如果该文件调用了其他的类，则会对引用的类进行编译操作，如果被引用的类被修改了，则会重新编译新版本的.class文件 java封装java类的成员都应该设置为私有的，否则很危险。通常使用共有的方法来对私有成员进行读写。成员方法大部分设计为共有方法，一旦设计成公有的，不能随便删除，因为其他的代码可能依赖它。 静态导入包 12import static java.lang.System.*;out.println("hello world!");//静态导入包，不用加类名前缀就可以直接访问方法。]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式（三）：行为型模式]]></title>
    <url>%2F2017%2F04%2F07%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考链接：Java开发中的23种设计模式详解(转)Java经典设计模式之五大创建型模式（附实例和详解） 行为型模式细分为如下11种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 先来张图，看看这11中模式的关系： 第一类：通过父类与子类的关系进行实现。第二类：两个类之间。第三类：类的状态。第四类：通过中间类 1. 策略模式策略模式定义了一系列算法，并将每个算法封装起来，使他们可以相互替换，且算法的变化不会影响到使用算法的客户。需要设计一个接口，为一系列实现类提供统一的方法，多个实现类实现该接口，设计一个抽象类（可有可无，属于辅助类），提供辅助函数，关系图如下： 图中ICalculator提供统一的方法，AbstractCalculator是辅助类，提供辅助方法，接下来，依次实现下每个类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public interface ICalculator &#123; public int calculate(String exp); &#125;public abstract class AbstractCalculator &#123; public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125; &#125; # 3个实现类## 加法public class Plus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"\\+"); return arrayInt[0]+arrayInt[1]; &#125; &#125;## 减法public class Minus extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"-"); return arrayInt[0]-arrayInt[1]; &#125; &#125; ## 乘法public class Multiply extends AbstractCalculator implements ICalculator &#123; @Override public int calculate(String exp) &#123; int arrayInt[] = split(exp,"\\*"); return arrayInt[0]*arrayInt[1]; &#125; &#125; # 测试类public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = "2+8"; ICalculator cal = new Plus(); int result = cal.calculate(exp); System.out.println(result); &#125; &#125; 策略模式的决定权在用户，系统本身提供不同算法的实现，新增或者删除算法，对各种算法做封装。因此，策略模式多用在算法决策系统中，外部用户只需要决定用哪个算法即可。 2. 模板方法模式一个抽象类中，有一个主方法，再定义1…n个方法，可以是抽象的，也可以是实际的方法，定义一个类，继承该抽象类，重写抽象方法，通过调用抽象类，实现对子类的调用，先看个关系图： 12345678910111213141516171819202122232425262728293031323334353637public abstract class AbstractCalculator &#123; /*主方法，实现对本类其它方法的调用*/ public final int calculate(String exp,String opt)&#123; int array[] = split(exp,opt); return calculate(array[0],array[1]); &#125; /*被子类重写的方法*/ abstract public int calculate(int num1,int num2); public int[] split(String exp,String opt)&#123; String array[] = exp.split(opt); int arrayInt[] = new int[2]; arrayInt[0] = Integer.parseInt(array[0]); arrayInt[1] = Integer.parseInt(array[1]); return arrayInt; &#125; &#125; public class Plus extends AbstractCalculator &#123; @Override public int calculate(int num1,int num2) &#123; return num1 + num2; &#125; &#125; public class StrategyTest &#123; public static void main(String[] args) &#123; String exp = "8+8"; AbstractCalculator cal = new Plus(); int result = cal.calculate(exp, "\\+"); System.out.println(result); &#125; &#125; 3. 观察者模式观察者模式很好理解，类似于邮件订阅和RSS订阅，当我们浏览一些博客或wiki时，经常会看到RSS图标，就这的意思是，当你订阅了该文章，如果后续有更新，会及时通知你。其实，简单来讲就一句话：当一个对象变化时，其它依赖该对象的对象都会收到通知，并且随着变化！对象之间是一种一对多的关系。先来看看关系图： MySubject类就是我们的主对象，Observer1和Observer2是依赖于MySubject的对象，当MySubject变化时，Observer1和Observer2必然变化。AbstractSubject类中定义着需要监控的对象列表，可以对其进行修改：增加或删除被监控对象，且当MySubject变化时，负责通知在列表内存在的对象。我们看实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public interface Observer &#123; public void update(); &#125; public class Observer1 implements Observer &#123; @Override public void update() &#123; System.out.println("observer1 has received!"); &#125; &#125; public class Observer2 implements Observer &#123; @Override public void update() &#123; System.out.println("observer2 has received!"); &#125; &#125; public interface Subject &#123; /*增加观察者*/ public void add(Observer observer); /*删除观察者*/ public void del(Observer observer); /*通知所有的观察者*/ public void notifyObservers(); /*自身的操作*/ public void operation(); &#125; public abstract class AbstractSubject implements Subject &#123; private Vector&lt;Observer&gt; vector = new Vector&lt;Observer&gt;(); @Override public void add(Observer observer) &#123; vector.add(observer); &#125; @Override public void del(Observer observer) &#123; vector.remove(observer); &#125; @Override public void notifyObservers() &#123; Enumeration&lt;Observer&gt; enumo = vector.elements(); while(enumo.hasMoreElements())&#123; enumo.nextElement().update(); &#125; &#125; &#125; public class MySubject extends AbstractSubject &#123; @Override public void operation() &#123; System.out.println("update self!"); notifyObservers(); &#125; &#125; # testpublic class ObserverTest &#123; public static void main(String[] args) &#123; Subject sub = new MySubject(); sub.add(new Observer1()); sub.add(new Observer2()); sub.operation(); &#125; &#125; 4. 迭代子模式顾名思义，迭代器模式就是顺序访问聚集中的对象，一般来说，集合中非常常见，如果对集合类比较熟悉的话，理解本模式会十分轻松。这句话包含两层意思：一是需要遍历的对象，即聚集对象，二是迭代器对象，用于对聚集对象进行遍历访问。我们看下关系图： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public interface Collection &#123; public Iterator iterator(); /*取得集合元素*/ public Object get(int i); /*取得集合大小*/ public int size(); &#125; public interface Iterator &#123; //前移 public Object previous(); //后移 public Object next(); public boolean hasNext(); //取得第一个元素 public Object first(); &#125; public class MyCollection implements Collection &#123; public String string[] = &#123;"A","B","C","D","E"&#125;; @Override public Iterator iterator() &#123; return new MyIterator(this); &#125; @Override public Object get(int i) &#123; return string[i]; &#125; @Override public int size() &#123; return string.length; &#125; &#125; public class MyIterator implements Iterator &#123; private Collection collection; private int pos = -1; public MyIterator(Collection collection)&#123; this.collection = collection; &#125; @Override public Object previous() &#123; if(pos &gt; 0)&#123; pos--; &#125; return collection.get(pos); &#125; @Override public Object next() &#123; if(pos&lt;collection.size()-1)&#123; pos++; &#125; return collection.get(pos); &#125; @Override public boolean hasNext() &#123; if(pos&lt;collection.size()-1)&#123; return true; &#125;else&#123; return false; &#125; &#125; @Override public Object first() &#123; pos = 0; return collection.get(pos); &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; Collection collection = new MyCollection(); Iterator it = collection.iterator(); while(it.hasNext())&#123; System.out.println(it.next()); &#125; &#125; &#125; 输出A B C D E 5. 责任链模式有多个对象，每个对象持有对下一个对象的引用，这样就会形成一条链，请求在这条链上传递，直到某一对象决定处理该请求。但是发出者并不清楚到底最终那个对象会处理该请求，所以，责任链模式可以实现，在隐瞒客户端的情况下，对系统进行动态的调整。先看看关系图： Abstracthandler类提供了get和set方法，方便MyHandle类设置和修改引用对象，MyHandle类是核心，实例化后生成一系列相互持有的对象，构成一条链。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public interface Handler &#123; public void operator(); &#125; public abstract class AbstractHandler &#123; private Handler handler; public Handler getHandler() &#123; return handler; &#125; public void setHandler(Handler handler) &#123; this.handler = handler; &#125; &#125; public class MyHandler extends AbstractHandler implements Handler &#123; private String name; public MyHandler(String name) &#123; this.name = name; &#125; @Override public void operator() &#123; System.out.println(name+"deal!"); if(getHandler()!=null)&#123; getHandler().operator(); &#125; &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; MyHandler h1 = new MyHandler("h1"); MyHandler h2 = new MyHandler("h2"); MyHandler h3 = new MyHandler("h3"); h1.setHandler(h2); h2.setHandler(h3); h1.operator(); &#125; &#125; 输出： h1deal!h2deal!h3deal! 此处强调一点就是，链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。 6. 命令模式命令模式很好理解，举个例子，司令员下令让士兵去干件事情，从整个事情的角度来考虑，司令员的作用是，发出口令，口令经过传递，传到了士兵耳朵里，士兵去执行。这个过程好在，三者相互解耦，任何一方都不用去依赖其他人，只需要做好自己的事儿就行，司令员要的是结果，不会去关注到底士兵是怎么实现的。我们看看关系图： Invoker是调用者（司令员），Receiver是被调用者（士兵），MyCommand是命令，实现了Command接口，持有接收对象，看实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface Command &#123; public void exe(); &#125; public class MyCommand implements Command &#123; private Receiver receiver; public MyCommand(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void exe() &#123; receiver.action(); &#125; &#125; public class Receiver &#123; public void action()&#123; System.out.println("command received!"); &#125; &#125; public class Invoker &#123; private Command command; public Invoker(Command command) &#123; this.command = command; &#125; public void action()&#123; command.exe(); &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; Receiver receiver = new Receiver(); Command cmd = new MyCommand(receiver); Invoker invoker = new Invoker(cmd); invoker.action(); &#125; &#125; 输出： command received! 这个很好理解，命令模式的目的就是达到命令的发出者和执行者之间解耦，实现请求和执行分开，熟悉Struts的同学应该知道，Struts其实就是一种将请求和呈现分离的技术，其中必然涉及命令模式的思想！* 7. 备忘录模式主要目的是保存一个对象的某个状态，以便在适当的时候恢复对象，个人觉得叫备份模式更形象些，通俗的讲下：假设有原始类A，A中有各种属性，A可以决定需要备份的属性，备忘录类B是用来存储A的一些内部状态，类C呢，就是一个用来存储备忘录的，且只能存储，不能修改等操作。做个图来分析一下： Original类是原始类，里面有需要保存的属性value及创建一个备忘录类，用来保存value值。Memento类是备忘录类，Storage类是存储备忘录的类，持有Memento类的实例，该模式很好理解。直接看源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Original &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public Original(String value) &#123; this.value = value; &#125; public Memento createMemento()&#123; return new Memento(value); &#125; public void restoreMemento(Memento memento)&#123; this.value = memento.getValue(); &#125; &#125; public class Memento &#123; private String value; public Memento(String value) &#123; this.value = value; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; &#125; public class Storage &#123; private Memento memento; public Storage(Memento memento) &#123; this.memento = memento; &#125; public Memento getMemento() &#123; return memento; &#125; public void setMemento(Memento memento) &#123; this.memento = memento; &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; // 创建原始类 Original origi = new Original("egg"); // 创建备忘录 Storage storage = new Storage(origi.createMemento()); // 修改原始类的状态 System.out.println("初始化状态为：" + origi.getValue()); origi.setValue("niu"); System.out.println("修改后的状态为：" + origi.getValue()); // 回复原始类的状态 origi.restoreMemento(storage.getMemento()); System.out.println("恢复后的状态为：" + origi.getValue()); &#125; &#125; 8. 状态模式核心思想就是：当对象的状态改变时，同时改变其行为，很好理解！就拿QQ来说，有几种状态，在线、隐身、忙碌等，每个状态对应不同的操作，而且你的好友也能看到你的状态，所以，状态模式就两点：1、可以通过改变状态来获得不同的行为。2、你的好友能同时看到你的变化。看图： State类是个状态类，Context类可以实现切换，我们来看看代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class State &#123; private String value; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void method1()&#123; System.out.println("execute the first opt!"); &#125; public void method2()&#123; System.out.println("execute the second opt!"); &#125; &#125; public class Context &#123; private State state; public Context(State state) &#123; this.state = state; &#125; public State getState() &#123; return state; &#125; public void setState(State state) &#123; this.state = state; &#125; public void method() &#123; if (state.getValue().equals("state1")) &#123; state.method1(); &#125; else if (state.getValue().equals("state2")) &#123; state.method2(); &#125; &#125; &#125; # testpublic class Test &#123; public static void main(String[] args) &#123; State state = new State(); Context context = new Context(state); //设置第一种状态 state.setValue("state1"); context.method(); //设置第二种状态 state.setValue("state2"); context.method(); &#125; &#125; 输出： execute the first opt!execute the second opt! 根据这个特性，状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，区别开他们的一些功能，比如说简单的权限控制等。 9. 访问者模式访问者模式把数据结构和作用于结构上的操作解耦合，使得操作集合可相对自由地演化。访问者模式适用于数据结构相对稳定算法又易变化的系统。因为访问者模式使得算法操作增加变得容易。若系统数据结构对象易于变化，经常有新的数据对象增加进来，则不适合使用访问者模式。访问者模式的优点是增加操作很容易，因为增加操作意味着增加新的访问者。访问者模式将有关行为集中到一个访问者对象中，其改变不影响系统数据结构。其缺点就是增加新的数据结构很困难。—— From 百科 简单来说，访问者模式就是一种分离对象数据结构与行为的方法，通过这种分离，可达到为一个被访问者动态添加新的操作而无需做其它的修改的效果。简单关系图： 123456789101112131415161718192021222324252627282930313233343536373839public interface Visitor &#123; public void visit(Subject sub); &#125; public class MyVisitor implements Visitor &#123; @Override public void visit(Subject sub) &#123; System.out.println("visit the subject："+sub.getSubject()); &#125; &#125; public interface Subject &#123; public void accept(Visitor visitor); public String getSubject(); &#125; public class MySubject implements Subject &#123; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; @Override public String getSubject() &#123; return "love"; &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; Visitor visitor = new MyVisitor(); Subject sub = new MySubject(); sub.accept(visitor); &#125; &#125; 输出：visit the subject：love 该模式适用场景：如果我们想为一个现有的类增加新功能，不得不考虑几个事情：1、新功能会不会与现有功能出现兼容性问题？2、以后会不会再需要添加？3、如果类不允许修改代码怎么办？ 面对这些问题，最好的解决方法就是使用访问者模式，访问者模式适用于数据结构相对稳定的系统，把数据结构和算法解耦 10. 中介者模式中介者模式也是用来降低类类之间的耦合的，因为如果类类之间有依赖关系的话，不利于功能的拓展和维护，因为只要修改一个对象，其它关联的对象都得进行修改。如果使用中介者模式，只需关心和Mediator类的关系，具体类类之间的关系及调度交给Mediator就行，这有点像spring容器的作用。先看看图： User类统一接口，User1和User2分别是不同的对象，二者之间有关联，如果不采用中介者模式，则需要二者相互持有引用，这样二者的耦合度很高，为了解耦，引入了Mediator类，提供统一接口，MyMediator为其实现类，里面持有User1和User2的实例，用来实现对User1和User2的控制。这样User1和User2两个对象相互独立，他们只需要保持好和Mediator之间的关系就行，剩下的全由MyMediator类来维护！基本实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public interface Mediator &#123; public void createMediator(); public void workAll(); &#125; public class MyMediator implements Mediator &#123; private User user1; private User user2; public User getUser1() &#123; return user1; &#125; public User getUser2() &#123; return user2; &#125; @Override public void createMediator() &#123; user1 = new User1(this); user2 = new User2(this); &#125; @Override public void workAll() &#123; user1.work(); user2.work(); &#125; &#125; public abstract class User &#123; private Mediator mediator; public Mediator getMediator()&#123; return mediator; &#125; public User(Mediator mediator) &#123; this.mediator = mediator; &#125; public abstract void work(); &#125; public class User1 extends User &#123; public User1(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println("user1 exe!"); &#125; &#125; public class User2 extends User &#123; public User2(Mediator mediator)&#123; super(mediator); &#125; @Override public void work() &#123; System.out.println("user2 exe!"); &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; Mediator mediator = new MyMediator(); mediator.createMediator(); mediator.workAll(); &#125; &#125; user1 exe!user2 exe! 11. 解释器模式一般主要应用在OOP开发中的编译器的开发中，所以适用面比较窄。 Context类是一个上下文环境类，Plus和Minus分别是用来计算的实现，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public interface Expression &#123; public int interpret(Context context); &#125; public class Plus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()+context.getNum2(); &#125; &#125; public class Minus implements Expression &#123; @Override public int interpret(Context context) &#123; return context.getNum1()-context.getNum2(); &#125; &#125; public class Context &#123; private int num1; private int num2; public Context(int num1, int num2) &#123; this.num1 = num1; this.num2 = num2; &#125; public int getNum1() &#123; return num1; &#125; public void setNum1(int num1) &#123; this.num1 = num1; &#125; public int getNum2() &#123; return num2; &#125; public void setNum2(int num2) &#123; this.num2 = num2; &#125; &#125; public class Test &#123; public static void main(String[] args) &#123; // 计算9+2-8的值 int result = new Minus().interpret((new Context(new Plus() .interpret(new Context(9, 2)), 8))); System.out.println(result); &#125; &#125; 最后输出正确的结果：3 基本就这样，解释器模式用来做各种各样的解释器，如正则表达式等的解释器等等！]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式（二）：结构型模式]]></title>
    <url>%2F2017%2F04%2F07%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考链接：Java开发中的23种设计模式详解(转)Java经典设计模式之五大创建型模式（附实例和详解） 一、结构型模式分类：一共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。其中适配器模式主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。其中的对象的适配器模式是各种结构型模式的起源。 二、适配器模式：适配器模式主要分为三类：类的适配器模式、对象的适配器模式、接口的适配器模式。 适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题。有点抽象，我们来看看详细的内容。 应用场景： 类的适配器模式：当希望将一个类转换成满足另一个新接口的类时，可以使用类的适配器模式，创建一个新类，继承原有的类，实现新的接口即可。 对象的适配器模式：当希望将一个对象转换成满足另一个新接口的对象时，可以创建一个Wrapper类，持有原类的一个实例，在Wrapper类的方法中，调用实例的方法就行。 接口的适配器模式：当不希望实现一个接口中所有的方法时，可以创建一个抽象类Wrapper，实现所有方法，我们写别的类的时候，继承抽象类即可。 三、装饰模式：装饰模式：在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象。 关系图： 装饰模式的特点： （1） 装饰对象和真实对象有相同的接口。这样客户端对象就能以和真实对象相同的方式和装饰对象交互。（2） 装饰对象包含一个真实对象的引用（reference）（3） 装饰对象接受所有来自客户端的请求。它把这些请求转发给真实的对象。（4） 装饰对象可以在转发这些请求以前或以后增加一些附加功能。这样就确保了在运行时，不用修改给定对象的结构就可以在外部增加附加的功能。在面向对象的设计中，通常是通过继承来实现对给定类的功能扩展。继承不能做到这一点，继承的功能是静态的，不能动态增删。123456789101112131415161718192021222324252627282930313233343536373839404142# 1. sourceablepublic interface Sourceable &#123; public void method(); &#125;# 2. sourcepublic class Source implements Sourceable &#123; @Override public void method() &#123; System.out.println("the original method!"); &#125; &#125;# 3. decoratorpublic class Decorator implements Sourceable &#123; private Sourceable source; public Decorator(Sourceable source)&#123; super(); this.source = source; &#125; @Override public void method() &#123; System.out.println("before decorator!"); source.method(); System.out.println("after decorator!"); &#125; &#125;# 4. testpublic class DecoratorTest &#123; public static void main(String[] args) &#123; Sourceable source = new Source(); Sourceable obj = new Decorator(source); obj.method(); &#125; &#125; 装饰器模式的应用场景： 1、需要扩展一个类的功能。 2、动态的为一个对象增加功能，而且还能动态撤销。（继承不能做到这一点，继承的功能是静态的，不能动态增删。） 缺点：产生过多相似的对象，不易排错！ 四、代理模式：代理模式就是多一个代理类出来，替原对象进行一些操作。代理类就像中介，它比我们掌握着更多的信息。 关系图： 12345678910111213141516171819202122232425262728293031323334# 3. proxypublic class Proxy implements Sourceable &#123; private Source source; public Proxy()&#123; super(); this.source = new Source(); &#125; @Override public void method() &#123; before(); source.method(); atfer(); &#125; private void after() &#123; System.out.println("after proxy!"); &#125; private void before() &#123; System.out.println("before proxy!"); &#125; &#125;# 4. testpublic class ProxyTest &#123; public static void main(String[] args) &#123; Sourceable source = new Proxy(); source.method(); &#125; &#125; 代理模式的应用场景： 如果已有的方法在使用的时候需要对原有的方法进行改进，此时有两种办法： 1、修改原有的方法来适应。这样违反了“对扩展开放，对修改关闭”的原则。 2、就是采用一个代理类调用原有的方法，且对产生的结果进行控制。这种方法就是代理模式。 使用代理模式，可以将功能划分的更加清晰，有助于后期维护！ 五、外观模式：外观模式是为了解决类与类之间的依赖关系的，像spring一样，可以将类和类之间的关系配置到配置文件中，而外观模式就是将他们的关系放在一个Facade类中，降低了类类之间的耦合度，该模式中没有涉及到接口。 关系图： 六、桥接模式：在软件系统中，某些类型由于自身的逻辑，它具有两个或多个维度的变化，那么如何应对这种“多维度的变化”？如何利用面向对象的技术来使得该类型能够轻松的沿着多个方向进行变化，而又不引入额外的复杂度？这就要使用Bridge模式。 在提出桥梁模式的时候指出，桥梁模式的用意是将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化。这句话有三个关键词，也就是抽象化、实现化和脱耦。 抽象化：存在于多个实体中的共同的概念性联系，就是抽象化。作为一个过程，抽象化就是忽略一些信息，从而把不同的实体当做同样的实体对待。 实现化：抽象化给出的具体实现，就是实现化。 脱耦：所谓耦合，就是两个实体的行为的某种强关联。而将它们的强关联去掉，就是耦合的解脱，或称脱耦。在这里，脱耦是指将抽象化和实现化之间的耦合解脱开，或者说是将它们之间的强关联改换成弱关联 关系图： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public interface Sourceable &#123; public void method(); &#125; public class SourceSub1 implements Sourceable &#123; @Override public void method() &#123; System.out.println("this is the first sub!"); &#125; &#125; public class SourceSub2 implements Sourceable &#123; @Override public void method() &#123; System.out.println("this is the second sub!"); &#125; &#125; public abstract class Bridge &#123; private Sourceable source; public void method()&#123; source.method(); &#125; public Sourceable getSource() &#123; return source; &#125; public void setSource(Sourceable source) &#123; this.source = source; &#125; &#125; public class MyBridge extends Bridge &#123; public void method()&#123; getSource().method(); &#125; &#125; public class BridgeTest &#123; public static void main(String[] args) &#123; Bridge bridge = new MyBridge(); /*调用第一个对象*/ Sourceable source1 = new SourceSub1(); bridge.setSource(source1); bridge.method(); /*调用第二个对象*/ Sourceable source2 = new SourceSub2(); bridge.setSource(source2); bridge.method(); &#125; &#125; 这样，就通过对Bridge类的调用，实现了对接口Sourceable的实现类SourceSub1和SourceSub2的调用。接下来我再画个图，大家就应该明白了，因为这个图是我们JDBC连接的原理，有数据库学习基础的，一结合就都懂了。 七、组合模式组合模式，将对象组合成树形结构以表示“部分-整体”的层次结构，组合模式使得用户对单个对象和组合对象的使用具有一致性。掌握组合模式的重点是要理解清楚 “部分/整体” 还有 ”单个对象“ 与 “组合对象” 的含义。 组合模式让你可以优化处理递归或分级数据结构。 《设计模式》：将对象组合成树形结构以表示“部分整体”的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。 涉及角色： Component：是组合中的对象声明接口，在适当的情况下，实现所有类共有接口的默认行为。声明一个接口用于访问和管理Component子部件。 Leaf：在组合中表示叶子结点对象，叶子结点没有子结点。 Composite：定义有枝节点行为，用来存储子部件，在Component接口中实现与子部件有关操作，如增加(add)和删除(remove)等。 比如现实中公司内各部门的层级关系，请看代码： Component：是组合中的对象声明接口，在适当的情况下，实现所有类共有接口的默认行为。声明一个接口用于访问和管理Component子部件。 关系图： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class TreeNode &#123; private String name; private TreeNode parent; private Vector&lt;TreeNode&gt; children = new Vector&lt;TreeNode&gt;(); public TreeNode(String name)&#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public TreeNode getParent() &#123; return parent; &#125; public void setParent(TreeNode parent) &#123; this.parent = parent; &#125; //添加孩子节点 public void add(TreeNode node)&#123; children.add(node); &#125; //删除孩子节点 public void remove(TreeNode node)&#123; children.remove(node); &#125; //取得孩子节点 public Enumeration&lt;TreeNode&gt; getChildren()&#123; return children.elements(); &#125; &#125; public class Tree &#123; TreeNode root = null; public Tree(String name) &#123; root = new TreeNode(name); &#125; public static void main(String[] args) &#123; Tree tree = new Tree("A"); TreeNode nodeB = new TreeNode("B"); TreeNode nodeC = new TreeNode("C"); nodeB.add(nodeC); tree.root.add(nodeB); System.out.println("build the tree finished!"); &#125; &#125; 应用场景：将多个对象组合在一起进行操作，常用于表示树形结构中，例如二叉树，数等。 八、享元模式享元模式的主要目的是实现对象的共享，即共享池，当系统中对象多的时候可以减少内存的开销，通常与工厂模式一起使用。 一提到共享池，我们很容易联想到Java里面的JDBC连接池，想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。 关系图： FlyWeightFactory负责创建和管理享元单元，当一个客户端请求时，工厂需要检查当前对象池中是否有符合条件的对象，如果有，就返回已经存在的对象，如果没有，则创建一个新对象，FlyWeight是超类。一提到共享池，我们很容易联想到Java里面的JDBC连接池，想想每个连接的特点，我们不难总结出：适用于作共享的一些个对象，他们有一些共有的属性，就拿数据库连接池来说，url、driverClassName、username、password及dbname，这些属性对于每个连接来说都是一样的，所以就适合用享元模式来处理，建一个工厂类，将上述类似属性作为内部数据，其它的作为外部数据，在方法调用时，当做参数传进来，这样就节省了空间，减少了实例的数量。 举个例子，数据库连接池： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConnectionPool &#123; private Vector&lt;Connection&gt; pool; /*公有属性*/ private String url = "jdbc:mysql://localhost:3306/test"; private String username = "root"; private String password = "root"; private String driverClassName = "com.mysql.jdbc.Driver"; private int poolSize = 100; private static ConnectionPool instance = null; Connection conn = null; /*构造方法，做一些初始化工作*/ private ConnectionPool() &#123; pool = new Vector&lt;Connection&gt;(poolSize); for (int i = 0; i &lt; poolSize; i++) &#123; try &#123; Class.forName(driverClassName); conn = DriverManager.getConnection(url, username, password); pool.add(conn); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /* 返回连接到连接池 */ public synchronized void release() &#123; pool.add(conn); &#125; /* 返回连接池中的一个数据库连接 */ public synchronized Connection getConnection() &#123; if (pool.size() &gt; 0) &#123; Connection conn = pool.get(0); pool.remove(conn); return conn; &#125; else &#123; return null; &#125; &#125; &#125;]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式（一）：创建型模式]]></title>
    <url>%2F2017%2F04%2F07%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考链接：Java开发中的23种设计模式详解(转)Java经典设计模式之五大创建型模式（附实例和详解） 一、设计模式分类总体来说设计模式分为三大类： 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 其实还有两类：并发型模式和线程池模式。用一个图片来整体描述一下： 二、设计模式的六大原则 开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类，后面的具体设计中我们会提到这点。 里氏代换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。 依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：针对接口编程，依赖于抽象而不依赖于具体。 接口隔离原则（Interface Segregation Principle）这个原则的意思是：使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想，从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 迪米特法则（最少知道原则）（Demeter Principle）一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 合成复用原则（Composite Reuse Principle）原则是尽量使用合成/聚合的方式，而不是使用继承。 三、工厂方法模式1、普通工厂模式建立一个工厂类，对实现了同一接口的类进行实例的创建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 创建类的共同接口======Active.java======public Interface Active&#123; public void eat(); public void spark();&#125;# 创建实现类======Cat.java======public class Cat implements Active&#123; @Override pubic void eat()&#123; System.out.println("I eat fish!"); &#125;&#125;======Dog.java======public class Dog implements Active&#123; @Override pubic void eat()&#123; System.out.println("I eat meat!"); &#125;&#125;# 创建工厂类======ActiveFactory.java======public class ActiveFactory&#123; public Active produce(String type)&#123; if(type.equals("cat"))&#123; return new Cat(); &#125;else if(type.equals("dog"))&#123; return new Dog(); &#125;else &#123; System.out.println("Error! please intput correct type!"); return null; &#125; &#125; # test public static void main(String[] args)&#123; ActiveFactory activeFactory = new ActiveFactory(); Active active = activeFactory.produce("cat"); active.eat(); //output ===&gt; "I eat fish!" &#125;&#125; 2.多个工厂方法模式在普通工厂方法模式中，如果传递的字符串出错，则不能正确创建对象，而多个工厂方法模式是提供多个工厂方法，分别创建对象12345678910111213141516171819======ActiveFactory.java======public class ActiveFactory&#123; public Active produceCat()&#123; return new Cat(); &#125; public Active produceDog()&#123; return new Dog(); &#125; # test public static void main(String[] args)&#123; ActiveFactory activeFactory = new ActiveFactory(); Active active = activeFactory.produceCat("cat"); active.eat(); //output ===&gt; "I eat fish!" &#125;&#125; 3.静态工厂模式将多个工厂方法模式中的方法置为静态的，不需要创建实例，直接调用即可123456789101112131415161718======ActiveFactory.java======public class ActiveFactory&#123; public static Active produceCat()&#123; return new Cat(); &#125; public static Active produceDog()&#123; return new Dog(); &#125; # test public static void main(String[] args)&#123; Active active = ActiveFactory.produceCat("cat"); active.eat(); //output ===&gt; "I eat fish!" &#125;&#125; 四、抽象工厂模式在一般的工厂模式中，类的创建依赖工厂类，如果要扩展程序，必须对工厂类进行修改，违背了闭包原则，因此需要用抽象工厂模式，创建多个工厂类，一旦需要增加新的功能，直接增加新的工厂类就可以123456789101112131415161718192021222324======Provider.java======public interface Provider&#123; public Active produce();&#125;======ActiveCatFactory.java======public class ActiveCatFactory implements Provider&#123; public Active produce()&#123; return new Cat(); &#125;&#125;======ActiveDogFactory.java======public class ActiveDogFactory implements Provider&#123; public Active produce()&#123; return new Dog(); &#125;&#125; 五、单例模式在Java应用中，单例对象能保证在一个JVM中，该对象只有一个实例存在. 12345678910111213141516171819202122232425======Singleton.java======public class Singleton &#123; /* 持有私有静态实例，防止被引用，此处赋值为null，目的是实现延迟加载 */ private static Singleton instance = null; /* 私有构造方法，防止被实例化 */ private Singleton() &#123; &#125; /* 静态工程方法，创建实例 */ public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; /* 如果该对象被用于序列化，可以保证对象在序列化前后保持一致 */ public Object readResolve() &#123; return instance; &#125; &#125; 这个类可以满足基本要求，但是，像这样毫无线程安全保护的类，如果我们把它放入多线程的环境下，肯定就会出现问题了，如何解决？我们首先会想到对getInstance方法加synchronized关键字，如下： 123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; 但是，synchronized关键字锁住的是这个对象，这样的用法，在性能上会有所下降，因为每次调用getInstance()，都要对对象上锁，事实上，只有在第一次创建对象的时候需要加锁，之后就不需要了，所以，这个地方需要改进。我们改成下面这个：12345678910public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (instance) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; 改程序在多线程的时候还是有可能发生错误，因此需要进一步改善123456private static class SingletonFactory&#123; private static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonFactory.instance; &#125; 参考链接 六、建造者模式工厂类模式提供的是创建单个类的模式，而建造者模式则是将各种产品集中起来进行管理，用来创建复合对象，所谓复合对象就是指某个类具有不同的属性，其实建造者模式就是前面抽象工厂模式和最后的Test结合起来得到的。 123456789101112131415161718192021public class Builder &#123; private List&lt;Active&gt; list = new ArrayList&lt;Active&gt;(); public void produceCatActive(int count)&#123; for(int i=0; i&lt;count; i++)&#123; list.add(new Cat()); &#125; &#125; public void produceDogActive(int count)&#123; for(int i=0; i&lt;count; i++)&#123; list.add(new Dog()); &#125; &#125; public static void main(String[] args)&#123; Builder builder = new Builder(); builder.produceMailSender(10); &#125; &#125; 七、原型模式原型模式虽然是创建型的模式，但是与工厂模式没有关系，从名字即可看出，该模式的思想就是将一个对象作为原型，对其进行复制、克隆，产生一个和原对象类似的新对象。本小结会通过对象的复制，进行讲解。在Java中，复制对象是通过clone()实现的，先创建一个原型类：1234567public class Prototype implements Cloneable &#123; public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; &#125; 深浅复制的例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Prototype implements Cloneable, Serializable &#123; private static final long serialVersionUID = 1L; private String string; private SerializableObject obj; /* 浅复制 */ public Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; /* 深复制 */ public Object deepClone() throws IOException, ClassNotFoundException &#123; /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); &#125; public String getString() &#123; return string; &#125; public void setString(String string) &#123; this.string = string; &#125; public SerializableObject getObj() &#123; return obj; &#125; public void setObj(SerializableObject obj) &#123; this.obj = obj; &#125; &#125; class SerializableObject implements Serializable &#123; private static final long serialVersionUID = 1L; &#125;]]></content>
      <categories>
        <category>笔记</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm学习笔记-storm.yaml配置项]]></title>
    <url>%2F2017%2F03%2F18%2Fstorm%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-storm-yaml%E9%85%8D%E7%BD%AE%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[配置项 配置说明 storm.zookeeper.servers Zookeeper服务器列表 storm.zookeeper.port zookeeper 连接端口 storm.local.dir storm使用的本地文件系统目录（必须存在并且storm进程可读写） storm.cluster.mode Storm集群运行模式([distributedllocall) storm.local.mode.zmq Local模式下是否使用ZeroMQ作消息系统，如果设置为false则使用java消息系统默认为false storm.zookeeper.root ZooKeeper中Storm的根目录位置 storm.zookeeper.session.timeout 客户端连接ZooKeeper超时时间 storm.id 运行中拓扑的id,由stormname和一个唯随机数组成 nimbus.host nimbus服务器地址 nimbus.thrift.port nimbus的thrift监听端口 nimbus.childopts 通过storm-deploy项目部署时指定给nimbus进程的jvm选项 nimbus.task.timeout.secs 心跳超时时间，超时后nimbus会认为task死掉并重分配给另一个地址 nimbus.monitor. freq .secs nimbu归心跳和重分配任务的时间间隔注意如果是机器宫掉nimbus会立即接管并处理 nimbus.supervisor.timeout.secs supervisor的心跳超时时间，一旦超过nimbus会认为该supervisor已死并停止为它分发新任务 nimbus.task.launch.secs task启动时的一个特殊超时设置在启动后第一次心跳前会使用该值来临时 替代nimbus.task.timeout.secs. nimbus.reassign 当发现task失败是nimbus是否重新分配执行，默认为真 nimbus.file.copy.expiration.secs nimbus判断上传I下载链接的超时时间，当空闲时间超过该设定时nimbus会认为链接死掉并主动断开 ui.port Storm UI的服务端口 drpc.servers DRPC服务器列表，以便DRPCSpout知道和谁通讯 drpc.port Storm DRPC的服务端口 supervisor.slots.ports supervisor上能够运行workers的端口列表.每个worker占用一个端口,且每个端口只运行一个worker.通过这项配置可以调整每台机器上运行的worker数.(调整slot数/每机) supervisor.childopts 在storm-deploy项目中使用,用来配置supervisor守护进程的jvm选项 supervisor.worker.timeout.secs supervisor中的worker心跳超时时间,一旦超时supervisor会尝试重启worker进程. supervisor.worker.start.timeout.secs supervisor初始启动时，worker的心跳超时时间，当超过该时间supervisor会尝试重启worker。因为JVM初始启动和配置会带来的额外消耗，从而使得第一次心跳会超过supervisor.worker.timeout.secs的设定 supervisor.enable supervisor是否应当运行分配给他的workers.默认为true,该选项用来进行Storm的单元测试,一般不应修改. supervisor.heartbeat.frequency.secs supervisor心跳发送频率(多久发送一次) supervisor.monitor.frequency.secs supervisor 检查worker心跳的频率 worker.childopts supervisor启动worker时使用的jvm选项.所有的”%ID%”字串会被替换为对应worker的标识符 worker.heartbeat.frequency.secs worker的心跳发送时间间隔 task.heartbeat.frequency.secs task汇报状态心跳时间间隔 task.refresh.poll.secs task与其他tasks之间链接同步的频率.(如果task被重分配,其他tasks向它发送消息需要刷新连接).一般来讲，重分配发生时其他tasks会理解得到通知。该配置仅仅为了防止未通知的情况。 topology.debug 如果设置成true，Storm将记录发射的每条信息。 topology.optimize master是否在合适时机通过在单个线程内运行多个task以达到优化topologies的目的. topology.workers 执行该topology集群中应当启动的进程数量.每个进程内部将以线程方式执行一定数目的tasks.topology的组件结合该参数和并行度提示来优化性能 topology.ackers topology中启动的acker任务数.Acker保存由spout发送的tuples的记录，并探测tuple何时被完全处理.当Acker探测到tuple被处理完毕时会向spout发送确认信息.通常应当根据topology的吞吐量来确定acker的数目，但一般不需要太多.当设置为0时,相当于禁用了消息可靠性,storm会在spout发送tuples后立即进行确认. topology.message.timeout.secs topology中spout发送消息的最大处理超时时间.如果一条消息在该时间窗口内未被成功ack,Storm会告知spout这条消息失败。而部分spout实现了失败消息重播功能。 topology.kryo.register 注册到Kryo(Storm底层的序列化框架)的序列化方案列表.序列化方案可以是一个类名,或者是com.esotericsoftware.kryo.Serializer的实现. topology.skip.missing.kryo.registrations Storm是否应该跳过它不能识别的kryo序列化方案.如果设置为否task可能会装载失败或者在运行时抛出错误. topology.max.task.parallelism 在一个topology中能够允许的最大组件并行度.该项配置主要用在本地模式中测试线程数限制. topology.max.spout.pending 一个spout task中处于pending状态的最大的tuples数量.该配置应用于单个task,而不是整个spouts或topology. topology.state.synchronization.timeout.secs 组件同步状态源的最大超时时间(保留选项,暂未使用) topology.stats.sample.rate 用来产生task统计信息的tuples抽样百分比 topology.fall.back.on.java.serialization topology中是否使用java的序列化方案 zmq.threads 每个worker进程内zeromq通讯用到的线程数 zmq.linger.millis 当连接关闭时,链接尝试重新发送消息到目标主机的持续时长.这是一个不常用的高级选项,基本上可以忽略. java.library.path JVM启动(如Nimbus,Supervisor和workers)时的java.library.path设置.该选项告诉JVM在哪些路径下定位本地库.]]></content>
      <categories>
        <category>笔记</category>
        <category>storm</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下U盘分区]]></title>
    <url>%2F2017%2F03%2F18%2Flinux%E4%B8%8BU%E7%9B%98%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[转载自：https://www.zybuluo.com/fanisfun/note/677301]]></content>
      <categories>
        <category>解决方案</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>分区</tag>
        <tag>U盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux磁盘分区和挂载]]></title>
    <url>%2F2017%2F03%2F18%2Flinux%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA%E5%92%8C%E6%8C%82%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[磁盘分区显示磁盘和分区情况sudo fdisk -l 1234567891011121314151617181920Disk /dev/xvda: 42.9 GB, 42949672960 bytes255 heads, 63 sectors/track, 5221 cylinders, total 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x00054506 Device Boot Start End Blocks Id System/dev/xvda1 * 2048 75495423 37746688 83 Linux/dev/xvda2 75497470 83884031 4193281 5 Extended/dev/xvda5 75497472 83884031 4193280 82 Linux swap / SolarisDisk /dev/xvdb: 107.4 GB, 107374182400 bytes255 heads, 63 sectors/track, 13054 cylinders, total 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x00000000Disk /dev/xvdb doesn\'t contain a valid partition table 对磁盘进行分区操作sudo fdisk /dev/xvdb 123456789101112131415161718Command (m for help): m ## 打印命令列表Command action a toggle a bootable flag # 将分区标记为可启动盘 b edit bsd disklabel c toggle the dos compatibility flag d delete a partition # 删除一个分区 l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition\'s system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only) 输入n，增加硬盘一个新分区选择e，指定分区为扩展分区（extended）出现Partition number(1-4)时，输入１表示只分一个区输入p，打印分区表123456789Disk /dev/xvdb: 107.4 GB, 107374182400 bytes255 heads, 63 sectors/track, 13054 cylinders, total 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0xf88fabb6 Device Boot Start End Blocks Id System/dev/xvdb1 2048 209715199 104856576 5 Extended 硬盘格式化操作sudo mkfs -t ext4 /dev/xvdb 1234567mkfs 命令的语法如下：mkfs [-V] [-t fstype] [fs-options] filesys说明：-V 显示简要的使用方法。-t 指定要建立何种文件系统，如：ext3, ext4。fs 指定建立文件系统时的参数。-v 显示版本信息与详细的使用方法。 挂载硬盘分区sudo mount -t ext4 /dev/xvdb /deb/sdb #指定硬盘分区文件系统为ext4，将/dev/xvdb 分区挂载到/dev/sdb sudo df -h #查看分区挂载情况123456789Filesystem Size Used Avail Use% Mounted on/dev/xvda1 36G 2.3G 32G 7% /none 4.0K 0 4.0K 0% /sys/fs/cgroupudev 476M 12K 476M 1% /devtmpfs 98M 392K 97M 1% /runnone 5.0M 4.0K 5.0M 1% /run/locknone 486M 0 486M 0% /run/shmnone 100M 0 100M 0% /run/user/dev/xvdb 99G 60M 94G 1% /dev/sdb 1234567891011121314# mount命令详解mount [-afFnrsvw] [-t vfstype] [-L label] [-o options] device dirmount [-lhv]说明：-a 加载文件/etc/fstab中设置的所有设备。-f 不实际加载设备。可与-v等参数同时使用以查看mount的执行过程。-F 需与-a参数同时使用。所有在/etc/fstab中设置的设备会被同时加载，可加快执行速度。-t vfstype 指定加载的文件系统类型，如：ext3, ext4。-L label 给挂载点指定一个标签名称。-l 显示分区的label。-h 显示帮助信息。-v 显示mount的版本信息。device 要挂载的分区或文件。如果device是一个文件，挂载时须加上 -o loop参数。dir 分区的挂载点。 设置开机自动挂载sudo vi /etc/fstab UUID=XXXXX /dev/sdb ext4 defaults 0 3 UUID获取命令： ls -l /dev/disk/by-uuid/ 或者 blkid /dev/xvdb fstab配置详解：/etc/fstab 中一共有６列： file system：指定要挂载的文件系统的设备名称（如：/dev/sdb）。也可以采用UUID，UUID可以通过使用blkid命令来查看（如：blkid /dev/sdb）指定设备的UUID号。 mount point：挂载点。就是自己手动创建一个目录，然后把分区挂载到这个目录下。 type：用来指定文件系统的类型。如：ext3, ext4, ntfs等。 option dump：0表示不备份；１表示要将整个中的内容备份。此处建议设置为0。 pass：用来指定fsck如何来检查硬盘。0表示不检查；挂载点为分区／（根分区）必须设置为1，其他的挂载点不能设置为1；如果有挂载ass设置成大于1的值，则在检查完根分区后，然后按pass的值从小到大依次检查，相同数值的同时检查。如：/home 和 /boot 的pass 设置成2，/devdata 的pass 设置成3，则系统在检查完根分区，接着同时检查/boot和/home，再检查/devdata。]]></content>
      <categories>
        <category>解决方案</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka操作命令]]></title>
    <url>%2F2017%2F03%2F16%2Fkafka%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[kafka常用操作命令123456789101112131415## 1 创建新的topickafka-topic --create --zookeeper zkhost:2181/kafka --replication-factor 1 --partitions 3 --topic topic-name## 2 查询所有的topickafka-topics --list --zookeeper zkhost:2181/kafka## 3 查看topic详细信息kafka-topics --describe --zookeeper zkhost:2181/kafka --topic topicname## 4 生产消息kafka-console-producer --broker-list kafkahost:9092 --topic topicname## 5 消费消息kafka-console-consumer --bootstrap-server kafkahost:9092 --topic topicname]]></content>
      <categories>
        <category>笔记</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase基本操作命令]]></title>
    <url>%2F2017%2F03%2F09%2FHbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、基本命令： 建表：create ‘testtable’,’coulmn1’,’coulmn2’ 也可以建表时加coulmn的属性如： 1create 'testtable',&#123;NAME =&gt; 'coulmn1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '10', COMPRESSION =&gt; 'LZO', TTL =&gt; '30000', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'false'&#125;, &#123;NAME =&gt; 'coulmn', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '30', COMPRESSION =&gt; 'LZO', TTL =&gt; '30000', IN_MEMORY =&gt; 'true'&#125; (其中的属性有versions：设置历史版本数，TTL：过期时间，COMPRESSION：压缩方式，当配置lzo的情况) 删除表：drop ‘testtable’ （删除表之前先要禁用表，命令disable ‘testtable’） 启用和禁用表： enable ‘testtable’ 和disable ‘testtable’ 其它的基本命令：describe ‘testtable’（查看表结构），alert 修改表结构，list 列出所有表。 查询前10条数据: scan&#39;testtable&#39;,{COLUMNS=&gt;&#39;info&#39;,LIMIT=&gt;10,STARTROW=&gt;&#39;666632331200000020160305&#39;,STOPROW=&gt;&#39;666632331200000020160308&#39;} 分区合并合并两个预分区，合并预分区要提供该区encode值，该值在16010hbase管理web可查 merge_region &#39;region1的encode值&#39;,&#39;region2的encode值&#39; 手动触发major_compact动作cd /opt/hbase-1.2.1/bin./hbase shellmajor_compact ‘table’quit 手动触发flush动作cd /opt/hbase-1.2.1/bin./hbase shellflush ‘table1’flush ‘table2’quit 每次hbase启动，最好设置一下参数，让hbase区自动均衡cd /opt/hbase-1.2.1/bin./hbase shellbalance_switch true 二、日常维护的命令1，major_compact ‘testtable’，通常生产环境会关闭自动major_compact(配置文件中hbase.hregion.majorcompaction设为0)，选择一个晚上用户少的时间窗口手工major_compact，如果hbase更新不是太频繁，可以一个星期对所有表做一次major_compact，这个可以在做完一次major_compact后，观看所有的storefile数量，如果storefile数量增加到major_compact后的storefile的近二倍时，可以对所有表做一次major_compact，时间比较长，操作尽量避免高锋期。 2，flush ‘testtable’，将所有memstore刷新到hdfs，通常如果发现regionserver的内存使用过大，造成该机的regionserver很多线程block，可以执行一下flush操作，这个操作会造成hbase的storefile数量剧增，应尽量避免这个操作，还有一种情况，在hbase进行迁移的时候，如果选择拷贝文件方式，可以先停写入，然后flush所有表，拷贝文件。 3，balance_switch true或者balance_switch flase，配置master是否执行平衡各个regionserver的region数量，当我们需要维护或者重启一个regionserver时，会关闭balancer，这样就使得region在regionserver上的分布不均，这个时候需要手工的开启balance。 三、重启一个regionserverbin/graceful_stop.sh —restart —reload —debug nodename 这个操作是平滑的重启regionserver进程，对服务不会有影响，他会先将需要重启的regionserver上面的所有region迁移到其它的服务器，然后重启，最后又会将之前的region迁移回来，但我们修改一个配置时，可以用这种方式重启每一台机子，这个命令会关闭balancer，所以最后我们要在hbase shell里面执行一下balance_switch true，对于hbase regionserver重启，不要直接kill进程，这样会造成在zookeeper.session.timeout这个时间长的中断，也不要通过bin/hbase-daemon.sh stop regionserver去重启，如果运气不太好，-ROOT-或者.META.表在上面的话，所有的请求会全部失败。 四、关闭下线一台regionserverbin/graceful_stop.sh —stop nodename 和上面一样，系统会在关闭之前迁移所有region，然后stop进程，同样最后我们要手工balance_switch true，开启master的region均衡。 五、检查region是否正常以及修复bin/hbase hbck (检查) bin/hbase hbck -fix （修复） 会返回所有的region是否正常挂载，如没有正常挂载可以使用下一条命令修复，如果还是不能修复，那需要看日志为什么失败，手工处理。 六、hbase的迁移1，copytable方式 bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable —peer.adr=zookeeper1,zookeeper2,zookeeper3:/hbase ‘testtable’ 目前0.92之前的版本的不支持多版本的复制，0.94已经支持多个版本的复制。当然这个操作需要添加hbase目录里的conf/mapred-site.xml，可以复制hadoop的过来。 2，Export/Import bin/hbase org.apache.hadoop.hbase.mapreduce.Export testtable /user/testtable [versions] [starttime] [stoptime] bin/hbase org.apache.hadoop.hbase.mapreduce.Import testtable /user/testtable 跨版本的迁移，我觉得是一个不错的选择，而且copytable不支持多版本，而export支持多版本，比copytable更实用一些。 3，直接拷贝hdfs对应的文件 首先拷贝hdfs文件，如bin/hadoop distcp hdfs://srcnamenode:9000/hbase/testtable/ hdfs://distnamenode:9000/hbase/testtable/ 然后在目的hbase上执行bin/hbase org.jruby.Main bin/add_table.rb /hbase/testtable 生成meta信息后，重启hbase 这个操作是简单的方式，操作之前可以关闭hbase的写入，执行flush所有表（上面有介绍）,再distcp拷贝，如果hadoop版本不一致，可以用hftp接口的方式，我推荐使用这种方式，成本低。]]></content>
      <categories>
        <category>笔记</category>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式大数据分析平台环境搭建]]></title>
    <url>%2F2017%2F03%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[安装ubuntu在windows上使用ultraISO软件来刻录Ubuntu系统安装盘，设置U盘启动，安装系统参考链接：Ubuntu 14.04 64位系统安装cuda8.0+cudnn7.5+opencv+caffe 血泪教程 安装NVIDIA驱动http://www.nvidia.cn/Download/index.aspx?lang=cn选择合适型号，下载驱动包 首先按Ctrl+Alt+F1，进入text mode，关闭显示驱动 sudo service lightdm stop 增加执行权限 12sudo chmod +x NVIDIA-Linux-x86_64-375.39.runsudo ./NVIDIA-Linux-x86_64-375.39.run 安装成功后，重启图像界面服务 sudo service lightdm start 安装cuda+cudnn+opencv+caffe参考这个链接：原-Ubuntu-14-04-64位系统安装cuda8-0-cudnn7-5-opencv-caffe-血泪教程 caffe_github_link:https://github.com/BVLC/caffe.gitopencv_github_link:https://github.com/opencv/opencv 配置静态ip + 安装ssh-server静态ipubuntu14.04设置静态ip 找到文件并作如下修改： sudo vim /etc/network/interfaces 修改如下部分： auto eth0iface eth0 inet staticaddress 192.168.1.4gateway 192.168.1.1 #这个地址你要确认下 网关是不是这个地址netmask 255.255.255.0 network 192.168.0.0broadcast 192.168.0.255 修改dns解析 因为以前是dhcp解析，所以会自动分配dns服务器地址 而一旦设置为静态ip后就没有自动获取到的dns服务器了 要自己设置一个 sudo vim /etc/resolv.conf 写上一个公网的DNS nameserver 202.114.0.131(公网dns服务器)nameserver 202.114.0.242（公网dns服务器）nameserver 192.168.1.1（局域网内可以访问外网的机器，非网关服务器放在前面） sudo vim /etc/resolvconf/resolv.d/base nameserver 202.114.0.131(公网dns服务器)nameserver 202.114.0.242（公网dns服务器）nameserver 192.168.1.1（局域网内可以访问外网的机器，非网关服务器放在前面） 重启网卡： sudo /etc/init.d/network restart 安装ssh-server 判断是否安装ssh服务Ubuntu系统默认安装ssh-client，如果想远程登录主机，就需要安装ssh-server，如下命令： 1ps -e|grep sshps -e|grep ssh ssh-agent表示ssh-client启动，sshd表示ssh-server启动了 如果缺少sshd，说明ssh服务没有启动或者没有安装。1234sudo apt-get install openssh-client #安装ssh-client命令sudo apt-get install openssh-server #安装ssh-server命令sudo /etc/init.d/ssh start #启动服务ps -e|grep sshps -e|grep ssh #查看是否正确启动。 配置ssh服务端口默认端口是 22sudo gedit /etc/ssh/sshd_configsudo /etc/init.d/ssh restart #重启ssh服务生效 设置开机自动启动 安装storm+kafka+zookeeper+hdfs+hbase+jdk前期准备工作 首先下载好这几个文件，这里用的版本分别是： storm 0.9.6 zookeeper 3.4.0 jdk 1.8.0.92 kafka 2.11.0.10.0.0 hadoop 2.6.4 hbase 1.2.4 设置ip地址映射sudo vi /etc/hosts 12345678910111213141516171819202122232425262728293031323334# test192.168.0.136 pc1080192.168.0.116 pcwrp192.168.0.117 pcxrr192.168.0.136 cloud01192.168.0.116 cloud02192.168.0.117 cloud03192.168.0.136 hadoop01192.168.0.116 hadoop02192.168.0.117 hadoop03192.168.0.136 zk01192.168.0.116 zk02192.168.0.117 zk03192.168.0.136 kafka01192.168.0.116 kafka02192.168.0.117 kafka03192.168.0.136 kafka01192.168.0.116 kafka02192.168.0.117 kafka03192.168.0.136 storm01192.168.0.116 storm02192.168.0.117 storm03192.168.0.136 hbase01192.168.0.116 hbase02192.168.0.117 hbase03# end test 这里面需要注意一下，之所以需要配置多组ip映射，是因为在相应的软件中很多默认参数需要用到这些映射 三台机机器实现互相ssh免密码登录，并且实现自身环回登录参考链接：ssh免密码登录 设置环境变量vi ~/.bashrc 123456789101112131415161718192021222324252627282930313233343536# javaexport JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin# hadoopexport MAVEN_HOME=/home/hadoop/cloud/apache-maven-3.3.9/export ZOOKEEPER_HOME=/home/hadoop/cloud/zookeeper-3.4.9/export HADOOP_HOME=/home/hadoop/cloud/hadoop-2.6.4/export HBASE_HOME=/home/hadoop/cloud/hbase-1.2.4/export FLUME_HOME=/home/hadoop/cloud/apache-flume-1.6.0/export KAFKA_HOME=/home/hadoop/cloud/kafka_2.11-0.10.0.0/export STORM_HOME=/home/hadoop/cloud/apache-storm-0.9.6/export HIVE_HOME=/home/hadoop/cloud/apache-hive-1.2.1/export HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_PREFIX=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib/native"#export CHUKWA_HOME=/home/hadoop/cloud/chukwa-0.8.0/#export CHUKWA_CONF_DIR=$CHUKWA_HOME/etc/chukwaPATH=$PATH:$HOME/binPATH=$JAVA_HOME/bin/:$PATHPATH=$MAVEN_HOME/bin/:$PATHPATH=$ZOOKEEPER_HOME/bin/:$PATHPATH=$HBASE_HOME/bin/:$PATHPATH=$FLUME_HOME/bin/:$PATHPATH=$KAFKA_HOME/bin/:$PATHPATH=$STORM_HOME/bin/:$PATHPATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/binPATH=$HIVE_HOME/bin/:$PATHPATH=$PATH:$CHUKWA_HOME/bin:$CHUKWA_HOME/sbinexport PATH 安装zookeeper 下载安装包，解压到 /home/hadoop/cloud 目录下 添加环境变量 配置/etc/hosts 文件，做好ip地址映射 cat zoo_sample.cfg &gt;&gt; zoo.cfg 编辑 zoo.cfg文件 12345678910111213tickTime=2000initLimit=10syncLimit=5# zookeeper数据保存路径dataDir=/home/hadoop/cloud/zookeeper-3.4.9/data/clientPort=2181maxClientCnxns=150maxSessionTimeout=100000autopurge.snapRetainCount=6autopurge.purgeInterval=48server.1=zk01:2888:3888server.2=zk02:2888:3888server.3=zk03:2888:3888 在节点配置的dataDir目录中创建一个myid文件，里面内容为一个数字，用来标识当前主机，$ZOOKEEPER_HOME/conf/zoo.cfg文件中配置的server.X，则myid文件中就输入这个数字X。（即在每个节点上新建并设置文件myid，其内容与zoo.cfg中的id相对应） 123cd /home/hadoop/cloud/zookeeper-3.4.9/data/mkdir myidvi myid 启动zookeeper 1$ZOOKEEPER_HOME/bin/zkServer.sh start 安装storm 下载安装包，解压到 /home/hadoop/cloud 目录下 设置环境变量 修改 storm 主目录下conf/storm.yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243storm.zookeeper.servers: - "zk01" - "zk02" - "zk03"storm.zookeeper.port: 2181storm.zookeeper.root: "/storm"nimbus.host: "storm01"supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 - 6704 - 6705 - 6706 - 6707 - 6708storm.local.dir: "/home/hadoop/cloud/apache-storm-0.9.6/local"drpc.servers: - "storm01"drpc.port: 3772drpc.worker.threads: 100drpc.queue.size: 1024drpc.invocations.port: 3773drpc.request.timeout.secs: 800drpc.childopts: "-Xmx2048m"drpc.port: 3772drpc.worker.threads: 100drpc.queue.size: 1024drpc.invocations.port: 3773drpc.request.timeout.secs: 800drpc.childopts: "-Xmx2048m"#JVM options of worker#edited by persistsuperivisor.childopts: "-Xmx2048m"worker.childopts: "-Xmx2048m"#JVM options of worker#edited by persistsuperivisor.childopts: "-Xmx2048m"worker.childopts: "-Xmx2048m" 启动storm命令（启动storm之前需要启动zookeeper） 123nimubs-host: nohup bin/storm nimbus&amp;nimubs-host: nohup bin/storm ui&amp;superivisor-hosts: nohup bin/storm superivisor&amp; 浏览器输入：http://pc1080:8080 查看storm UI界面 安装hadoop 下载安装包，解压到 /home/hadoop/cloud 目录下 设置环境变量 修改主目录下 etc/hadoop-env.sh 1export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/ 修改主目录下 etc/yarn-env.sh 1export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/ 修改 etc/hadoop/mapred-env.sh 1export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/ 修改主目录下 etc/core-site.xml 123456789101112131415161718192021222324252627&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/cloud/hadoop-2.6.4/data/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zk01:2181,zk02:2181,zk03:2181&lt;/value&gt; &lt;description&gt;这里是ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点&lt;/description&gt; &lt;/property&gt; # add ours here &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;description&gt;允许所有用户组用户代理&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;hadoop01&lt;/value&gt; &lt;description&gt;允许挂载的主机域名&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 修改主目录下 etc/hdfs-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nfs.exports.allowed.hosts&lt;/name&gt; &lt;value&gt;hadoop01 rw;hadoop02 rw;hadoop03 rw;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop01:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/cloud/hadoop-2.6.4/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/hadoop/cloud/hadoop-2.6.4/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; ## add options here &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;/name&gt; &lt;value&gt;NEVER&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.max-component-length&lt;/name&gt; &lt;value&gt;255&lt;/value&gt; &lt;description&gt;Defines the maximum number of bytes in UTF-8 encoding in each component of a path. A value of 0 will disable the check.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.max-directory-items&lt;/name&gt; &lt;value&gt;6000000&lt;/value&gt; &lt;description&gt;Defines the maximum number of items that a directory may contain. Cannot set the property to a value less than 1 or more than 6400000.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.min-block-size&lt;/name&gt; &lt;value&gt;1048576&lt;/value&gt; &lt;description&gt;Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.max-blocks-per-file&lt;/name&gt; &lt;value&gt;1048576&lt;/value&gt; &lt;description&gt;Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.max-xattrs-per-inode&lt;/name&gt; &lt;value&gt;32&lt;/value&gt; &lt;description&gt; Maximum number of extended attributes per inode. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.fs-limits.max-xattr-size&lt;/name&gt; &lt;value&gt;16384&lt;/value&gt; &lt;description&gt; The maximum combined size of the name and value of an extended attribute in bytes. It should be larger than 0, and less than or equal to maximum size hard limit which is 32768. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.transfer.threads&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改主目录下 etc/httpfs-env.sh 123456789101112131415161718export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/export HTTPFS_HOME=/home/hadoop/cloud/hadoop-2.6.4/export HTTPFS_CONFIG=/home/hadoop/cloud/hadoop-2.6.4/etc/hadoopexport CATALINA_BASE=/home/hadoop/cloud/hadoop-2.6.4/share/hadoop/httpfs/tomcatexport HTTPFS_LOG=$&#123;HTTPFS_HOME&#125;/httpfs/logsexport HTTPFS_TEMP=$&#123;HTTPFS_HOME&#125;/httpfs/tempexport HTTPFS_HTTP_PORT=14000export HTTPFS_ADMIN_PORT=`expr $&#123;HTTPFS_HTTP_PORT&#125; + 1` 修改 etc/hadoop/slavers 123hadoop01hadoop02hadoop03 以上只是部分配置，其中hadoop01是默认的主节点，默认配置的主机 启动hadoop 首先需要启动zookeeper 格式化hdfs： bin/hdfs namenode -format 启动hdfs和yarn： sbin/start-all.sh,启动成功master出现NameNode，SecondaryNameNode，ResourceManger进程，slave节点出现DataNode和NodeManager注意事项：如果增加了多个ip映射，那么有可能提醒是否需要添加host到ssh表，这种情况框需要重新设置ssh互通 web查看hdfs: https://hadoop01:50070/yarn: https://hadoop01:8088/ 遇到的问题：主节点上启动start-all.sh, 只有主节点的NameNode和SecondaryNameNode和DataNode启动，从节点的DataNode没有启动 重启部分坏死节点：bin/Hadoop-daemon.sh start DataNodebin/Hadoop-daemon.sh start jobtracker 动态加入新节点：bin/Hadoop-daemon.sh —config ./conf start DataNodebin/Hadoop-daemon.sh —config ./conf start tasktracker 更新配置文件命令：1for ip in `seq 2 3`; do scp /home/hadoop/cloud/hadoop-2.6.4/etc/hadoop/* hadoop@hadoop0$ip:/home/hadoop/cloud/hadoop-2.6.4/etc/hadoop/; done 更新环境变量：1for ip in `seq 2 3`; do scp /home/hadoop/.bashrc hadoop@hadoop0$ip:/home/hadoop/; done 安装HBase 下载安装包，解压到 /home/hadoop/cloud 目录下 修改～/.bashrc文件，添加HBASE_HOME环境变量； 修改 conf/hbase-env.sh 1234567export JAVA_HOME=/home/hadoop/cloud/jdk1.8.0_92/export HBASE_CLASSPATH=/opt/hbase/conf (extral classpath like java, Optional)# 此配置信息，设置由zk集群管理，故为false export HBASE_MANAGES_ZK=false export HBASE_HOME=/opt/hbase (Optional)#Hbase日志目录 export HBASE_LOG_DIR=$HBASE_HOME/logs(Optional) 修改 conf/hbase-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hbase01:9000/hbase&lt;/value&gt; &lt;description&gt;The directory shared by region servers.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;Property from ZooKeeper config zoo.cfg. The port at which the clients will connect. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.bindAddress&lt;/name&gt; &lt;value&gt;192.168.100.101&lt;/value&gt; &lt;description&gt;HBase Master web ui bind address&lt;/description&gt; **必须填写master节点地址** &lt;/property&gt; &lt;property&gt; &lt;name&gt;zookeeper.session.timeout&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hbase01,hbase02,hbase03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/cloud/hbase-1.2.4/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/hadoop/cloud/hbase-1.2.4/zookeeper&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.maxClientCnxns&lt;/name&gt; &lt;value&gt;300&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;/name&gt; &lt;value&gt;NEVER&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 regionservers 123hbase01hbase02 #设置regionserver的节点hbase03 同步配置 1for ip in `seq 2 3`; do scp /home/hadoop/cloud/hbase-1.2.4/conf/* hadoop@hadoop0$ip:/home/hadoop/cloud/hbase-1.2.4/conf/; done 启动hbase 1234bin/start-hbase.sh启动后，jps可以看到有这两个进程HRegionServerHMaster 启动Hbase Shell 1bin/hbase shell 访问HBase UI查看hbase管理界面http://192.168.181.66:16010 hbase日常维护参考链接：Hbase基本操作命令 注意hbase集群安装的大坑 hbase-site.xml必须添加hbase.master.info.bindAddress配置项，设置为master节点的ip地址 设置/etc/hosts文件，注释掉 127.0.1.1 hostname这一行，替换成局域网ip hostname，否则HRegionServer 无法链接Master节点 错误参考链接：http://www.cnblogs.com/colorfulkoala/archive/2012/07/09/2583841.html 安装kafka参考链接：kafka]]></content>
      <categories>
        <category>解决方案</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>深度学习</tag>
        <tag>大数据</tag>
        <tag>cuda</tag>
        <tag>opencv</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu制作UbuntuLiveCD+备份系统镜像教程]]></title>
    <url>%2F2017%2F03%2F03%2FUbuntu%E5%88%B6%E4%BD%9CUbuntuLiveCD-%E5%A4%87%E4%BB%BD%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[安装UbuntuLiveCD 准备：U盘，ubuntu官方镜像文件 windows解决方案 安装UltralSO，打开ubuntu镜像文件，写入U盘 安装UNetBootin，打开Ubuntu镜像文件，写入U盘 linux解决方案 插入U盘 sudo fdisk -l cd ubuntu-iso的目录 sudo dd if=ubuntu-14.04.5-desktop-amd64.iso of=/dev/sdX(sdX表示插入的U盘) 备份系统镜像linux提供的dd命令可以直接复制磁盘的数据 使用U盘启动UbuntuLiveCD，类似WinPE系统 sudo fdisk -l /dev/sdX(sdX代表你要克隆的磁盘)如图所示，linux系统有三个分区分别是sda8 sda9 sda10,其中sda8 是交换分区，不用备份，只需要备份sda9和sda10空间 1sudo dd bs=512 count=1953523712 skip=1646948352 if=/dev/sda of=savedPath/ghost.img 将ghost.img恢复到新硬盘 1dd if=savedPath/ghost.img of=/dev/sda 不要直接在计算机上用本地磁盘启动系统后执行dd命令生成本地磁盘的镜像。而应该用livecd启动计算机。因此计算机运行时会对系统盘产生大量写操作。 直接对运行中的系统盘生成的镜像，在恢复到其他硬盘上时，很可能会无法启动！ 使用gzip进行压缩和解压缩gzip参数：-c 表示输出到stdout-d 表示解压缩-1 表示最快压缩-9 表示最好压缩默认使用的是-6压缩级别 dd备份命令：1sudo dd bs=512 count=1953523712 skip=1646948352 if=/dev/sda | gzip -6 &gt; /ghost.img.gz dd还原命令：1gzip -dc /ghost.img.gz.gz | dd of=/dev/sda]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习笔记-生成deploy.prototxt文件]]></title>
    <url>%2F2017%2F02%2F28%2Fcaffe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%94%9F%E6%88%90deploy-prototxt%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[deploy.prototxt文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137name: "LeNet"/*原来训练与测试两层数据层*//*layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TRAIN &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_train_lmdb" batch_size: 64 backend: LMDB &#125;&#125;layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TEST &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_test_lmdb" batch_size: 100 backend: LMDB &#125;&#125;*//*被替换成如下*/layer &#123; name: "data" type: "Input" top: "data" input_param &#123; shape: &#123; dim: 1 dim: 1 dim: 28 dim: 28 &#125; &#125;&#125;/*卷积层与全连接层中的权值学习率，偏移值学习率，偏移值初始化方式,因为这些值在caffemodel文件中已经提供*/layer &#123; name: "conv1" type: "Convolution" bottom: "data" top: "conv1" convolution_param &#123; num_output: 20 kernel_size: 5 stride: 1 weight_filler &#123; type: "xavier" &#125; &#125;&#125;layer &#123; name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: "conv2" type: "Convolution" bottom: "pool1" top: "conv2" convolution_param &#123; num_output: 50 kernel_size: 5 stride: 1 weight_filler &#123; type: "xavier" &#125; &#125;&#125;layer &#123; name: "pool2" type: "Pooling" bottom: "conv2" top: "pool2" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layer &#123; name: "ip1" type: "InnerProduct" bottom: "pool2" top: "ip1" inner_product_param &#123; num_output: 500 weight_filler &#123; type: "xavier" &#125; &#125;&#125;layer &#123; name: "relu1" type: "ReLU" bottom: "ip1" top: "ip1"&#125;layer &#123; name: "ip2" type: "InnerProduct" bottom: "ip1" top: "ip2" inner_product_param &#123; num_output: 10 weight_filler &#123; type: "xavier" &#125; &#125;&#125;/*删除了原有的测试模块的测试精度层*//*输出层的类型由SoftmaxWithLoss变成Softmax，训练是输出时是loss，应用时是prob*/layer &#123; name: "prob" type: "Softmax" bottom: "ip2" top: "prob"&#125; 总结 去掉前面的训练数据和验证数据层，换成Input类型的层，其中input参数意思如下： 第一个：对待识别样本图片进行数据增广的数量，一个图片会变成10个，之后输入到网络进行识别。如果不进行数据增广，可以设置成1。第二个：图片的通道数，一般灰度图片为单通道，则值为1，如果为非灰度图3通道图片则为3。第三个：图片的高度，单位像素。第四个：图片的宽度，单位像素。 卷积层与全连接层中的权值学习率，偏移值学习率，偏移值初始化方式,因为这些值在caffemodel文件中已经提供 删除了原有的测试模块的测试精度层 输出层的类型由SoftmaxWithLoss变成Softmax，训练是输出时是loss，应用时是prob]]></content>
      <categories>
        <category>笔记</category>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm学习笔记--流分组]]></title>
    <url>%2F2017%2F02%2F28%2Fstorm%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B5%81%E5%88%86%E7%BB%84%2F</url>
    <content type="text"><![CDATA[storm 数据流分组 Shuffle grouping:随机分组，保证每个bolts得到相同数量的tuple Fields grouping：根据上一个bolts发出的tuple的字段来分组，同一字段的tuple被分到同一个Task里面 Partical Key grouping： 同Fields grouping,增加负载均衡策略 All grouping: 每个bolt的task都执行一遍上一级的bolt发射的tuples，慎重选择 Global grouping：全局分组，tuple被分配到bolt中的一个task，实现事务性的topology，一般来讲所有的tuple都被分配到拥有最小task_id的bolt任务处理 None grouping：不分组，不关注并行处理、负载均衡策略时采用该方式分组，目前等同于shuffle grouping，另外storm会把bolt任务和他的上游提供数据的任务安排在同一个线程下 Direct grouping：指定分組，只有被声明为DirectStream的消息流才能用这种分组方法，而且这种消息tuple必须使用emitDirect方法来发射tuple Local or shuffle grouping：如果目标bolt在一个work进程中一个或者多个task，那么tuples将会被随机分派到这个进程中的tasks，否则，如同shuffle grouping storm 分组策略及代码实现参考链接]]></content>
      <categories>
        <category>笔记</category>
        <category>storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习笔记--solver.prototxt配置文件]]></title>
    <url>%2F2017%2F02%2F28%2Fcaffe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-solver-prototxt%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[solver配置选项在Deep Learning中，往往loss function是非凸的，没有解析解，我们需要通过优化方法来求解。solver的主要作用就是交替调用前向（forward)算法和后向（backward)算法来更新参数，从而最小化loss，实际上就是一种迭代的优化算法。 到目前的版本，caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。 Stochastic Gradient Descent (type: “SGD”), AdaDelta (type: “AdaDelta”), Adaptive Gradient (type: “AdaGrad”), Adam (type: “Adam”), Nesterov’s Accelerated Gradient (type: “Nesterov”) and RMSprop (type: “RMSProp”) 参数含义net: “examples/AAA/train_val.prototxt” #训练或者测试配置文件test_iter: 40 #完成一次测试需要的迭代次数test_interval: 475 #测试间隔base_lr: 0.01 #基础学习率lr_policy: “step” #学习率变化规律gamma: 0.1 #学习率变化指数stepsize: 9500 #学习率变化频率display: 20 #屏幕显示间隔max_iter: 47500 #最大迭代次数momentum: 0.9 #动量weight_decay: 0.0005 #权重衰减snapshot: 5000 #保存模型间隔snapshot_prefix: “models/A1/caffenet_train” #保存模型的前缀solver_mode: GPU #是否使用GPU 训练样本总共:121368个batch_szie:256将所有样本处理完一次（称为一代，即epoch)需要：121368/256=475 次迭代才能完成所以这里将test_interval设置为475，即处理完一次所有的训练数据后，才去进行测试。所以这个数要大于等于475.如果想训练100代，则最大迭代次数为47500； 测试样本同理，如果有1000个测试样本，batch_size为25，那么需要40次才能完整的测试一次。 所以test_iter为40；这个数要大于等于40. 学习率学习率变化规律我们设置为随着迭代次数的增加，慢慢变低。总共迭代47500次，我们将变化5次，所以stepsize设置为47500/5=9500，即每迭代9500次，我们就降低一次学习率。]]></content>
      <categories>
        <category>笔记</category>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库和数据仓库的区别]]></title>
    <url>%2F2017%2F02%2F28%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[知乎问题：https://www.zhihu.com/question/20623931 数据库数据库是面向事务的设计，一般存储在线交易数据，尽量避免冗余，一般采用符合范式的规则来设计，主要是基本的、日常的事务处理，例如银行交易。例如MySQL，Oracle，MS SQL 数据仓库数据仓库是面向主题设计的。一般存储历史数据，采用反范式的规则来设计，有意引入冗余，为了分析数据而设计，反映历史变化，用于支持管理决策。例如Hive]]></content>
      <categories>
        <category>IT</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh免密码登录]]></title>
    <url>%2F2017%2F02%2F17%2Fssh%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[单机配置方式客户端（A）免密码登录服务器（B） A: ssh-keygen -t rsa A: cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys A: scp ~/.ssh/id_rsa.pub XXX@remotehost:/home/XXX/pubfile B: ssh-keygen -t rsa B: cat ~/pubfile &gt;&gt; ~/.ssh/authorized_keys A: ssh-add ~/.ssh/id_rsa A: chmod 700 ~/.ssh A: chmod 600 ~/.ssh/authorized_keys 集群配置方式12345#在cluster1上操作。并且假设共有3个集群ssh-keygen -t rsa #生成密钥对for ip in `seq 2 3`; do ssh-copy-id -i .ssh/id_rsa.pub uname@192.168.100.10$ip; done #这里集群的ip地址为192.168.100.101~103#同样在cluster2和cluster3上做同样的操作 安装ansibleansible是一个很方便的集群管理工作，打通了ssh之后，可以在集群上面操作同样的命令 1234567sudo apt-get install ansiblecd /etc/ansible/vi hosts #修改组名称ansible groupId -m shell -a 'shell commands;' #shell命令# 查看某组的目标机器上的当前时间ansible groupname -a 'date' 数据某组上所有目标机器的当前时间]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>教程</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装hadoop和hbase教程]]></title>
    <url>%2F2017%2F02%2F17%2Fubuntu%E5%AE%89%E8%A3%85hadoop%E5%92%8Chbase%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[ubuntu安装hadoop和hbase教程创建hadoop用户和hadoop用户组sudo addgroup hadoop sudo adduser -ingourp hadoop hadoop sudo gedit /etc/sudoers 最后显示结果如下： 安装ssh免密码登录见此文 配置java环境见此文 安装hadoop2 官网下载： http://mirror.bit.edu.cn/apache/hadoop/common/ 安装,修改文件拥有者为hadoop,修改权限为774 sudo chown -R hadoop:hadoop hadoop-install-path sudo chmod 774 hadoop-install-path 配置环境变量 123456789101112#HADOOP VARIABLES STARTexport JAVA_HOME=/usr/lib/jvm/java­7­openjdk­amd64 #注意修改路径export HADOOP_INSTALL=/usr/local/hadoopexport PATH=$PATH:$HADOOP_INSTALL/binexport PATH=$PATH:$HADOOP_INSTALL/sbinexport HADOOP_MAPRED_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_HOME=$HADOOP_INSTALLexport HADOOP_HDFS_HOME=$HADOOP_INSTALLexport YARN_HOME=$HADOOP_INSTALLexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/nativeexport HADOOP_OPTS="­Djava.library.path=$HADOOP_INSTALL/lib"#HADOOP VARIABLES END 单机配置（非分布式）执行如下操作：12345cd /usr/local/hadoop$ mkdir ./input$ cp ./etc/hadoop/*.xml ./input # 将配置文件作为输入文件$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop­mapreduce­examples­*.jar grep ./input ./output 'dfs[az.]+'$ cat ./output/* # 查看运行结果 注意hadoop不会覆盖结果文件，因此需要删除./output 伪分布式配置（单台机器模拟多个节点）需要修改两个配置文件：core-site.xml hdfs-site.xml 修改 core-sit.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改配置文件 hdfs-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 执行NameNode初始化 ./bin/hdfs namenode -format 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。 开启NameNode 和 DataNode守护进程 ./sbin/start-dfs.sh 通过$jps命令可以查看是否启动相关进程（NameNode，DataNode,SecondaryNameNode） 成功启动后在浏览器输入http://localhost:50070可以查看NameNode和DataNode信息 注意：启动失败可以尝试如下操作 12345# 针对 DataNode 没法启动的解决方法./sbin/stop-dfs.sh # 关闭rm -r ./tmp # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据./bin/hdfs namenode -format # 重新格式化 NameNode./sbin/start-dfs.sh # 重启 启动YARN 伪分布式不一定要启动YARN 启动Yarn来管理资源，负责任务调度 修改配置文件 mapred-site.xml mv ./etc/hadoop/mapred­site.xml.template ./etc/hadoop/mapred­site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改配置文件 yarn-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux­services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动yarn ./sbin/start­-yarn.sh # 启动YARN ./sbin/mr­jobhistory­daemon.sh start historyserver # 开启历史服务器，才能在Web中查看任务运行情况 开启jps，出现NodeManager和ResourceManager两个后台进程，通过web界面ResourceManager 可以查看任务运行情况 关闭yarn ./sbin/stop-yarn.sh ./sbin/mr­jobhistory­daemon.sh stop historyserver 注意:不启动 YARN 需重命名 mapred­site.xml如果不想启动 YARN，务必把配置文件 mapred­site.xml 重命名，改成 mapred­site.xml.template，需要用时改回来就行。否则在该配置文件存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，这也是为何该配置文件初始文件名为 mapred­site.xml.template。]]></content>
      <categories>
        <category>解决方案</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>ubuntu</tag>
        <tag>教程</tag>
        <tag>hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]解决 Ubuntu14.04 系统WiFi经常掉线问题]]></title>
    <url>%2F2017%2F01%2F17%2F%E5%8E%9F-%E8%A7%A3%E5%86%B3-Ubuntu14-04-%E7%B3%BB%E7%BB%9FWiFi%E7%BB%8F%E5%B8%B8%E6%8E%89%E7%BA%BF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[首先使用$lspci -vv &gt;&gt; filename 命令来查看本机的硬件信息，查看网卡部分可以看到网卡驱动型号。我安装的Ubuntu14,04.5系统内核是4.4.0-31-generic, 系统自带的无线网卡驱动是RealtekRTL8723BE. 由于内核版本和无线网卡驱动版本不兼容导致无线网卡过一段时间就会出现自动休眠省电，因此需要进行设置来保证无线网卡的正常使用。 解决方案： 找到/etc/modprobe.d/rtl8723be.conf (没有的话自己新建)， 写入下面的代码：1options rtl8723be ips=0 fwlps=0 swlps=0 swenc=1 保存后重启就ok 说明： ips, “using no link power save (default 1 is open）不使用链接省电 默认是1 默认是开启 就是启用省电。 fwlps, “using linked fw control power save (default 1 is open）链接FW控制省电 默认是1 就是打开省电设置 swenc, “using hardware crypto (default 0 [hardware]）硬件加密设置 默认是0]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>wifi</tag>
        <tag>系统bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]Ubuntu 14.04 64位系统安装cuda8.0+cudnn7.5+opencv+caffe 血泪教程]]></title>
    <url>%2F2017%2F01%2F14%2F%E5%8E%9F-Ubuntu-14-04-64%E4%BD%8D%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85cuda8-0-cudnn7-5-opencv-caffe-%E8%A1%80%E6%B3%AA%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Ubuntu 14.04 64位系统安装caffe 血泪教程1.安装环境 硬件配置： 笔记本：神舟战神 Z6-i78154S2 CPU: Intel Core i7-4720HQ 显卡: Intel 集成显卡 + GTX 960M 硬盘: SSD + HDD 软件配置： 操作系统：Win10 (SSD) + Ubuntu 14.04（内核版本：4.4.0-31-generic ，使用$ uname -r 可以查看） 2.安装Ubuntu 14.04 64位操作系统由于我电脑有两块硬盘，SSD已经安装了Win10，因此在HDD里面划分150G用于安装Ubuntu系统，启动文件还是在SSD中。1、需要准备的软件：ubuntu14.04系统镜像，EasyBCD软件，用于在windows上面安装ubuntu系统引导。2、进入windows操作系统，安装easyBCD软件，打开磁盘管理器，压缩卷（空间大小不少于8G，用于存放Ubuntu系统），删除卷，使之成为“未分配”状态（黑色）3、把Ubuntu系统镜像拷贝到C盘根目录下，把ubuntu系统镜像文件中casper目录下 initrd.lz 和 vmlinuz.efi 拷贝出来放到C盘根目录下。4、运行EasyBCD软件，“添加新条目”==》”NeoGrub”==》“安装”==》“配置”输入一下内容：1234title Install Ubunturoot (hd0,0)kernel (hd0,0)/vmlinuz.efi boot=casper iso-scan/filename=/ubuntu-14.04.5-desktop-amd64.iso ro quiet splash locale=zh_CN.UTF-8initrd (hd0,0)/initrd.lz menu.list格式一般有四行，kernel到……UTF-8需要放在一行，且每个参数需要用空格隔开，注意kernel后面的文件名要和下载的文件名一致。 title xxx, bootloader进去之后看到的菜单选项名称，必须保留 root (hd[n],[m])，root开头,然后一个空格,加一个分区名称(hd[n­],[m­])。表示 iso、vmlinuz.efi和 initrd.lz 的绝对路径，其中n表示iso文件所在的盘符序号，m表示第几个分区，n和m都是从0开始算起 kernel (hd[n],[m]), 以kernel 开头,然后加一个空格,并在其后给定vmlinuz.efi文件存放路径,这个命令行的作用是告诉计算机将使用(hd[n­1],[m­1])分区下的 linux 目录中的 kernel 内核来启动。ro表示只读。filename后面的iso务必与目标iso文件命名一致。 initrd (hd[n,hd[m])/initrd.lz ,he kernal 行类似，致命安装的文件放在哪个分区和哪个目录中 至此，windows上所有的准备工作完成5、重启选择NeoGrub引导加载器。进入ubuntu安装界面，桌面有一个“安装” 和“实例”，注意不要急着安装。6、Ctrl+ALt+t，打开终端，输入一下命令取消isodevice光驱分区，否则会有挂载错误。1sudo umount -l /isodevice/ 执行之后，点击桌面安装图标7、Ubuntu系统“安装类型”选择“其他选项”，我们进行手动分区 首先设置Swap挂载点：大小通常和自己的内存一样或者两倍，如果物理内存大，逻辑分区，也可以不用设置。 设置boot分区：不一定需要分出来，看你把引导挂在那个位置，逻辑分区，设置建议大小200～300MB，逻辑分区 设置“/”分区，剩下全部大小，逻辑分区 设置“安装系统引导的设备”：如果有/boot分区，则选择/boot分区，没有设置的话，可以放在其他位置。（提示：如果先设置了主分区，后面就默认全是主分区了，以后重装系统的话需要将基本磁盘转为动态磁盘才可以将硬盘划分为未分配状态，比较麻烦，建议全部设置为逻辑分区） 8、一路Next，安装成功，重启先进入windows系统，打开EasyBCD==”添加新条目“++》”Linux/BSD”==&gt;”GRUB 2”==&gt;”名称自己定义”==&gt;”驱动器和上一步设置的安装系统引导设备一致”==》“添加条目”。在编辑引导菜单里面可以修改引导顺序或者删除NeoGrub引导。重启进入Ubuntu 参考链接：http://blog.csdn.net/xlf13872135090/article/details/24093203 http://www.jb51.net/os/windows/298507.html 3.安装cuda8.0 1.更改系统默认源打开软件更新器=》Ubuntu软件==》下载自==》其他站点==》选择最佳服务器，我选择的是mirros.hust.edu.cn，这里说明一下，如果不更改系统默认源，后面安装cuda的依赖很多都找不到。2.准备安装文件：cuda8.0.44，从Nvida官网下载，下载链接 ，经过多次尝试，使用runfile (local)模式安装成功可能性要高些。下载cuDNN，下载链接 ，需要注册审核，可以从网上其他地方下载。3.安装开发依赖包，工具软件123sudo apt-get install build-essentialsudo apt-get install vim cmake gitsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libboost-all-dev libhdf5-serial-dev libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler 更新gcc到4.9版本，如果是gcc5.0版本，需要进行降级。12345678910sudo add-apt-repository ppa:ubuntu-toolchain-r/testsudo apt-get updatesudo apt-get install gcc-4.9sudo apt-get install g++-4.9更新软链接sudo sucd ../../usr/binln -s /usr/bin/g++-4.9 /usr/bin/g++ -fln -s /usr/bin/gcc-4.9 /usr/bin/gcc -f 其他注意事项参加cuda的官方文档, 遇到问题首先查看官方文档4.检查电脑环境是否具备安装cuda条件 检查电脑GPU是否是CUDA-capable 12终端输入 $ lspci | grep -i nvidia如果显示自己的NVIDIA GPU版本信息，则表示没问题，可以去CUDA官网查看自己的GPU版本是否在支持的列表中 检查Linux版本是否支持（Ubuntu14.04没问题) 检查自己的系统的内核是否支持,内核可以用$ uname -r 命令查看，官网给出的内核版本是最低内核版本要求 5.安装cuda8.0 (a)禁用nouveau123456789101112$ lsmod | grep nouveau如果有输出则表示nouveau正在加载因此我们需要禁用,方法如下：在/etc/modprobe.d目录下添加文件blacklist-nouveau.conf输入以下内容blacklist nouveauoptions nouveau modeset=0打开终端运行$ sudo update-initramfs -u重启后再次运行$ lsmod | grep nouveau如果没有输出，则禁用成功 (b) 重启电脑，到达登录界面时，进入text mode ,按Ctrl+Alt+F1，登录账户，关闭图形化界面1$ sudo service lightdm stop (c) cd到cuda安装文件路径，运行 $ sudo sh cuda_8.0.44_linux.run根据提示一步步操作，注意不要安装openGL（如果你的电脑和我一样是双显卡，且主显卡为非NVIDIA的GPU，选择no，否则可以yes），否则会覆盖系统集成先看的openGL,导致无法进入桌面注意：如果没有安装oenpGL，则在后面编译cuda自带的samples会报错，缺少can not find /usr/ld -lglut，这个是由于没有安装openGL造成的，因此需要手动安装 (d)安装成功后显示installed，如果安装失败，在/tmp目录下有cuda的安装日志，可以查看日志排错。 我第一次安装遇到了 ”缺少 libGLU.so libXmu.so libGL.so 依赖“的问题, 解决方法：sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-devsudo update-initramfs -usudo ldconfig //环境变量立即生效 (e)输入$ sudo service lightdm start 重启图形化界面，如果能够成功登录，cuda安装没问题了 (f) 重启电脑，检查 Device Node Verification检查路径“/dev”是否存在 nvidia*的多个文件，没有的话，参考官方文档的操作步骤，进行添加 (g) 设置环境变量12345678910111213$ sudo vim /etc/profile在文件末尾添加以下两行$ export PATH=/usr/local/cuda-8.0/bin:$PATH$ export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64使文件生效$ source /etc/profile动态链接库设置创建文件：sudo vim /etc/ld.so.conf.d/cuda.conf写入：/usr/local/cuda/lib64保存之后使其立即生效：sudo ldconfig -v (h)安装cuda完毕后的检查工作 检查NVIDIA 驱动是否安装成功: $ cat /etc/driver/nvidia/version, 安装成功则输出版本号信息 检查cuda Toolkit：$ nvcc -V 会输出CUDA的版本信息 尝试编译cuda提供的例子 cd ~/NVIDIA_CUDA-8.0_Samplesmake all -j4cd bin/x86_64/linux/release/./deviceQuery如果显示了设备信息，则cuda安装成功 由于我之前没有安装openGL，因此报告”/usr/bin/ld: 找不到 -lglut“错误解决方案如下:$ sudo apt-get install freeglut3-dev 至此，CUDA8.0的安装结束 4.安装cuDNN进入到cuDNN的文件目录，执行以下命令1234567891011tar -zxvf cudnn-7.5-linux-x64-v5.0-ga.tgzcd cudasudo cp lib64/* /usr/local/cuda/lib64/sudo cp include/cudnn.h /usr/local/cuda/include/更新软链接：cd /usr/local/cuda/lib64/sudo chmod +r libcudnn.so.5.0.5sudo ln -sf libcudnn.so.5.0.5 libcudnn.so.5sudo ln -sf libcudnn.so.5 libcudnn.sosudo ldconfig 5.安装Intel MKL或者AtlasMKL需要收费，这里安装Atlas1sudo apt-get install libatlas-base-dev 6.安装OpenCV（不是必须的）根据官网的安装步骤来进行安装：官网链接, 安装编译opencv的环境：sudo apt-get install build-essential make cmake git libgtk2.0-dev pkg-config python python-dev python-numpy libavcodec-dev libavformat-dev libswscale-dev libjpeg-dev libpng-dev libtiff-dev git下载opencv源码：12345678910cd ~git clone https://github.com/itseez/opencvcd opencvgit checkout 2.4.13.2 # 你需要安装哪个版本就切换到哪个分支上sudo mkdir buildcd buildsudo cmake -j4 -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..sudo make -j4sudo make -j4 installsudo ldconfig 7.安装caffe所需要的Python环境有两种方法： 方法一：去Anaconda官网下载安装包切换到文件所在目录，执行 12345678bash Anaconda-2.3.0-Linux-x86_64.sh添加Anaconda Library Path在 /etc/ld.so.conf 中新建anaconda.conf添加以下路径(实际为你的anaconda安装路径)/home/username/anaconda/libsudo vim /etc/profileexport LD_LIBRARY_PATH="/home/username/anaconda/lib:$LD_LIBRARY_PATH"source /etc/profile 方法二：直接手动安装Python的依赖包 1sudo apt-get install python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose python-dev python-sklearn python-skimage python-h5py python-protobuf python-leveldb python-networkx python-gflags cython ipython python-yaml 使用方法一或者方法二安装完caffe的Python依赖后，执行git命令，下载caffe12cd ~git clone https://github.com/BVLC/caffe.git 进入caffe目录下的python 文件夹，安装requirement里面的包，执行命令12sudo sufor req in $(cat requirements.txt); do pip install $req; done 安装完成后，退出root权限exit 8.编译caffe首先需要修改配置文件123cd ~/caffecp Makefile.config.example Makefile.configvim Makefile.config 需要修改两处：（1）使用cuDNN USE_CUDNN := 1 这里去掉#，取消注释为USE_CUDNN := 1（2）修改python包目录（如果使用方法二安装python依赖的需要修改，使用方法一不需要）PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/lib/python2.7/dist-packages/numpy/core/include改为PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/local/lib/python2.7/dist-packages/numpy/core/include因为新安装的python包目录在这里： /usr/local/lib/python2.7/dist-packages/ 接下来就到了激动人心的编译时刻了1234make all -j4make testmake runtestmake pycaffe 这个时候进入caffe下的python目录，试试python wrapper有没有安装好12$python$ import caffe 如果不报错，则安装成功]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>教程</tag>
        <tag>caffe</tag>
        <tag>cuda</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]分布式网站架构后续：zookeeper技术浅析]]></title>
    <url>%2F2016%2F12%2F05%2F%E8%BD%AC-%E5%88%86%E5%B8%83%E5%BC%8F%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E5%90%8E%E7%BB%AD-zookeeper%E6%8A%80%E6%9C%AF%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[转载自：http://www.cnblogs.com/sharpxiajun/archive/2013/06/02/3113923.html Zookeeper是hadoop的一个子项目，虽然源自hadoop，但是我发现zookeeper脱离hadoop的范畴开发分布式框架的运用越来越多。今天我想谈谈zookeeper，本文不谈如何使用zookeeper，而是zookeeper到底有哪些实际的运用，哪些类型的应用能发挥zookeeper的优势，最后谈谈zookeeper对分布式网站架构能产生怎样的作用。 Zookeeper是针对大型分布式系统的高可靠的协调系统。由这个定义我们知道zookeeper是个协调系统，作用的对象是分布式系统。为什么分布式系统需要一个协调系统了？理由如下： 开发分布式系统是件很困难的事情，其中的困难主要体现在分布式系统的“部分失败”。“部分失败”是指信息在网络的两个节点之间传送时候，如果网络出了故障，发送者无法知道接收者是否收到了这个信息，而且这种故障的原因很复杂，接收者可能在出现网络错误之前已经收到了信息，也可能没有收到，又或接收者的进程死掉了。发送者能够获得真实情况的唯一办法就是重新连接到接收者，询问接收者错误的原因，这就是分布式系统开发里的“部分失败”问题。 Zookeeper就是解决分布式系统“部分失败”的框架。Zookeeper不是让分布式系统避免“部分失败”问题，而是让分布式系统当碰到部分失败时候，可以正确的处理此类的问题，让分布式系统能正常的运行。 下面我要讲讲zookeeper的实际运用场景： 场景一：有一组服务器向客户端提供某种服务（例如：我前面做的分布式网站的服务端，就是由四台服务器组成的集群，向前端集群提供服务），我们希望客户端每次请求服务端都可以找到服务端集群中某一台服务器，这样服务端就可以向客户端提供客户端所需的服务。对于这种场景，我们的程序中一定有一份这组服务器的列表，每次客户端请求时候，都是从这份列表里读取这份服务器列表。那么这分列表显然不能存储在一台单节点的服务器上，否则这个节点挂掉了，整个集群都会发生故障，我们希望这份列表时高可用的。高可用的解决方案是：这份列表是分布式存储的，它是由存储这份列表的服务器共同管理的，如果存储列表里的某台服务器坏掉了，其他服务器马上可以替代坏掉的服务器，并且可以把坏掉的服务器从列表里删除掉，让故障服务器退出整个集群的运行，而这一切的操作又不会由故障的服务器来操作，而是集群里正常的服务器来完成。这是一种主动的分布式数据结构，能够在外部情况发生变化时候主动修改数据项状态的数据机构。Zookeeper框架提供了这种服务。这种服务名字就是：统一命名服务，它和javaEE里的JNDI服务很像。 场景二：分布式锁服务。当分布式系统操作数据，例如：读取数据、分析数据、最后修改数据。在分布式系统里这些操作可能会分散到集群里不同的节点上，那么这时候就存在数据操作过程中一致性的问题，如果不一致，我们将会得到一个错误的运算结果，在单一进程的程序里，一致性的问题很好解决，但是到了分布式系统就比较困难，因为分布式系统里不同服务器的运算都是在独立的进程里，运算的中间结果和过程还要通过网络进行传递，那么想做到数据操作一致性要困难的多。Zookeeper提供了一个锁服务解决了这样的问题，能让我们在做分布式数据运算时候，保证数据操作的一致性。 场景三：配置管理。在分布式系统里，我们会把一个服务应用分别部署到n台服务器上，这些服务器的配置文件是相同的（例如：我设计的分布式网站框架里，服务端就有4台服务器，4台服务器上的程序都是一样，配置文件都是一样），如果配置文件的配置选项发生变化，那么我们就得一个个去改这些配置文件，如果我们需要改的服务器比较少，这些操作还不是太麻烦，如果我们分布式的服务器特别多，比如某些大型互联网公司的hadoop集群有数千台服务器，那么更改配置选项就是一件麻烦而且危险的事情。这时候zookeeper就可以派上用场了，我们可以把zookeeper当成一个高可用的配置存储器，把这样的事情交给zookeeper进行管理，我们将集群的配置文件拷贝到zookeeper的文件系统的某个节点上，然后用zookeeper监控所有分布式系统里配置文件的状态，一旦发现有配置文件发生了变化，每台服务器都会收到zookeeper的通知，让每台服务器同步zookeeper里的配置文件，zookeeper服务也会保证同步操作原子性，确保每个服务器的配置文件都能被正确的更新。 场景四：为分布式系统提供故障修复的功能。集群管理是很困难的，在分布式系统里加入了zookeeper服务，能让我们很容易的对集群进行管理。集群管理最麻烦的事情就是节点故障管理，zookeeper可以让集群选出一个健康的节点作为master，master节点会知道当前集群的每台服务器的运行状况，一旦某个节点发生故障，master会把这个情况通知给集群其他服务器，从而重新分配不同节点的计算任务。Zookeeper不仅可以发现故障，也会对有故障的服务器进行甄别，看故障服务器是什么样的故障，如果该故障可以修复，zookeeper可以自动修复或者告诉系统管理员错误的原因让管理员迅速定位问题，修复节点的故障。大家也许还会有个疑问，master故障了，那怎么办了？zookeeper也考虑到了这点，zookeeper内部有一个“选举领导者的算法”，master可以动态选择，当master故障时候，zookeeper能马上选出新的master对集群进行管理。 下面我要讲讲zookeeper的特点： zookeeper是一个精简的文件系统。这点它和hadoop有点像，但是zookeeper这个文件系统是管理小文件的，而hadoop是管理超大文件的。zookeeper提供了丰富的“构件”，这些构件可以实现很多协调数据结构和协议的操作。例如：分布式队列、分布式锁以及一组同级节点的“领导者选举”算法。zookeeper是高可用的，它本身的稳定性是相当之好，分布式集群完全可以依赖zookeeper集群的管理，利用zookeeper避免分布式系统的单点故障的问题。zookeeper采用了松耦合的交互模式。这点在zookeeper提供分布式锁上表现最为明显，zookeeper可以被用作一个约会机制，让参入的进程不在了解其他进程的（或网络）的情况下能够彼此发现并进行交互，参入的各方甚至不必同时存在，只要在zookeeper留下一条消息，在该进程结束后，另外一个进程还可以读取这条信息，从而解耦了各个节点之间的关系。zookeeper为集群提供了一个共享存储库，集群可以从这里集中读写共享的信息，避免了每个节点的共享操作编程，减轻了分布式系统的开发难度。zookeeper的设计采用的是观察者的设计模式，zookeeper主要是负责存储和管理大家关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式。 由此可见zookeeper很利于分布式系统开发，它能让分布式系统更加健壮和高效。 前不久我参加了部门的hadoop兴趣小组，测试环境的hadoop、mapreduce、hive及hbase都是我来安装的，安装hbase时候安装要预先安装zookeeper，最早我是在四台服务器上都安装了zookeeper，但是同事说安装四台和安装三台是一回事，这是因为zookeeper要求半数以上的机器可用，zookeeper才能提供服务，所以3台的半数以上就是2台了，4台的半数以上也是两台，因此装了三台服务器完全可以达到4台服务器的效果，这个问题说明zookeeper进行安装的时候通常选择奇数台服务器。在学习hadoop的过程中，我感觉zookeeper是最难理解的一个子项目，原因倒不是它技术负责，而是它的应用方向很让我困惑，所以我有关hadoop技术第一篇文章就从zookeeper开始，也不讲具体技术实现，而从zookeeper的应用场景讲起，理解了zookeeper应用的领域，我想再学习zookeeper就会更加事半功倍。 之所以今天要谈谈zookeeper，也是为我上一篇文章分布式网站框架的补充。虽然我设计网站架构是分布式结构，也做了简单的故障处理机制，比如：心跳机制，但是对集群的单点故障还是没有办法的，如果某一台服务器坏掉了，客户端任然会尝试连接这个服务器，导致部分请求的阻塞，也会导致服务器资源的浪费。不过我目前也不想去修改自己的框架，因为我总觉得在现有的服务上添加zookeeper服务会影响网站的效率，如果有独立的服务器集群部署zookeeper还是值得考虑的，但是服务器资源太宝贵了，这个可能性不大。幸好我们部门也发现了这样的问题，我们部门将开发一个强大的远程调用框架，将集群管理和通讯管理这块剥离出来，集中式提供高效可用的服务，等部门的远程框架开发完毕，我们的网站加入新的服务，我想我们的网站将会更加稳定和高效。]]></content>
      <categories>
        <category>综述</category>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>zookeeper</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]Linux下的Kafka配置步骤]]></title>
    <url>%2F2016%2F12%2F05%2F%E8%BD%AC-Linux%E4%B8%8B%E7%9A%84Kafka%E9%85%8D%E7%BD%AE%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[转载自：http://blog.csdn.net/suifeng3051/article/details/38321043 Kafka集群配置比较简单，为了更好的让大家理解，在这里要分别介绍下面三种配置 单节点：一个broker的集群 单节点：多个broker的集群 多节点：多broker集群 一、单节点单broker实例的配置 1. 首先启动zookeeper服务**必须使用root用户启动kafka** Kafka本身提供了启动zookeeper的脚本（在kafka/bin/目录下）和zookeeper配置文件（在kafka/config/目录下），首先进入Kafka的主目录（可通过 whereis kafka命令查找到）： 1234567[root@localhost kafka-0.8]# bin/zookeeper-server-start.sh config/zookeeper.propertieszookeeper配置文件的一些重要属性:# Data directory where the zookeeper snapshot is stored.dataDir=/tmp/zookeeper# The port listening for client requestclientPort=2181默认情况下，zookeeper服务器会监听 2181端口，更详细的信息可去zookeeper官网查阅。 ## 2. 启动Kafka broker 运行kafka提供的启动kafka服务脚本即可： 12345678[root@localhost kafka-0.8]# bin/kafka-server-start.sh config/server.propertiesbroker配置文件中的重要属性：# broker的id. 每个broker的id必须是唯一的.Broker.id=0# 存放log的目录log.dir=/tmp/kafka8-logs# Zookeeper 连接串zookeeper.connect=localhost:2181 ## 3. 创建一个仅有一个Partition的topic [root@localhost kafka-0.8]# bin/kafka-create-topic.sh --zookeeper localhost:2181 --replica 1 --partition 1 --topic kafkatopic ## 4. 用Kafka提供的生产者客户端启动一个生产者进程来发送消息 [root@localhost kafka-0.8]# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kafkatopic 其中有两个参数需要注意： - broker-list:定义了生产者要推送消息的broker地址，以形式 - topic：生产者发送给哪个topic ## 5. 启动一个Consumer实例来消费消息 [root@localhost kafka-0.8]# bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafkatopic --from-beginning 当你执行这个命令之后，你便可以看到控制台上打印出的生产者生产的消息： ![这里写图片描述](http://img.blog.csdn.net/20140731173902203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3VpZmVuZzMwNTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 和消费者相关的属性配置存放在Consumer.properties文件中，重要的属性有： # consumer的group id (A string that uniquely identifies a set of consumers # within the same consumer group) groupid=test-consumer-group # zookeeper 连接串 zookeeper.connect=localhost:2181 # 二、单节点运行多broker实例 前面的步骤和单节点运行但broker实例一样 ## 1、启动zookeeper ## 2、启动Kafka的broker 要想在一台机器上启动多个broker实例，只需要准备多个server.properties文件即可，比如我们要在一台机器上启动两个broker： 首先我们要准备两个server.properties配置文件 12345678server-1brokerid=1port=9092log.dir=/temp/kafka8-logs/broker1 server-2brokerid=2port=9093log.dir=/temp/kafka8-logs/broker2 然后我们再用这两个配置文件分别启动一个broker 12[root@localhost kafka-0.8]# env JMX_PORT=9999 bin/kafka-server-start.sh config/server-1.properties[root@localhost kafka-0.8]# env JMX_PORT=10000 bin/kafka-server-start.sh config/server-2.properties 可以看到我们启动是为每个broker都指定了不同的JMX Port，JMX Port主要用来利用jconsole等工具进行监控和排错 3.创建一个topic现在我们要创建一个含有两个Partition分区和2个备份的broker： [root@localhost kafka-0.8]# bin/kafka-create-topic.sh --zookeeper localhost:2181 --replica 2 --partition 2 --topic othertopic 4.启动Producer发送消息如果我们要用一个Producer发送给多个broker，唯一需要改变的就是在broker-list属性中指定要连接的broker： [root@localhost kafka-0.8]# bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093 --topic othertopic 如果我们要让不同的Producer发送给不同的broker，我们也仅仅需要为每个Producer配置响应的broker-list属性即可。 5.启动一个消费者来消费消息和之前的命令一样 [root@localhost kafka-0.8]# bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic othertopic --from-beginning 三、集群模式（多节点多实例）介绍了上面两种配置方法，再理解集群配置就简单了，比如我们要配置如下图所示集群：zookeeper配置文件（zookeeper.properties）：不变broker的配置配置文件(server.properties)：按照单节点多实例配置方法在一个节点上启动两个实例，不同的地方是zookeeper的连接串需要把所有节点的zookeeper都连接起来 Zookeeper 连接串 zookeeper.connect=node1:2181,node2:2181]]></content>
      <categories>
        <category>解决方案</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>kafka</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]Markdown使用技巧总结——字体，颜色，字号，背景，首行缩进等]]></title>
    <url>%2F2016%2F12%2F05%2F%E8%BD%AC-Markdown%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E5%AD%97%E4%BD%93%EF%BC%8C%E9%A2%9C%E8%89%B2%EF%BC%8C%E5%AD%97%E5%8F%B7%EF%BC%8C%E8%83%8C%E6%99%AF%EF%BC%8C%E9%A6%96%E8%A1%8C%E7%BC%A9%E8%BF%9B%E7%AD%89%2F</url>
    <content type="text"><![CDATA[转载自：http://blog.csdn.net/u010177286/article/details/50358720 Markdown 常用技巧：换行: 方法1: 连续两个以上空格+回车方法2：使用html语言换行标签：首行缩进两个字符：(每个表示一个空格，连续使用两个即可） &ensp; 半角的空格&emsp; 全角的空格字体、字号与颜色: Markdown是一种可以使用普通文本编辑器编写的标记语言，通过类似HTML的标记语法，它可以使普通文本内容具有一定的格式。但是它本身是不支持修改字体、字号与颜色等功能的！ CSDN-markdown编辑器是其衍生版本，扩展了Markdown的功能（如表格、脚注、内嵌HTML等等）！对，就是内嵌HTML，接下来要讲的功能就需要使用内嵌HTML的方法来实现。 字体，字号和颜色编辑如下代码 我是黑体字 我是微软雅黑 我是华文彩云 color=#0099ff size=72 face="黑体" color=#00ffff color=gray Size：规定文本的尺寸大小。可能的值：从 1 到 7 的数字。浏览器默认值是 3具体颜色分类及标记请参照：各种颜色 背景色:Markdown本身不支持背景色设置，需要采用内置html的方式实现：借助 table, tr, td 等表格标签的 bgcolor 属性来实现背景色的功能。举例如下： 背景色是：orange效果如下：背景色是：orange2015/12/19 16:04:07分割线：你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线 链接：链接文字都是用 [方括号] 来标记,在方块括号后面紧接着圆括号并插入网址链接即可:可参照This link has no title attribute. 代码块： 代码块：用2个以上TAB键起始的段落，会被认为是代码块（效果如下）： struct { int year; int month; int day; }bdate; 如果在一个行内需要引用代码，只要用反引号`引起来就好(Esc健） 代码块与语法高亮：在需要高亮的代码块的前一行及后一行使用三个反引号“`”，同时第一行反引号后面表面代码块所使用的语言插入互联网上图片： 使用LaTex数学公式:行内公式：使用两个”$”符号引用公式: $公式$行间公式：使用两对“”符号引用公式：公式$$]]></content>
      <categories>
        <category>IT</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]Linux常用命令大全]]></title>
    <url>%2F2016%2F12%2F05%2F%E8%BD%AC-Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[转载自：http://www.php100.com/html/webkaifa/Linux/2009/1106/3485.html 1、系统信息arch 显示机器的处理器架构(1)uname -m 显示机器的处理器架构(2)uname -r 显示正在使用的内核版本dmidecode -q 显示硬件系统部件 - (SMBIOS / DMI)hdparm -i /dev/hda 罗列一个磁盘的架构特性hdparm -tT /dev/sda 在磁盘上执行测试性读取操作cat /proc/cpuinfo 显示CPU info的信息cat /proc/interrupts 显示中断cat /proc/meminfo 校验内存使用cat /proc/swaps 显示哪些swap被使用cat /proc/version 显示内核的版本cat /proc/net/dev 显示网络适配器及统计cat /proc/mounts 显示已加载的文件系统lspci -tv 罗列 PCI 设备lsusb -tv 显示 USB 设备date 显示系统日期cal 2007 显示2007年的日历表date 041217002007.00 设置日期和时间 - 月日时分年.秒clock -w 将时间修改保存到 BIOS 关机 (系统的关机、重启以及登出 )shutdown -h now 关闭系统(1)init 0 关闭系统(2)telinit 0 关闭系统(3)shutdown -h hours:minutes &amp; 按预定时间关闭系统shutdown -c 取消按预定时间关闭系统shutdown -r now 重启(1)reboot 重启(2)logout 注销 2、文件和目录cd /home 进入 ‘/ home’ 目录’cd .. 返回上一级目录cd ../.. 返回上两级目录cd 进入个人的主目录cd ~user1 进入个人的主目录cd - 返回上次所在的目录pwd 显示工作路径ls 查看目录中的文件ls -F 查看目录中的文件ls -l 显示文件和目录的详细资料ls -a 显示隐藏文件ls [0-9] 显示包含数字的文件名和目录名tree 显示文件和目录由根目录开始的树形结构(1)lstree 显示文件和目录由根目录开始的树形结构(2)mkdir dir1 创建一个叫做 ‘dir1’ 的目录’mkdir dir1 dir2 同时创建两个目录mkdir -p /tmp/dir1/dir2 创建一个目录树rm -f file1 删除一个叫做 ‘file1’ 的文件’rmdir dir1 删除一个叫做 ‘dir1’ 的目录’rm -rf dir1 删除一个叫做 ‘dir1’ 的目录并同时删除其内容rm -rf dir1 dir2 同时删除两个目录及它们的内容mv dir1 new_dir 重命名/移动 一个目录cp file1 file2 复制一个文件cp dir/ . 复制一个目录下的所有文件到当前工作目录cp -a /tmp/dir1 . 复制一个目录到当前工作目录cp -a dir1 dir2 复制一个目录ln -s file1 lnk1 创建一个指向文件或目录的软链接ln file1 lnk1 创建一个指向文件或目录的物理链接touch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm)file file1 outputs the mime type of the file as texticonv -l 列出已知的编码iconv -f fromEncoding -t toEncoding inputFile &gt; outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding.find . -maxdepth 1 -name .jpg -print -exec convert “{}” -resize 80x60 “thumbs/{}” \; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick) 3、文件搜索find / -name file1 从 ‘/‘ 开始进入根文件系统搜索文件和目录find / -user user1 搜索属于用户 ‘user1’ 的文件和目录find /home/user1 -name *.bin 在目录 ‘/ home/user1’ 中搜索带有’.bin’ 结尾的文件find /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件find /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件find / -name *.rpm -exec chmod 755 ‘{}’ \; 搜索以 ‘.rpm’ 结尾的文件并定义其权限find / -xdev -name *.rpm 搜索以 ‘.rpm’ 结尾的文件，忽略光驱、捷盘等可移动设备locate *.ps 寻找以 ‘.ps’ 结尾的文件 - 先运行 ‘updatedb’ 命令whereis halt 显示一个二进制文件、源码或man的位置which halt 显示一个二进制文件或可执行文件的完整路径 4、挂载一个文件系统mount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 ‘/ mnt/hda2’ 已经存在umount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 ‘/ mnt/hda2’ 退出fuser -km /mnt/hda2 当设备繁忙时强制卸载umount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用mount /dev/fd0 /mnt/floppy 挂载一个软盘mount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrommount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrommount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrommount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件mount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统mount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享 5、磁盘空间df -h 显示已经挂载的分区列表ls -lSr |more 以尺寸大小排列文件和目录du -sh dir1 估算目录 ‘dir1’ 已经使用的磁盘空间’du -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小rpm -q -a —qf ‘%10{SIZE}t%{NAME}n’ | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统)dpkg-query -W -f=’${Installed-Size;10}t${Package}n’ | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) 6、用户和群组groupadd group_name 创建一个新用户组groupdel group_name 删除一个用户组groupmod -n new_group_name old_group_name 重命名一个用户组useradd -c “Name Surname “ -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 “admin” 用户组的用户useradd user1 创建一个新用户userdel -r user1 删除一个用户 ( ‘-r’ 排除主目录)usermod -c “User FTP” -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性passwd 修改口令passwd user1 修改一个用户的口令 (只允许root执行)chage -E 2005-12-31 user1 设置用户口令的失效期限pwck 检查 ‘/etc/passwd’ 的文件格式和语法修正以及存在的用户grpck 检查 ‘/etc/passwd’ 的文件格式和语法修正以及存在的群组newgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组 7、文件的权限 - 使用 “+” 设置权限，使用 “-“ 用于取消ls -lh 显示权限ls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示chmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限chmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限chown user1 file1 改变一个文件的所有人属性chown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性chgrp group1 file1 改变文件的群组chown user1:group1 file1 改变一个文件的所有人和群组属性find / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件chmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限chmod u-s /bin/file1 禁用一个二进制文件的 SUID位chmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的chmod g-s /home/public 禁用一个目录的 SGID 位chmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件chmod o-t /home/public 禁用一个目录的 STIKY 位 8、文件的特殊属性 -使用 “+” 设置权限，使用 “-“ 用于取消chattr +a file1 只允许以追加方式读写文件chattr +c file1 允许这个文件能被内核自动压缩/解压chattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件chattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接chattr +s file1 允许一个文件被安全地删除chattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘chattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件lsattr 显示特殊的属性 9、打包和压缩文件bunzip2 file1.bz2 解压一个叫做 ‘file1.bz2’的文件bzip2 file1 压缩一个叫做 ‘file1’ 的文件gunzip file1.gz 解压一个叫做 ‘file1.gz’的文件gzip file1 压缩一个叫做 ‘file1’的文件gzip -9 file1 最大程度压缩rar a file1.rar test_file 创建一个叫做 ‘file1.rar’ 的包rar a file1.rar file1 file2 dir1 同时压缩 ‘file1’, ‘file2’ 以及目录 ‘dir1’rar x file1.rar 解压rar包unrar x file1.rar 解压rar包tar -cvf archive.tar file1 创建一个非压缩的 tarballtar -cvf archive.tar file1 file2 dir1 创建一个包含了 ‘file1’, ‘file2’ 以及 ‘dir1’的档案文件tar -tf archive.tar 显示一个包中的内容tar -xvf archive.tar 释放一个包tar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下tar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包tar -xvfj archive.tar.bz2 解压一个bzip2格式的压缩包tar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包tar -xvfz archive.tar.gz 解压一个gzip格式的压缩包zip file1.zip file1 创建一个zip格式的压缩包zip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包unzip file1.zip 解压一个zip格式压缩包 10、Ubuntu系统命令DEB 包 (Debian, Ubuntu 以及类似系统)dpkg -i package.deb 安装/更新一个 deb 包dpkg -r package_name 从系统删除一个 deb 包dpkg -l 显示系统中所有已经安装的 deb 包dpkg -l | grep httpd 显示所有名称中包含 “httpd” 字样的deb包dpkg -s package_name 获得已经安装在系统中一个特殊包的信息dpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表dpkg —contents package.deb 显示尚未安装的一个包所提供的文件列表dpkg -S /bin/ping 确认所给的文件由哪个deb包提供 APT 软件工具 (Debian, Ubuntu 以及类似系统)apt-get install package_name 安装/更新一个 deb 包apt-cdrom install package_name 从光盘安装/更新一个 deb 包apt-get update 升级列表中的软件包apt-get upgrade 升级所有已安装的软件apt-get remove package_name 从系统删除一个deb包apt-get check 确认依赖的软件仓库正确apt-get clean 从下载的软件包中清理缓存apt-cache search searched-package 返回包含所要搜索字符串的软件包名称 11、查看文件内容cat file1 从第一个字节开始正向查看文件的内容tac file1 从最后一行开始反向查看一个文件的内容more file1 查看一个长文件的内容less file1 类似于 ‘more’ 命令，但是它允许在文件中和正向操作一样的反向操作head -2 file1 查看一个文件的前两行tail -2 file1 查看一个文件的最后两行tail -f /var/log/messages 实时查看被添加到一个文件中的内容 12、文本处理12345678910111213141516171819202122232425262728293031cat file1 file2 ... | command &lt;&gt; file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUTcat file1 | command( sed, grep, awk, grep, etc...) &gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中cat file1 | command( sed, grep, awk, grep, etc...) &gt;&gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中grep Aug /var/log/messages 在文件 &apos;/var/log/messages&apos;中查找关键词&quot;Aug&quot;grep ^Aug /var/log/messages 在文件 &apos;/var/log/messages&apos;中查找以&quot;Aug&quot;开始的词汇grep [0-9] /var/log/messages 选择 &apos;/var/log/messages&apos; 文件中所有包含数字的行grep Aug -R /var/log/* 在目录 &apos;/var/log&apos; 及随后的目录中搜索字符串&quot;Aug&quot;sed &apos;s/stringa1/stringa2/g&apos; example.txt 将example.txt文件中的 &quot;string1&quot; 替换成 &quot;string2&quot;sed &apos;/^$/d&apos; example.txt 从example.txt文件中删除所有空白行sed &apos;/ *#/d; /^$/d&apos; example.txt 从example.txt文件中删除所有注释和空白行echo &apos;esempio&apos; | tr &apos;[:lower:]&apos; &apos;[:upper:]&apos; 合并上下单元格内容sed -e &apos;1d&apos; result.txt 从文件example.txt 中排除第一行sed -n &apos;/stringa1/p&apos; 查看只包含词汇 &quot;string1&quot;的行sed -e &apos;s/ *$//&apos; example.txt 删除每一行最后的空白字符sed -e &apos;s/stringa1//g&apos; example.txt 从文档中只删除词汇 &quot;string1&quot; 并保留剩余全部sed -n &apos;1,5p;5q&apos; example.txt 查看从第一行到第5行内容sed -n &apos;5p;5q&apos; example.txt 查看第5行sed -e &apos;s/00*/0/g&apos; example.txt 用单个零替换多个零cat -n file1 标示文件的行数cat example.txt | awk &apos;NR%2==1&apos; 删除example.txt文件中的所有偶数行echo a b c | awk &apos;&#123;print $1&#125;&apos; 查看一行第一栏echo a b c | awk &apos;&#123;print $1,$3&#125;&apos; 查看一行的第一和第三栏paste file1 file2 合并两个文件或两栏的内容paste -d &apos;+&apos; file1 file2 合并两个文件或两栏的内容，中间用&quot;+&quot;区分sort file1 file2 排序两个文件的内容sort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份)sort file1 file2 | uniq -u 删除交集，留下其他的行sort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件)comm -1 file1 file2 比较两个文件的内容只删除 &apos;file1&apos; 所包含的内容comm -2 file1 file2 比较两个文件的内容只删除 &apos;file2&apos; 所包含的内容comm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 13、字符设置和文件格式转换dos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIXunix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOSrecode ..HTML &lt; page.txt &gt; page.html 将一个文本文件转换成htmlrecode -l | more 显示所有允许的转换格式 14、文本处理12345678910111213141516171819202122232425262728293031cat file1 file2 ... | command &lt;&gt; file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUTcat file1 | command( sed, grep, awk, grep, etc...) &gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中cat file1 | command( sed, grep, awk, grep, etc...) &gt;&gt; result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中grep Aug /var/log/messages 在文件 &apos;/var/log/messages&apos;中查找关键词&quot;Aug&quot;grep ^Aug /var/log/messages 在文件 &apos;/var/log/messages&apos;中查找以&quot;Aug&quot;开始的词汇grep [0-9] /var/log/messages 选择 &apos;/var/log/messages&apos; 文件中所有包含数字的行grep Aug -R /var/log/* 在目录 &apos;/var/log&apos; 及随后的目录中搜索字符串&quot;Aug&quot;sed &apos;s/stringa1/stringa2/g&apos; example.txt 将example.txt文件中的 &quot;string1&quot; 替换成 &quot;string2&quot;sed &apos;/^$/d&apos; example.txt 从example.txt文件中删除所有空白行sed &apos;/ *#/d; /^$/d&apos; example.txt 从example.txt文件中删除所有注释和空白行echo &apos;esempio&apos; | tr &apos;[:lower:]&apos; &apos;[:upper:]&apos; 合并上下单元格内容sed -e &apos;1d&apos; result.txt 从文件example.txt 中排除第一行sed -n &apos;/stringa1/p&apos; 查看只包含词汇 &quot;string1&quot;的行sed -e &apos;s/ *$//&apos; example.txt 删除每一行最后的空白字符sed -e &apos;s/stringa1//g&apos; example.txt 从文档中只删除词汇 &quot;string1&quot; 并保留剩余全部sed -n &apos;1,5p;5q&apos; example.txt 查看从第一行到第5行内容sed -n &apos;5p;5q&apos; example.txt 查看第5行sed -e &apos;s/00*/0/g&apos; example.txt 用单个零替换多个零cat -n file1 标示文件的行数cat example.txt | awk &apos;NR%2==1&apos; 删除example.txt文件中的所有偶数行echo a b c | awk &apos;&#123;print $1&#125;&apos; 查看一行第一栏echo a b c | awk &apos;&#123;print $1,$3&#125;&apos; 查看一行的第一和第三栏paste file1 file2 合并两个文件或两栏的内容paste -d &apos;+&apos; file1 file2 合并两个文件或两栏的内容，中间用&quot;+&quot;区分sort file1 file2 排序两个文件的内容sort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份)sort file1 file2 | uniq -u 删除交集，留下其他的行sort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件)comm -1 file1 file2 比较两个文件的内容只删除 &apos;file1&apos; 所包含的内容comm -2 file1 file2 比较两个文件的内容只删除 &apos;file2&apos; 所包含的内容comm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 15、文件系统分析badblocks -v /dev/hda1 检查磁盘hda1上的坏磁块fsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性fsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性e2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性e2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性fsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性fsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性fsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性dosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 16、初始化一个文件系统mkfs /dev/hda1 在hda1分区创建一个文件系统mke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统mke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统mkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统fdformat -n /dev/fd0 格式化一个软盘mkswap /dev/hda3 创建一个swap文件系统 17、SWAP文件系统mkswap /dev/hda3 创建一个swap文件系统swapon /dev/hda3 启用一个新的swap文件系统swapon /dev/hda2 /dev/hdb3 启用两个swap分区 18、备份dump -0aj -f /tmp/home0.bak /home 制作一个 ‘/home’ 目录的完整备份dump -1aj -f /tmp/home0.bak /home 制作一个 ‘/home’ 目录的交互式备份restore -if /tmp/home0.bak 还原一个交互式备份rsync -rogpav —delete /home /tmp 同步两边的目录rsync -rogpav -e ssh —delete /home ip_address:/tmp 通过SSH通道rsyncrsync -az -e ssh —delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录rsync -az -e ssh —delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr ‘dd of=hda.gz’ 通过ssh在远程主机上执行一次备份本地磁盘的操作dd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件tar -Puf backup.tar /home/user 执行一次对 ‘/home/user’ 目录的交互式备份操作( cd /tmp/local/ &amp;&amp; tar c . ) | ssh -C user@ip_addr ‘cd /home/share/ &amp;&amp; tar x -p’ 通过ssh在远程目录中复制一个目录内容( tar c /home ) | ssh -C user@ip_addr ‘cd /home/backup-home &amp;&amp; tar x -p’ 通过ssh在远程目录中复制一个本地目录tar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接find /home/user1 -name ‘.txt’ | xargs cp -av —target-directory=/home/backup/ —parents 从一个目录查找并复制所有以 ‘.txt’ 结尾的文件到另一个目录find /var/log -name ‘.log’ | tar cv —files-from=- | bzip2 &gt; log.tar.bz2 查找所有以 ‘.log’ 结尾的文件并做成一个bzip包dd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作dd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容 19、网络 - （以太网和WIFI无线）ifconfig eth0 显示一个以太网卡的配置ifup eth0 启用一个 ‘eth0’ 网络设备ifdown eth0 禁用一个 ‘eth0’ 网络设备ifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址ifconfig eth0 promisc 设置 ‘eth0’ 成混杂模式以嗅探数据包 (sniffing)dhclient eth0 以dhcp模式启用 ‘eth0’route -n show routing tableroute add -net 0/0 gw IP_Gateway configura default gatewayroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network ‘192.168.0.0/16’route del 0/0 gw IP_gateway remove static routeecho “1” &gt; /proc/sys/net/ipv4/ip_forward activate ip routinghostname show hostname of systemhost www.example.com lookup hostname to resolve name to ip address and viceversa(1)nslookup www.example.com lookup hostname to resolve name to ip address and viceversa(2)ip link show show link status of all interfacesmii-tool eth0 show link status of ‘eth0’ethtool eth0 show statistics of network card ‘eth0’netstat -tup show all active network connections and their PIDnetstat -tupl show all network services listening on the system and their PIDtcpdump tcp port 80 show all HTTP trafficiwlist scan show wireless networksiwconfig eth1 show configuration of a wireless network cardhostname show hostnamehost www.example.com lookup hostname to resolve name to ip address and viceversanslookup www.example.com lookup hostname to resolve name to ip address and viceversawhois www.example.com lookup on Whois database]]></content>
      <categories>
        <category>IT</category>
        <category>linux</category>
      </categories>
      <tags>
        <tag>命令</tag>
        <tag>笔记</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]Ubuntu下配置java+Eclipse开发环境]]></title>
    <url>%2F2016%2F12%2F05%2F%E5%8E%9F-Ubuntu%E4%B8%8B%E9%85%8D%E7%BD%AEjava-Eclipse%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[1、复制文件到待安装目录cp /media/ubuntu/娱乐/jdk-8u92-linux-x64.tar.gz /opt/software 2、提升文件的操作权限（必须做，否则无法配置成功）chmod +x /opt/software/jdk-8u92-linux-x64.tar.gz 3、解压文件tar zxvf /opt/software/jdk-8u92-linux-x64.tar.gz -C /opt/software/ 4、添加环境变量当前用户的path（/home/username/.bashrc 和 .bash_profile）全局的path（/etc/bashrc 和/etc/profile） 立即生效的方法 source 配置文件1234export JAVA_HOME=/opt/software/jdk1.8.0_92export JRE_HOME=/opt/software/jdk1.8.0_92/jreexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 修改默认的JDK（注意替换路径，如果能直接显示sun的jdk的话就不用操作）sudo update-alternatives —install “/usr/bin/java” “java” “/usr/java/jdk_8u60/bin/java” 300sudo update-alternatives —install “/usr/bin/javac” “javac” “/usr/java/jdk_8u60/bin/javac” 300sudo update-alternatives —install “/usr/bin/javaws” “javaws” “/usr/java/jdk_8u60/bin/javaws” 300 重要参考链接：http://www.linuxidc.com/Linux/2015-09/122689.htm 5、解压eclipse6、创建快捷方式sudo gedit /usr/share/applications/eclipse.desktop然后写入以下语句（绝对不能有空格，否则很苦逼的说）[Desktop Entry] Encoding=UTF-8 Name=eclipse Comment=Eclipse IDE Exec=/usr/local/eclipse/eclipse_SDK/eclipse Icon=/usr/local/eclipse/eclipse_SDK/icon.xpm Terminal=false StartupNotify=true Type=Application Categories=Application;Development; Exec=/usr/local/eclipse/eclipse_SDK/eclipseIcon=/usr/local/eclipse/eclipse_SDK/icon.xpm 这个地方要修改为你的eclipse安装目录。 7、手动把eclipse.desktop拷贝到桌面上（注意系统如果是中文，则/home/username/桌面/为桌面目录）sudo cp /usr/share/applications/eclipse.desktop /home/ubuntu/桌面 8、修改图标文件的访问权限，普通用户没有root权限sudo chmod 777 /home/ubuntu/桌面/eclipse.desktopchmod u+x /home/ubuntu/桌面/eclipse.desktop]]></content>
      <categories>
        <category>解决方案</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>教程</tag>
        <tag>java</tag>
        <tag>eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]Java线程同步，synchronized锁住的是代码还是对象]]></title>
    <url>%2F2016%2F10%2F24%2F%E8%BD%AC-Java%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%EF%BC%8Csynchronized%E9%94%81%E4%BD%8F%E7%9A%84%E6%98%AF%E4%BB%A3%E7%A0%81%E8%BF%98%E6%98%AF%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[作者：叉叉哥转载自： http://blog.csdn.net/xiao__gui/article/details/8188833 在Java中，synchronized关键字是用来控制线程同步的，就是在多线程的环境下，控制synchronized代码段不被多个线程同时执行。synchronized既可以加在一段代码上，也可以加在方法上。 关键是，不要认为给方法或者代码段加上synchronized就万事大吉，看下面一段代码： 123456789101112131415161718192021222324252627282930class Sync &#123; public synchronized void test() &#123; System.out.println("test开始.."); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("test结束.."); &#125; &#125; class MyThread extends Thread &#123; public void run() &#123; Sync sync = new Sync(); sync.test(); &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread thread = new MyThread(); thread.start(); &#125; &#125; &#125; 运行结果： test开始.. test开始.. test开始.. test结束.. test结束.. test结束.._ 可以看出来，上面的程序起了三个线程，同时运行Sync类中的test()方法，虽然test()方法加上了synchronized，但是还是同时运行起来，貌&#20284;synchronized没起作用。&nbsp; 将test()方法上的synchronized去掉，在方法内部加上synchronized(this)： 1234567891011public void test() &#123; synchronized(this)&#123; System.out.println("test开始.."); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("test结束.."); &#125; &#125; 运行结果： test开始.. test开始.. test开始.. test结束.. test结束.. test结束.. 一切还是这么平静，没有看到synchronized起到作用。 实际上，synchronized(this)以及非static的synchronized方法（至于static synchronized方法请往下看），只能防止多个线程同时执行同一个对象的同步代码段。 回到本文的题目上：synchronized锁住的是代码还是对象。答案是：synchronized锁住的是括号里的对象，而不是代码。对于非static的synchronized方法，锁的就是对象本身也就是this。 当synchronized锁住一个对象后，别的线程如果也想拿到这个对象的锁，就必须等待这个线程执行完成释放锁，才能再次给对象加锁，这样才达到线程同步的目的。即使两个不同的代码段，都要锁同一个对象，那么这两个代码段也不能在多线程环境下同时运行。 所以我们在用synchronized关键字的时候，能缩小代码段的范围就尽量缩小，能在代码段上加同步就不要再整个方法上加同步。这叫减小锁的粒度，使代码更大程度的并发。原因是基于以上的思想，锁的代码段太长了，别的线程是不是要等很久，等的花儿都谢了。当然这段是题外话，与本文核心思想并无太大关联。 再看上面的代码，每个线程中都new了一个Sync类的对象，也就是产生了三个Sync对象，由于不是同一个对象，所以可以多线程同时运行synchronized方法或代码段。 为了验证上述的观点，修改一下代码，让三个线程使用同一个Sync的对象。 1234567891011121314151617181920212223class MyThread extends Thread &#123; private Sync sync; public MyThread(Sync sync) &#123; this.sync = sync; &#125; public void run() &#123; sync.test(); &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; Sync sync = new Sync(); for (int i = 0; i &lt; 3; i++) &#123; Thread thread = new MyThread(sync); thread.start(); &#125; &#125; &#125; 运行结果： test开始.. test结束.. test开始.. test结束.. test开始.. test结束.. 可以看到，此时的synchronized就起了作用。 那么，如果真的想锁住这段代码，要怎么做？也就是，如果还是最开始的那段代码，每个线程new一个Sync对象，怎么才能让test方法不会被多线程执行。 解决也很简单，只要锁住同一个对象不就行了。例如，synchronized后的括号中锁同一个固定对象，这样就行了。这样是没问题，但是，比较多的做法是让synchronized锁这个类对应的Class对象。 1234567891011121314151617181920212223242526272829303132class Sync &#123; public void test() &#123; synchronized (Sync.class) &#123; System.out.println("test开始.."); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("test结束.."); &#125; &#125; &#125; class MyThread extends Thread &#123; public void run() &#123; Sync sync = new Sync(); sync.test(); &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread thread = new MyThread(); thread.start(); &#125; &#125; &#125; 运行结果： test开始.. test结束.. test开始.. test结束.. test开始.. test结束.. 上面代码用synchronized(Sync.class)&lt;/span&gt;实现了全局锁的效果。 最后说说static synchronized方法，static方法可以直接类名加方法名调用，方法中无法使用this，所以它锁的不是this，而是类的Class对象，所以，static synchronized方法也相当于全局锁，相当于锁住了代码段。 参考链接http://www.cnblogs.com/techyc/archive/2013/03/19/2969677.html]]></content>
      <categories>
        <category>IT</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]Ubuntu14.04下安装单机版Storm运行环境]]></title>
    <url>%2F2016%2F10%2F20%2F%E5%8E%9F-Ubuntu14-04%E4%B8%8B%E5%AE%89%E8%A3%85%E5%8D%95%E6%9C%BA%E7%89%88Storm%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[1、安装java环境 如果选择的是sun的java安装包，去官网下载后，记得提升文件权限（chmod +x path/文件名） 设置环境变量，在/etc/profile文件中添加以下内容（针对所有用户，相当于windows下面的系统变量） 1234export JAVA_HOME=/opt/software/jdk1.8.0_92export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 设置默认jdk 12345sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_92/bin/java 300 sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.8.0_92/bin/javac 300 sudo update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/jdk1.8.0_92/bin/jar 300 sudo update-alternatives --install /usr/bin/javah javah /usr/lib/jvm/jdk1.8.0_92/bin/javah 300 sudo update-alternatives --install /usr/bin/javap javap /usr/lib/jvm/jdk1.8.0_92/bin/javap 300 查看系统已经存在的jdksudo update­alternatives ­­list java 配置默认jdksudo update­alternatives ­­config java 表示当前默认的JDK，我这里已经设置好了最后使用java javac java -version命令来查看是否配置成功 2、安装Python Ubuntu默认安装了，直接使用Python命令来检测是否安装成功 3、安装Zookeeper安装前，把后面要用到的安装工具全部安装好sudo apt-get install g++sudo apt-get install uuid-devsudo apt-get install gitsudo apt-get install automakesudo apt-get install libtool 下载Zookeeper3.4.6 提升文件权限（chnmod +x path/文件名） 解压 tar -xvzf filename 配置ZooKeeper环境变量 sudo gedit /etc/profile 在/etc/profile中添加环境变量 12export ZOOKEEPER_HOME=/opt/software/zookeeper-3.4.6export PATH=\$PATH:\$ZOOKEEPER_HOME/bin 利用source /etc/profile 命令来让环境变量立即生效 4、安装ZeroMQ123456下载zeromq2.1.7.tar.gz编译安装./configureMakeSudo make installSudo ldconfig 5、安装Jzmp安装git工具，不安装也可以直接去github下载release版本123456sudo apt-get install git进入jzmp解压包的根目录编译和安装（前面的java的环境变量没有配置好，或者没有设置成默认的，这里无法通过）./configuremakesudo make install 6、安装storm1234567去Apache下载apache-storm-0.9.6.tar.gz解压后设置环境变量Sudo gedit /etc/profile末尾添加export STORM_HOME=/opt/software/apache-storm-0.9.6export PATH=$PATH:$STORM_HOME/bin 7、设置Storm的配置文件首先设置zookeeper的配置文件1234567891011进入 cd /opt/software/zookeeper-3.4.6/conf从sample_zoo.cfg文件复制一份并重命名为zoo.cfg添加以下内容dataDir=/opt/zookeeper/datadataLogDir=/opt/zookeeper/logserver.1=127.0.0.1:2888:3888dataDir放置数据信息dataLogDir放置日志信息注意：必须手动创建该目录，否则后面启动zookeeper的时候，zkServer.sh status将会报错，最终显示的界面都是错误信息server.1由于是本地模式，所以只配置了一个，如果有多个机器，可以进行多个配置server.2进入/opt/software/zookeeper-3.4.6/bin目录进行测试 zkServer.sh startzkServer.sh statuszkServer.sh stop 接下来设置storm1234567891011121314151617进入 /opt/software/apache-storm-0.9.6/conf中的storm.yaml文件添加以下条目 storm.zookeeper.servers: - &quot;127.0.0.1&quot; nimbus.host: &quot;127.0.0.1&quot; storm.zookeeper.port: 2181 storm.local.dir : &quot;/home/linux/data&quot; supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703添加的时候注意，每一行必须有一个空格在前面，其中nimbus.host:后面有个空格，.local.dir后面的冒号前后都有空格，这个奇葩的设定让我浪费了半天的时间。进入opt/software/apache-storm-0.9.6/bin中用以下命令启动Storm nimbusStorm supervisorStorm ui 至此Storm的安装工作完成了]]></content>
      <categories>
        <category>解决方案</category>
        <category>storm</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>教程</tag>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]java中关键字super和this的使用]]></title>
    <url>%2F2016%2F10%2F11%2F%E5%8E%9F-java%E4%B8%AD%E5%85%B3%E9%94%AE%E5%AD%97super%E5%92%8Cthis%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1、子类的构造函数如果要引用super的话，必须把super放在函数的首位1234567891011121314151617class Base &#123;Base() &#123;System.out.println("Base");&#125;&#125;public class Checket extends Base &#123;Checket() &#123;super();//调用父类的构造方法，一定要放在方法的首个语句System.out.println("Checket");&#125;public static void main(String argv[]) &#123;Checket c = new Checket();&#125;&#125; 如果想用super继承父类构造的方法，但是没有放在第一行的话，那么在super之前的语句，肯定是为了满足自己想要完成某些行为的语句，但是又用了super继承父类的构造方法。那么以前所做的修改就都回到以前了，就是说又成了父类的构造方法了。 2．在Java中，有时还会遇到子类中的成员变量或方法与超类（有时也称父类）中的成员变量或方法同名。因为子类中的成员变量或方法名优先级高，所以子类中的同名成员变量或方法就隐藏了超类的成员变量或方法，但是我们如果想要使用超类中的这个成员变量或方法，就需要用到super. 1234567891011121314151617181920212223class Country &#123;String name;void value() &#123;name = "China";&#125;&#125;class City extends Country &#123;String name;void value() &#123;name = "Hefei";super.value();//不调用此方法时，super.name返回的是父类的成员变量的值nullSystem.out.println(name);System.out.println(super.name);&#125;public static void main(String[] args) &#123;City c=new City();c.value();&#125;&#125; 为了在子类中引用父类中的成员变量name和方法value()，在代码中使用了super、super.name和super.value(),若不调用super.value()时，super.name返回父类成员变量默认值null,调用此方法时，super.value()方法把成员变量name赋值为China,再利用super.name调用父类的成员变量的值。 另外，要注意的是super.name调用的是成员变量的值，12345678910111213141516171819202122232425class Country &#123;String name="xianfan";String value(String name) &#123;name = "China";return name;&#125;&#125;class City extends Country &#123;String name;String value(String name) &#123;name = "Hefei";super.value("失败");//不调用此方法时，super.name返回的是父类的成员变量的值nullSystem.out.println(name);System.out.println(super.name);return name;&#125;public static void main(String[] args) &#123;City c=new City();c.value("成功");&#125;&#125; 结果为：Hefeixianfan此时，super.name返回的值是父类成员变量的值xianfan,而此时的super.value()方法是不起作用的。 3．用super直接传递参数：123456789101112131415161718192021222324252627282930313233343536class Person &#123;public static void prt(String s) &#123;System.out.println(s);&#125;Person() &#123;prt("A Person.");&#125;Person(String name) &#123;prt("A person name is:" + name);&#125;&#125;public class Chinese extends Person &#123;Chinese() &#123;super(); // 调用父类构造函数（1）prt("A chinese.");// (4)&#125;Chinese(String name) &#123;super(name);// 调用父类具有相同形参的构造函数（2）prt("his name is:" + name);&#125;Chinese(String name, int age) &#123;this(name);// 调用当前具有相同形参的构造函数（3）prt("his age is:" + age);&#125;public static void main(String[] args) &#123;Chinese cn = new Chinese();cn = new Chinese("kevin");cn = new Chinese("kevin", 22);&#125;&#125; 结果为：A Person.A chinese.A person name is:kevinhis name is:kevinA person name is:kevinhis name is:kevinhis age is:22 在这段程序中，this和super不再是像以前那样用“.”连接一个方法或成员，而是直接在其后跟上适当的参数，因此它的意义也就有了变化。super后加参数的是用来调用父类中具有相同形式的构造函数，如1和2处。this后加参数则调用的是当前具有相同参数的构造函数，如3处。当然，在Chinese的各个重载构造函数中，this和super在一般方法中的各种用法也仍可使用，比如4处，你可以将它替换为“this.prt”(因为它继承了父类中的那个方法）或者是“super.prt”（因为它是父类中的方法且可被子类访问），它照样可以正确运行。但这样似乎就有点画蛇添足的味道了。 4．super和this的异同： super（参数）：调用基类中的某一个构造函数（应该为构造函数中的第一条语句） this（参数）：调用本类中另一种形成的构造函数（应该为构造函数中的第一条语句） super: 它引用当前对象的直接父类中的成员（用来访问直接父类中被隐藏的父类中成员数据或函数，基类与派生类中有相同成员定义时如：super.变量名 super.成员函数据名（实参） this：它代表当前对象名（在程序中易产生二义性之处，应使用this来指明当前对象；如果函数的形参与类中的成员数据同名，这时需用this来指明成员变量名） 调用super()必须写在子类构造方法的第一行，否则编译不通过。每个子类构造方法的第一条语句，都是隐含地调用super()，如果父类没有这种形式的构造函数，那么在编译的时候就会报错。 super()和this()类似,区别是，super()从子类中调用父类的构造方法，this()在同一类内调用其它方法。 super()和this()均需放在构造方法内第一行。 尽管可以用this调用一个构造器，但却不能调用两个。 this和super不能同时出现在一个构造函数里面，因为this必然会调用其它的构造函数，其它的构造函数必然也会有super语句的存在，所以在同一个构造函数里面有相同的语句，就失去了语句的意义，编译器也不会通过。 this()和super()都指的是对象，所以，均不可以在static环境中使用。包括：static变量,static方法，static语句块。 从本质上讲，this是一个指向本对象的指针, 然而super是一个Java关键字。 通过上面的例子，下面总结一下super的用法：第一、在子类构造方法中要调用父类的构造方法，用“super(参数列表)”的方式调用，参数不是必须的。同时还要注意的一点是：“super(参数列表)”这条语句只能用在子类构造方法体中的第一行。 第二、当子类方法中的局部变量或者子类的成员变量与父类成员变量同名时，也就是子类局部变量覆盖父类成员变量时，用“super.成员变量名”来引用父类成员变量。当然，如果父类的成员变量没有被覆盖，也可以用“super.成员变量名”来引用父类成员变量，不过这是不必要的。 第三、当子类的成员方法覆盖了父类的成员方法时，也就是子类和父类有完全相同的方法定义（但方法体可以不同），此时，用“super.方法名(参数列表)”的方式访问父类的方法。]]></content>
      <categories>
        <category>IT</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]java网络编程--socket套接字]]></title>
    <url>%2F2016%2F10%2F11%2F%E5%8E%9F-java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-socket%E5%A5%97%E6%8E%A5%E5%AD%97%2F</url>
    <content type="text"><![CDATA[Java 网络编程网络编程是指编写运行在多个设备（计算机）的程序，这些设备都通过网络连接起来。java.net包中J2SE的API包含有类和接口，它们提供低层次的通信细节。你可以直接使用这些类和接口，来专注于解决问题，而不用关注通信细节。java.net包中提供了两种常见的网络协议的支持： TCP： TCP是传输控制协议的缩写，它保障了两个应用程序之间的可靠通信。通常用于互联网协议，被称TCP / IP。 UDP:UDP是用户数据报协议的缩写，一个无连接的协议。提供了应用程序之间要发送的数据的数据包。 Socket 编程: 这是使用最广泛的网络概念，它已被解释地非常详细。套接字使用TCP提供了两台计算机之间的通信机制。 客户端程序创建一个套接字，并尝试连接服务器的套接字。当连接建立时，服务器会创建一个Socket对象。客户端和服务器现在可以通过对Socket对象的写入和读取来进行进行通信。 当使用套接字建立TCP链接时有4个步骤： 服务器实例化一个ServerSocket对象 服务器调用ServerSocket类的accept()方法等待客户端的连接 服务器正在等待的过程中，一个客户端实例化一个Socket对象，指定服务器和端口号，发送连接请求 在服务器端，accept()方法返回一个Socket的引用，该socket和客户端的socket相连接 URL 处理: 这部分会在另外的篇幅里讲，点击这里更详细地了解在Java语言中的URL处理。 示例程序: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109//服务器端程序，文件名HelloServer.javaimport java.net.*;import java.io.*;public class HelloServer extends Thread&#123; private ServerSocket serverSocket; //constructor public HelloServer(int port) throws IOException &#123; //1.创建绑定到特定端口的服务器套接字 serverSocket=new ServerSocket(port); serverSocket.setSoTimeout(20000);//休眠20ms &#125; @Override public void run() &#123; while(true) &#123; try &#123; System.out.println("Waiting for client on prot " +serverSocket.getLocalPort()+"..."); //2.服务器端调用ServerSocket类的accept（）方法，侦听并接受到此套接字的连接。 Socket server=serverSocket.accept(); System.out.println("Just connected to "+server.getRemoteSocketAddress()); //返回此套接字绑定的端点的地址，如果尚未绑定则返回 null。 //服务器端接收来数据 DataInputStream in=new DataInputStream(server.getInputStream()); System.out.println(in.readUTF()); //服务端发送数据 DataOutputStream out=new DataOutputStream(server.getOutputStream()); out.writeUTF("Thanks for connneciton to"+ server.getLocalSocketAddress() +"\nGoodbye!"); //关闭客户端 server.close(); &#125;catch(SocketTimeoutException s) &#123; System.out.println("Socket timed out!"); break; &#125;catch(IOException e) &#123; e.printStackTrace(); break; &#125; &#125; &#125; public static void main(String[] args) &#123; // TODO Auto-generated method stub int port=6063; try &#123; Thread t=new HelloServer(port); t.start(); &#125;catch(IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;``` ``` java//客户端程序，文件名HelloClient.javaimport java.net.*;import java.io.*;public class HelloClient &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub //1.设置服务器名称和端口 String serverName="localhost"; int port=6063; try &#123; System.out.println("Connecting to "+serverName+ " on port" +port); //2.建立socket对象，与服务器通信 Socket client=new Socket(serverName,port); System.out.println("Just connected to "+client.getRemoteSocketAddress()); //客户端发送数据给服务器端 OutputStream outToServer=client.getOutputStream(); DataOutputStream out=new DataOutputStream(outToServer); out.writeUTF("hello from"+client.getLocalSocketAddress()); //客户端接收来自服务器端的数据 InputStream inFromServer=client.getInputStream(); DataInputStream in=new DataInputStream(inFromServer); System.out.print("server says " +in.readUTF()); client.close(); &#125;catch(IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 编译运行结果：]]></content>
      <categories>
        <category>IT</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]C++继承：公有，私有，保护]]></title>
    <url>%2F2016%2F09%2F12%2F%E8%BD%AC-C-%E7%BB%A7%E6%89%BF-%E5%85%AC%E6%9C%89%EF%BC%8C%E7%A7%81%E6%9C%89%EF%BC%8C%E4%BF%9D%E6%8A%A4%2F</url>
    <content type="text"><![CDATA[转载自：csqlwy 好记性不如烂笔头—温故而知新的博客 C++继承：公有，私有，保护公有继承(public)、私有继承(private)、保护继承(protected)是常用的三种继承方式。 公有继承(public)公有继承的特点是基类的公有成员和保护成员作为派生类的成员时，它们都保持原有的状态，而基类的私有成员仍然是私有的，不能被这个派生类的子类所访问。 私有继承(private)私有继承的特点是基类的公有成员和保护成员都作为派生类的私有成员，并且不能被这个派生类的子类所访问。 保护继承(protected)保护继承的特点是基类的所有公有成员和保护成员都成为派生类的保护成员，并且只能被它的派生类成员函数或友元访问，基类的私有成员仍然是私有的。 下面列出三种不同的继承方式的基类特性和派生类特性。&amp;space | public | protected | private——|——|—-|——共有继承 | public | protected | 不可见私有继承 | private|private|不可见保护继承| protected|protected|不可见 在上图中：1）基类成员对派生类都是：共有和保护的成员是可见的，私有的的成员是不可见的。2）基类成员对派生类的对象来说：要看基类的成员在派生类中变成了什么类型的成员。如：私有继承时，基类的共有成员和私有成员都变成了派生类中的私有成员，因此对于派生类中的对象来说基类的共有成员和私有成员就是不可见的。 为了进一步理解三种不同的继承方式在其成员的可见性方面的区别，下面从三种不同角度进行讨论。 对于公有继承方式 基类成员对其对象的可见性：公有成员可见，其他不可见。这里保护成员同于私有成员。 基类成员对派生类的可见性：公有成员和保护成员可见，而私有成员不可见。这里保护成员同于公有成员。 基类成员对派生类对象的可见性：公有成员可见，其他成员不可见。 所以，在公有继承时，派生类的对象可以访问基类中的公有成员；派生类的成员函数可以访问基类中的公有成员和保护成员。这里，一定要区分清楚派生类的对象和派生类中的成员函数对基类的访问是不同的。 对于私有继承方式 基类成员对其对象的可见性：公有成员可见，其他成员不可见。 基类成员对派生类的可见性：公有成员和保护成员是可见的，而私有成员是不可见的。 基类成员对派生类对象的可见性：所有成员都是不可见的。所以，在私有继承时，基类的成员只能由直接派生类访问，而无法再往下继承。 对于保护继承方式 这种继承方式与私有继承方式的情况相同。两者的区别仅在于对派生类的成员而言，对基类成员有不同的可见性。 上述所说的可见性也就是可访问性。 关于可访问性还有另的一种说法。这种规则中，称派生类的对象对基类访问为水平访问，称派生类的派生类对基类的访问为垂直访问。 看看这样的例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include&lt;iostream&gt;using namespace std;//////////////////////////////////////////////////////////////////////////class A //父类&#123;private: int privatedateA;protected: int protecteddateA;public: int publicdateA;&#125;;//////////////////////////////////////////////////////////////////////////class B :public A //基类A的派生类B（共有继承）&#123;public: void funct() &#123; int b; b=privatedateA; //error：基类中私有成员在派生类中是不可见的 b=protecteddateA; //ok：基类的保护成员在派生类中为保护成员 b=publicdateA; //ok：基类的公共成员在派生类中为公共成员 &#125;&#125;;//////////////////////////////////////////////////////////////////////////class C :private A //基类A的派生类C（私有继承）&#123;public: void funct() &#123; int c; c=privatedateA; //error：基类中私有成员在派生类中是不可见的 c=protecteddateA; //ok：基类的保护成员在派生类中为私有成员 c=publicdateA; //ok：基类的公共成员在派生类中为私有成员 &#125;&#125;;//////////////////////////////////////////////////////////////////////////class D :protected A //基类A的派生类D（保护继承）&#123;public: void funct() &#123; int d; d=privatedateA; //error：基类中私有成员在派生类中是不可见的 d=protecteddateA; //ok：基类的保护成员在派生类中为保护成员 d=publicdateA; //ok：基类的公共成员在派生类中为保护成员 &#125;&#125;;//////////////////////////////////////////////////////////////////////////int main()&#123; int a; B objB; a=objB.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a=objB.protecteddateA; //error：基类的保护成员在派生类中为保护成员，对对象不可见 a=objB.publicdateA; //ok：基类的公共成员在派生类中为公共成员，对对象可见 C objC; a=objC.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a=objC.protecteddateA; //error：基类的保护成员在派生类中为私有成员，对对象不可见 a=objC.publicdateA; //error：基类的公共成员在派生类中为私有成员，对对象不可见 D objD; a=objD.privatedateA; //error：基类中私有成员在派生类中是不可见的,对对象不可见 a=objD.protecteddateA; //error：基类的保护成员在派生类中为保护成员，对对象不可见 a=objD.publicdateA; //error：基类的公共成员在派生类中为保护成员，对对象不可见 return 0;&#125;]]></content>
      <categories>
        <category>IT</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]C++中友元详解]]></title>
    <url>%2F2016%2F09%2F11%2F%E8%BD%AC-C-%E4%B8%AD%E5%8F%8B%E5%85%83%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[转载自：zhm_sunboy的博客 问题的提出 我们已知道类具备封装和信息隐 藏的特性。只有类的成员函数才能访问类的私有成员，程式中的其他函数是无法访问私有成员的。非成员函数能够访问类中的公有成员，但是假如将数据成员都定义 为公有的，这又破坏了隐藏的特性。另外，应该看到在某些情况下，特别是在对某些成员函数多次调用时，由于参数传递，类型检查和安全性检查等都需要时间开 销，而影响程式的运行效率。 为了解决上述问题，提出一种使用友元的方案。友元是一种定义在类外部的普通函数，但他需要在类体内进行说 明，为了和该类的成员函数加以区别，在说明时前面加以关键字friend。友元不是成员函数，但是他能够访问类中的私有成员。友元的作用在于提高程式的运 行效率，但是，他破坏了类的封装性和隐藏性，使得非成员函数能够访问类的私有成员。 友元能够是个函数，该函数被称为友元函数；友元也能够是个类，该类被称为友元类。 友元函数 友元函数的特点是能够访问类中的私有成员的非成员函数。友元函数从语法上看，他和普通函数相同，即在定义上和调用上和普通函数相同。下面举一例子说明友元函数的应用。 123456789101112131415161718192021222324252627282930313233 #include #include class Point &#123; public: Point(double xx, double yy) &#123; x=xx; y=yy; &#125; void Getxy(); friend double Distance(Point &amp;a, Point &amp;b); private: double x, y; &#125;; void Point::Getxy() &#123; cout&lt;&lt;"("&lt;&lt;&lt;","&lt;&lt;&lt;")"&lt;&lt; FONT&gt; &#125; double Distance(Point &amp;a, Point &amp;b) &#123; double dx = a.x - b.x; double dy = a.y - b.y; return sqrt(dx*dx+dy*dy); &#125; void main() &#123; Point p1(3.0, 4.0), p2(6.0, 8.0); p1.Getxy(); p2.Getxy(); double d = Distance(p1, p2); cout&lt;&lt;"Distance is"&lt;&lt;&lt; FONT&gt; &#125; 说明：在该程式中的Point类中说明了一个友元函数Distance()，他在说明时前边加friend关键字，标识他不是成员函数，而是友元函数。 他的定义方法和普通函数定义相同，而不同于成员函数的定义，因为他无需指出所属的类。但是，他能够引用类中的私有成员，函数体中 a.x，b.x，a.y，b.y都是类的私有成员，他们是通过对象引用的。在调用友元函数时，也是同普通函数的调用相同，不要像成员函数那样调用。本例 中，p1.Getxy()和p2.Getxy()这是成员函数的调用，要用对象来表示。而Distance(p1, p2)是友元函数的调用，他直接调 用，无需对象表示，他的参数是对象。(该程式的功能是已知两点坐标，求出两点的距离。) 友元类 友元除了前面讲过的函数以外，友元还能够是类，即一个类能够作另一个类的友元。当一个类作为另一个类的友元时，这就意味着这个类的任何成员函数都是另一个类的友元函数。 —————————————————-另一篇—————————————————————- 采用类的机制后实现了数据的隐藏与封装，类的数据成员一般定义为私有成员，成员函数一般定义为公有的，依此提供类与外界间的通信接口。但是，有时需要定义一 些函数，这些函数不是类的一部分，但又需要频繁地访问类的数据成员，这时可以将这些函数定义为该函数的友元函数。除了友元函数外，还有友元类，两者统称为 友元。友元的作用是提高了程序的运行效率（即减少了类型检查和安全性检查等都需要时间开销），但它破坏了类的封装性和隐藏性，使得非成员函数可以访问类的 私有成员。 友元函数 ：友元函数是可以直接访问类的私有成员的非成员函数。它是定义在类外的普通函数，它不属于任何类，但需要在类的定义中加以声明，声明时只需在友元的名称前加上关键字friend，其格式如下： friend 类型 函数名(形式参数); 友元函数的声明可以放在类的私有部分，也可以放在公有部分，它们是没有区别的，都说明是该类的一个友元函数。一个函数可以是多个类的友元函数，只需要在各个类中分别声明。友元函数的调用与一般函数的调用方式和原理一致。 友元类 ：友元类的所有成员函数都是另一个类的友元函数，都可以访问另一个类中的隐藏信息（包括私有成员和保护成员）。当希望一个类可以存取另一个类的私有成员时，可以将该类声明为另一类的友元类。定义友元类的语句格式如下： friend class 类名; 其中：friend和class是关键字，类名必须是程序中的一个已定义过的类。 例如，以下语句说明类B是类A的友元类： 1234567class A&#123; …public: friend class B; …&#125;; 经过以上说明后，类B的所有成员函数都是类A的友元函数，能存取类A的私有成员和保护成员。 使用友元类时注意： 友元关系不能被继承。 友元关系是单向的，不具有交换性。若类B是类A的友元，类A不一定是类B的友元，要看在类中是否有相应的声明。 友元关系不具有传递性。若类B是类A的友元，类C是B的友元，类C不一定是类A的友元，同样要看类中是否有相应的申明 注意事项： 友元可以访问类的私有成员。 只能出现在类定义内部，友元声明可以在类中的任何地方，一般放在类定义的开始或结尾。 友元可以是普通的非成员函数，或前面定义的其他类的成员函数，或整个类。 类必须将重载函数集中每一个希望设为友元的函数都声明为友元。 友元关系不能继承，基类的友元对派生类的成员没有特殊的访问权限。如果基类被授予友元关系，则只有基类具有特殊的访问权限。该基类的派生类不能访问授予友元关系的类。]]></content>
      <categories>
        <category>IT</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]java对象的引用和对象的赋值]]></title>
    <url>%2F2016%2F08%2F23%2F%E8%BD%AC-java%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%BC%95%E7%94%A8%E5%92%8C%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%B5%8B%E5%80%BC%2F</url>
    <content type="text"><![CDATA[JAVA 对象引用，以及对象赋值原文：http://blog.sina.com.cn/s/blog_4cd5d2bb0100ve9r.html Java对象及其引用关于对象与引用之间的一些基本概念: 初学Java时，在很长一段时间里，总觉得基本概念很模糊。后来才知道，在许多Java书中，把对象和对象的引用混为一谈。可是，如果我分不清对象与对象引用，那实在没法很好地理解下面的面向对象技术。把自己的一点认识写下来，或许能让初学Java的朋友们少走一点弯路。为便于说明，我们先定义一个简单的类： 12345 class Vehicle &#123; int passengers; int fuelcap; int mpg;&#125; 1Vehicle veh1 = new Vehicle(); 有了这个模板，就可以用它来创建对象：通常把这条语句的动作称之为创建一个对象，其实，它包含了四个动作。 右边的“new Vehicle”，是以Vehicle类为模板，在堆空间里创建一个Vehicle类对象（也简称为Vehicle对象）。 末尾的()意味着，在对象创建后，立即调用Vehicle类的构造函数，对刚生成的对象进行初始化。构造函数是肯定有的。如果你没写，Java会给你补上一个默认的构造函数。 左边的“Vehicle veh 1”创建了一个Vehicle类引用变量。所谓Vehicle类引用，就是以后可以用来指向Vehicle对象的对象引用。 =”操作符使对象引用指向刚创建的那个Vehicle对象。 我们可以把这条语句拆成两部分： Vehicle veh1; veh1 = new Vehicle(); 效果是一样的。这样写，就比较清楚了，有两个实体：一是对象引用变量，一是对象本身。 在堆空间里创建的实体，与在数据段以及栈空间里创建的实体不同。尽管它们也是确确实实存在的实体，但是，我们看不见，也摸不着。不仅如此，我们仔细研究一下第二句，找找刚创建的对象叫什么名字？有人说，它叫“Vehicle”。不对，“Vehicle”是类（对象的创建模板）的名字。一个Vehicle类可以据此创建出无数个对象，这些对象不可能全叫“Vehicle”。对象连名都没有，没法直接访问它。我们只能通过对象引用来间接访问对象。 为了形象地说明对象、引用及它们之间的关系，可以做一个或许不很妥当的比喻。对象好比是一只很大的气球，大到我们抓不住它。引用变量是一根绳， 可以用来系气球。如果只执行了第一条语句，还没执行第二条，此时创建的引用变量veh1还没指向任何一个对象，它的值是null。引用变量可以指向某个对象，或者为null。它是一根绳，一根还没有系上任何一个汽球的绳。执行了第二句后，一只新汽球做出来了，并被系在veh1这根绳上。我们抓住这根绳，就等于抓住了那只汽球。再来一句： Vehicle veh2; 就又做了一根绳，还没系上汽球。如果再加一句： veh2 = veh1; 系上了。这里，发生了复制行为。但是，要说明的是，对象本身并没有被复制，被复制的只是对象引用。结果是，veh2也指向了veh1所指向的对象。两根绳系的是同一只汽球。 如果用下句再创建一个对象： veh2 = new Vehicle(); 则引用变量veh2改指向第二个对象。 从以上叙述再推演下去，我们可以获得以下结论： （1）一个对象引用可以指向0个或1个对象（一根绳子可以不系汽球，也可以系一个汽球）； （2）一个对象可以有N个引用指向它（可以有N条绳子系住一个汽球）。如果再来下面语句： veh1 = veh2; 按上面的推断，veh1也指向了第二个对象。这个没问题。问题是第一个对象呢？没有一条绳子系住它，它飞了。多数书里说，它被Java的垃圾回收机制回收了。 这不确切。正确地说，它已成为垃圾回收机制的处理对象。至于什么时候真正被回收，那要看垃圾回收机制的心情了。 由此看来，下面的语句应该不合法吧？至少是没用的吧？ new Vehicle(); 不对。它是合法的，而且可用的。譬如，如果我们仅仅为了打印而生成一个对象，就不需要用引用变量来系住它。最常见的就是打印字符串： System.out.println(“I am Java!”); 字符串对象“I am Java!”在打印后即被丢弃。有人把这种对象称之为临时对象。对象与引用的关系将持续到对象回收。 Java对象及引用Java对象及引用是容易混淆却又必须掌握的基础知识，本章阐述Java对象和引用的概念，以及与其密切相关的参数传递。 先看下面的程序： StringBuffer s; s = new StringBuffer(“Hello World!”); 第一个语句仅为引用(reference)分配了空间，而第二个语句则通过调用类(StringBuffer)的构造函数StringBuffer(String str)为类生成了一个实例（或称为对象）。这两个操作被完成后，对象的内容则可通过s进行访问——在Java里都是通过引用来操纵对象的。 Java对象和引用的关系可以说是互相关联，却又彼此独立。彼此独立主要表现在：引用是可以改变的，它可以指向别的对象，譬如上面的s，你可以给它另外的对象，如： s = new StringBuffer(“Java”); 这样一来，s就和它指向的第一个对象脱离关系。 从存储空间上来说，对象和引用也是独立的，它们存储在不同的地方，对象一般存储在堆中，而引用存储在速度更快的堆栈中。 引用可以指向不同的对象，对象也可以被多个引用操纵，如： StringBuffer s1 = s; 这条语句使得s1和s指向同一个对象。既然两个引用指向同一个对象，那么不管使用哪个引用操纵对象，对象的内容都发生改变，并且只有一份，通过s1和s得到的内容自然也一样，(String除外，因为String始终不变，String s1=”AAAA”; String s=s1,操作s,s1由于始终不变，所以为s另外开辟了空间来存储s,)如下面的程序： 1234567891011StringBuffer s;s = new StringBuffer("Java");StringBuffer s1 = s;s1.append(" World");System.out.println("s1=" + s1.toString());//打印结果为：s1=Java WorldSystem.out.println("s=" + s.toString());//打印结果为：s=Java World 上面的程序表明，s1和s打印出来的内容是一样的，这样的结果看起来让人非常疑惑，但是仔细想想，s1和s只是两个引用，它们只是操纵杆而已，它们指向同一个对象，操纵的也是同一个对象，通过它们得到的是同一个对象的内容。这就像汽车的刹车和油门，它们操纵的都是车速，假如汽车开始的速度是80，然后你踩了一次油门，汽车加速了，假如车速升到了120，然后你踩一下刹车，此时车速是从120开始下降的，假如下降到60，再踩一次油门，车速则从60开始上升，而不是从第一次踩油门后的120开始。也就是说车速同时受油门和刹车影响，它们的影响是累积起来的，而不是各自独立（除非刹车和油门不在一辆车上）。所以，在上面的程序中，不管使用s1还是s操纵对象，它们对对象的影响也是累积起来的（更多的引用同理）。 只有理解了对象和引用的关系，才能理解参数传递。 一般面试题中都会考Java传参的问题，并且它的标准答案是 Java只有一种参数传递方式：那就是按值传递，即Java中传递任何东西都是传值。如果传入方法的是基本类型的东西，你就得到此基本类型的一份拷贝。如果是传递引用，就得到引用的拷贝。 一般来说，对于基本类型的传递，我们很容易理解，而对于对象，总让人感觉是按引用传递，看下面的程序： 12345678910111213141516171819202122232425262728public class ObjectRef &#123; //基本类型的参数传递 public static void testBasicType(int m) &#123; System.out.println("m=" + m);//m=50 m = 100; System.out.println("m=" + m);//m=100 &#125; //参数为对象，不改变引用的值 ？？？？？？ public static void add(StringBuffer s) &#123; s.append("_add"); &#125; //参数为对象，改变引用的值 ？？？？？ public static void changeRef(StringBuffer s) &#123; //牵着的绳子(s)换气球了 s = new StringBuffer("Java"); &#125; public static void main(String[] args) &#123; int i = 50; testBasicType(i); System.out.println(i);//i=50 StringBuffer sMain = new StringBuffer("init"); System.out.println("sMain=" + sMain.toString());//sMain=init add(sMain); System.out.println("sMain=" + sMain.toString());//sMain=init_add changeRef(sMain); System.out.println("sMain=" + sMain.toString());//sMain=init_add &#125;&#125; 以上程序的允许结果显示出，testBasicType方法的参数是基本类型，尽管参数m的值发生改变，但并不影响i。 add方法的参数是一个对象，当把sMain传给参数s时，s得到的是sMain的拷贝，所以s和sMain指向同一个对象，因此，使用s操作影响的其实就是sMain指向的对象，故调用add方法后，sMain指向的对象的内容发生了改变。 在changeRef方法中，参数也是对象，当把sMain传给参数s时，s得到的是sMain的拷贝，但与add方法不同的是，在方法体内改变了s指向的对象（也就是s指向了别的对象,牵着气球的绳子换气球了），给s重新赋值后，s与sMain已经毫无关联，它和sMain指向了不同的对象，所以不管对s做什么操作，都不会影响sMain指向的对象，故调用changeRef方法前后sMain指向的对象内容并未发生改变。 对于add方法的调用结果，可能很多人会有这种感觉：这不明明是按引用传递吗？对于这种问题，还是套用Bruce Eckel的话：这依赖于你如何看待引用，最终你会明白，这个争论并没那么重要。真正重要的是，你要理解，传引用使得（调用者的）对象的修改变得不可预期。 123456789101112131415161718192021222324252627public class Test&#123; public int i,j; public void test_m(Test a) &#123; Test b = new Test(); b.i = 1; b.j = 2; a = b; &#125; public void test_m1(Test a ) &#123; a.i = 1; a.j = 2; &#125; public static void main(String argv[]) &#123; Test t= new Test(); t.i = 5; t.j = 6; System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //5,6 t.test_m(t); System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //5,6,a和t都指向了一个对象，而在test_m中s又指向了另一个对象，所以对象t不变！！！ t.test_m1(t); System.out.println( "t.i = "+ t.i + " t.j= " + t.j); //1,2 &#125;&#125; 答案只有一个：Java里都是按值传递参数。如果参数是普通类型，传递的就是普通的，如果参数是对象的引用，传递的也是引用（引用本身也是一种类型）。而实际上，我们要明白，当参数是对象时，传引用会发生什么状况（就像上面的add方法）？ ========================================================================= 楼主，这样来记这个问题如下表达式：A a1 = new A();它代表A是类，a1是引用，a1不是对象，new A()才是对象，a1引用指向new A()这个对象。 在JAVA里，“=”不能被看成是一个赋值语句，它不是在把一个对象赋给另外一个对象，它的执行过程实质上是将右边对象的地址传给了左边的引用，使得左边的引用指向了右边的对象。JAVA表面上看起来没有指针，但它的引用其实质就是一个指针，引用里面存放的并不是对象，而是该对象的地址，使得该引用指向了对象。在JAVA里，“=”语句不应该被翻译成赋值语句，因为它所执行的确实不是一个赋值的过程，而是一个传地址的过程，被译成赋值语句会造成很多误解，译得不准确。 再如：A a2;它代表A是类，a2是引用，a2不是对象，a2所指向的对象为空null; 再如：a2 = a1;它代表，a2是引用，a1也是引用，a1所指向的对象的地址传给了a2(传址），使得a2和a1指向了同一对象。 综上所述，可以简单的记为，在初始化时，“=”语句左边的是引用，右边new出来的是对象。在后面的左右都是引用的“=”语句时，左右的引用同时指向了右边引用所指向的对象。 再所谓实例，其实就是对象的同义词。 如果需要赋值，就需要类实现Cloneable接口，实现clone()方法。 1234567891011class D implements Cloneable&#123;//实现Cloneable接口 String sex; D(String sex)&#123; this.sex=sex; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // 实现clone方法 return super.clone(); &#125;&#125; 赋值的时候： 12D d=new D("男");D d2=(D) d.clone();//把d赋值给d2 如果类中的变量不是主类型，而是对象，也需要调用该对象的clone()方法下面是一个完整的例子： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Test2 &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; // TODO Auto-generated method stub D d=new D("男"); C c=new C("张三","20",d); C new_c=(C) c.clone();//调用clone方法来赋值 new_c.name="李四"; d.sex="女";//d System.out.println(c.d.sex); System.out.println(c.name); &#125;&#125;class C implements Cloneable&#123; String name; String age; D d; C(String name,String age,D d) throws CloneNotSupportedException&#123; this.name=name; this.age=age; this.d=(D) d.clone();//调用clone方法来赋值，这样即便外部的d发生变化，c里的也不会变 &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125;&#125;class D implements Cloneable&#123;//实现Cloneable接口 String sex; D(String sex)&#123; this.sex=sex; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // 实现clone方法 return super.clone(); &#125;&#125;]]></content>
      <categories>
        <category>IT</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]Java学习笔记（一）]]></title>
    <url>%2F2016%2F08%2F23%2F%E5%8E%9F-Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[java的自动回收机制 java拥有自动回收机制，也可以手动使用finalize()方法进行释放资源。 java的参数传递123456789101112131415161718192021222324252627282930313233package com.test3;public class TestReference &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub A a1=new A(1,2); B b=new B(11,22); System.out.println(a1.b); a1.test(b); System.out.println(a1.b); //java中是值传递，输出都是2 &#125;&#125;class A&#123; int a,b; A(int a,int b)&#123; this.a=a; this.b=b; &#125; public void test(B b)&#123; b.a=a; &#125;&#125;class B&#123; int a,b; B(int a,int b)&#123; this.a=a; this.b=b; &#125; public void setA(int a)&#123; this.a=a; &#125;&#125; 从上面的代码看出，java的对象引用的传递和C++的不同，java的引用传递相当于值传递，C++的引用传递相当于传递的是地址。 java类的引用修饰符 类 成员变量 成员方法 public 包外可见 包外可见 包外可见 abstract 申明抽象类 X 抽象方法 final 不能有子类 最终域修饰符 最终方法控制符 static X 静态域修饰符 静态方法控制符 volatile X 共享域修饰符 X transient X 暂时性域修饰符 X private X 类内调用 类内调用 protected X 父子类之间调用 父子类之间调用 native X X 本地方法控制符 synchronized X X 同步方法控制符]]></content>
      <categories>
        <category>IT</category>
        <category>java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[原]欢迎使用CSDN-markdown编辑器]]></title>
    <url>%2F2016%2F08%2F22%2F%E5%8E%9F-%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8CSDN-markdown%E7%BC%96%E8%BE%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[欢迎使用Markdown编辑器写博客本Markdown编辑器使用StackEdit修改而来，用它写博客，将会带来全新的体验哦： Markdown和扩展Markdown简洁的语法 代码块高亮 图片链接和图片上传 LaTex数学公式 UML序列图和流程图 离线写博客 导入导出Markdown文件 丰富的快捷键 快捷键 加粗 Ctrl + B 斜体 Ctrl + I 引用 Ctrl + Q 插入链接 Ctrl + L 插入代码 Ctrl + K 插入图片 Ctrl + G 提升标题 Ctrl + H 有序列表 Ctrl + O 无序列表 Ctrl + U 横线 Ctrl + R 撤销 Ctrl + Z 重做 Ctrl + Y Markdown及扩展 Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— [ 维基百科 ] 使用简单的符号标识不同的标题，将某些文字标记为粗体或者斜体，创建一个链接等，详细语法参考帮助？。 本编辑器支持 Markdown Extra , 扩展了很多好用的功能。具体请参考Github. 表格Markdown Extra 表格语法： 项目 价格 Computer $1600 Phone $12 Pipe $1 可以使用冒号来定义对齐方式： 项目 价格 数量 Computer 1600 元 5 Phone 12 元 12 Pipe 1 元 234 定义列表Markdown Extra 定义列表语法：项目１项目２: 定义 A: 定义 B 项目３: 定义 C : 定义 D &gt; 定义D内容 代码块代码块语法遵循标准markdown代码，例如：12345678910@requires_authorizationdef somefunc(param1='', param2=0): '''A docstring''' if param1 &gt; param2: # interesting print 'Greater' return (param2 - param1 + 1) or Noneclass SomeClass: pass&gt;&gt;&gt; message = '''interpreter... prompt''' 脚注生成一个脚注footnote. footnote: 这里是 脚注 的 内容. 目录用 [TOC]来生成目录： [TOC] 数学公式使用MathJax渲染LaTex 数学公式，详见math.stackexchange.com. 行内公式，数学公式为：$\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$。 块级公式： x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a}更多LaTex语法请参考 这儿. UML 图:可以渲染序列图： 123张三-&gt;李四: 嘿，小四儿, 写博客了没?Note right of 李四: 李四愣了一下，说：李四--&gt;张三: 忙得吐血，哪有时间写。 或者流程图： 12345678st=&gt;start: 开始e=&gt;end: 结束op=&gt;operation: 我的操作cond=&gt;condition: 确认？st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 关于 序列图 语法，参考 这儿, 关于 流程图 语法，参考 这儿. 离线写博客即使用户在没有网络的情况下，也可以通过本编辑器离线写博客（直接在曾经使用过的浏览器中输入write.blog.csdn.net/mdeditor即可。Markdown编辑器使用浏览器离线存储将内容保存在本地。 用户写博客的过程中，内容实时保存在浏览器缓存中，在用户关闭浏览器或者其它异常情况下，内容不会丢失。用户再次打开浏览器时，会显示上次用户正在编辑的没有发表的内容。 博客发表后，本地缓存将被删除。 用户可以选择 把正在写的博客保存到服务器草稿箱，即使换浏览器或者清除缓存，内容也不会丢失。 注意：虽然浏览器存储大部分时候都比较可靠，但为了您的数据安全，在联网后，请务必及时发表或者保存到服务器草稿箱。 浏览器兼容 目前，本编辑器对Chrome浏览器支持最为完整。建议大家使用较新版本的Chrome。 IE９以下不支持 IE９，１０，１１存在以下问题 不支持离线功能 IE9不支持文件导入导出 IE10不支持拖拽文件导入]]></content>
      <categories>
        <category>IT</category>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>CSDN</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转]ROC曲线-阈值评价标准]]></title>
    <url>%2F2016%2F04%2F11%2F%E8%BD%AC-ROC%E6%9B%B2%E7%BA%BF-%E9%98%88%E5%80%BC%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86%2F</url>
    <content type="text"><![CDATA[ROC曲线指受试者工作特征曲线 / 接收器操作特性曲线(receiver operating characteristic curve), 是反映敏感性和特异性连续变量的综合指标,是用构图法揭示敏感性和特异性的相互关系，它通过将连续变量设定出多个不同的临界&#20540;，从而计算出一系列敏感性和特异性，再以敏感性为纵坐标、（1-特异性）为横坐标绘制成曲线，曲线下面积越大，诊断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性均较高的临界&#20540;。 &lt;/span&gt;&lt;/span&gt; &lt;/span&gt; #ROC曲线的例子 考虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。 TP：正确肯定的数目； FN：漏报，没有正确找到的匹配的数目； FP：误报，给出的匹配是不正确的； TN：正确拒绝的非匹配对数； 列联表如下表所示，1代表正类，0代表负类。 &nbsp; &nbsp; 预测 &nbsp; &nbsp; &nbsp; 1 0 合计 实际 1 True Positive（TP） False Negative（FN） Actual Positive(TP&#43;FN) &nbsp; 0 False Positive（FP) True Negative(TN) Actual Negative(FP&#43;TN) 合计 &nbsp; Predicted Positive(TP&#43;FP) Predicted Negative(FN&#43;TN) TP&#43;FP&#43;FN&#43;TN 从列联表引入两个新名词。其一是真正类率(true positive rate ,TPR), 计算公式为TPR=TP/ (TP&#43;&nbsp;FN)，刻画的是分类器所识别出的 正实例占所有正实例的比例。另外一个是负正类率(false positive rate,&nbsp;FPR),计算公式为FPR= FP / (FP &#43; TN)，计算的是分类器错认为正类的负实例占所有负实例的比例。还有一个真负类率（True Negative Rate，TNR），也称为specificity,计算公式为TNR=TN/ (FP&#43;&nbsp;TN) = 1-FPR。 ![](http://my.csdn.net/uploads/201206/20/1340175403_3476.jpg) 其中，两列True matches和True non-match分别代表应该匹配上和不应该匹配上的 两行Pred matches和Pred non-match分别代表预测匹配上和预测不匹配上的 ![](http://my.csdn.net/uploads/201206/20/1340175807_2760.jpg) 在一个二分类模型中，对于所得到的连续结果，假设已确定一个阀&#20540;，比如说 0.6，大于这个&#20540;的实例划归为正类，小于这个&#20540;则划到负类中。如果减小阀&#20540;，减到0.5，固然能识别出更多的正类，也就是提高了识别出的正例占所有正例 的比类，即TPR,但同时也将更多的负实例当作了正实例，即提高了FPR。为了形象化这一变化，在此引入ROC，ROC曲线可以用于评价一个分类器。&lt;/span&gt; &lt;/span&gt; ROC曲线和它相关的比率 (a)理想情况下，TPR应该接近1，FPR应该接近0。 ROC曲线上的每一个点对应于一个threshold，对于一个分类器，每个threshold下会有一个TPR和FPR。&lt;/span&gt; 比如Threshold最大时，TP=FP=0，对应于原点；Threshold最小时，TN=FN=0，对应于右上角的点(1,1) (b)P和N得分不作为特征间距离d的一个函数，随着阈&#20540;theta增加，TP和FP都增加 &lt;/span&gt;&lt;/span&gt; Receiver Operating Characteristic,翻译为&quot;接受者操作特性曲线&quot;，够拗口的。曲线由两个变量1-specificity 和 Sensitivity绘制. 1-specificity=FPR，即负正类率。Sensitivity即是真正类率，TPR(True positive rate),反映了正类覆盖程度。这个组合以1-specificity对sensitivity,即是以代价(costs)对收益(benefits)。 &nbsp; &nbsp; &nbsp; &nbsp;此外，ROC曲线还可以用来计算“均&#20540;平均精度”（mean average precision），这是当你通过改变阈&#20540;来选择最好的结果时所得到的平均精度（PPV）.&lt;/span&gt; 下表是一个逻辑回归得到的结果。将得到的实数&#20540;按大到小划分成10个个数 相同的部分。&lt;/span&gt; &lt;/span&gt; Percentile 实例数 正例数 1-特异度(%) 敏感度(%)&lt;/tr&gt; 10 6180 4879 2.73 34.64&lt;/tr&gt; 20 6180 2804 9.80 54.55&lt;/tr&gt; 30 6180 2165 18.22 69.92&lt;/tr&gt; 40 6180 1506 28.01 80.62&lt;/tr&gt; 50 6180 987 38.90 87.62&lt;/tr&gt; 60 6180 529 50.74 91.38&lt;/tr&gt; 70 6180 365 62.93 93.97&lt;/tr&gt; 80 6180 294 75.26 96.06&lt;/tr&gt; 90 6180 297 87.59 98.17&lt;/tr&gt; 100 6177 258 100.00 100.00&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;其正例数为此部分里实际的正类数。也就是说，将逻辑回归得到的结 果按从大到小排列，倘若以前10%的数&#20540;作为阀&#20540;，即将前10%的实例都划归为正类，6180个。其中，正确的个数为4879个，占所有正类的 4879/14084100%=34.64%，即敏感度；另外，有6180-4879=1301个负实例被错划为正类，占所有负类的1301 /47713100%=2.73%,即1-特异度。以这两组&#20540;分别作为x&#20540;和y&#20540;，在excel中作散点图。]]></content>
      <categories>
        <category>IT</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>ROC</tag>
        <tag>机器学习</tag>
        <tag>分类</tag>
      </tags>
  </entry>
</search>
